{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Load Libs'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from time import time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Load Data And Identify Classes'''\n",
    "PathTrain =  os.getcwd() + \"/data/train.csv\"\n",
    "PathTest =  os.getcwd() + \"/data/test.csv\"\n",
    "DFTrain = pd.read_csv(PathTrain, header=None)\n",
    "DFTest = pd.read_csv(PathTest, header=None)\n",
    "totalClasses = len(Counter(DFTrain.iloc[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTrain = DFTrain.iloc[:,:-1]\n",
    "#DTrain = np.c_[np.ones(len(DTrain)), DTrain]\n",
    "LabelsTrain = np.array(DFTrain.iloc[:,-1])\n",
    "DTest = DFTest.iloc[:,:-1]\n",
    "#DTest = np.c_[np.ones(len(DTest)), DTest]\n",
    "LabelsTest = np.array(DFTest.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTrain = normalize(DTrain)\n",
    "DTest = normalize(DTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''One Hot Encoding for Train and Test Labels'''\n",
    "oneHotTrainLables = np.zeros((len(DFTrain), totalClasses))\n",
    "\n",
    "for i in range(len(DFTrain)):\n",
    "    oneHotTrainLables[i][DFTrain.iloc[i, -1]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@@---------Q1_e---------@@@@@@@@@@\n",
      "@@@@@@@@@@-----SKLEARN_ReLU-----@@@@@@@@@@\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size=100, beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100, 100), learning_rate='constant',\n",
      "              learning_rate_init=0.1, max_iter=400, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "              random_state=None, shuffle=True, solver='sgd', tol=0.0001,\n",
      "              validation_fraction=0.1, verbose=True, warm_start=False)\n",
      "$$ Training Model...!!!\n",
      "Iteration 1, loss = 2.50405180\n",
      "Iteration 2, loss = 0.84682057\n",
      "Iteration 3, loss = 0.62123330\n",
      "Iteration 4, loss = 0.50731600\n",
      "Iteration 5, loss = 0.42956930\n",
      "Iteration 6, loss = 0.36740948\n",
      "Iteration 7, loss = 0.31982879\n",
      "Iteration 8, loss = 0.27890659\n",
      "Iteration 9, loss = 0.24658421\n",
      "Iteration 10, loss = 0.21510732\n",
      "Iteration 11, loss = 0.18912824\n",
      "Iteration 12, loss = 0.17335465\n",
      "Iteration 13, loss = 0.14934140\n",
      "Iteration 14, loss = 0.13519584\n",
      "Iteration 15, loss = 0.12295281\n",
      "Iteration 16, loss = 0.10832961\n",
      "Iteration 17, loss = 0.09556701\n",
      "Iteration 18, loss = 0.08845938\n",
      "Iteration 19, loss = 0.07581305\n",
      "Iteration 20, loss = 0.06640326\n",
      "Iteration 21, loss = 0.05943244\n",
      "Iteration 22, loss = 0.05215078\n",
      "Iteration 23, loss = 0.04870272\n",
      "Iteration 24, loss = 0.03838057\n",
      "Iteration 25, loss = 0.03591393\n",
      "Iteration 26, loss = 0.02947566\n",
      "Iteration 27, loss = 0.02611341\n",
      "Iteration 28, loss = 0.02089552\n",
      "Iteration 29, loss = 0.01815662\n",
      "Iteration 30, loss = 0.01530761\n",
      "Iteration 31, loss = 0.01174716\n",
      "Iteration 32, loss = 0.01142228\n",
      "Iteration 33, loss = 0.00911192\n",
      "Iteration 34, loss = 0.00820209\n",
      "Iteration 35, loss = 0.00771290\n",
      "Iteration 36, loss = 0.00716793\n",
      "Iteration 37, loss = 0.00657637\n",
      "Iteration 38, loss = 0.00621406\n",
      "Iteration 39, loss = 0.00585524\n",
      "Iteration 40, loss = 0.00551160\n",
      "Iteration 41, loss = 0.00524148\n",
      "Iteration 42, loss = 0.00505748\n",
      "Iteration 43, loss = 0.00486284\n",
      "Iteration 44, loss = 0.00464832\n",
      "Iteration 45, loss = 0.00447479\n",
      "Iteration 46, loss = 0.00439083\n",
      "Iteration 47, loss = 0.00424370\n",
      "Iteration 48, loss = 0.00408154\n",
      "Iteration 49, loss = 0.00405381\n",
      "Iteration 50, loss = 0.00391237\n",
      "Iteration 51, loss = 0.00381287\n",
      "Iteration 52, loss = 0.00374775\n",
      "Iteration 53, loss = 0.00363538\n",
      "Iteration 54, loss = 0.00360855\n",
      "Iteration 55, loss = 0.00350346\n",
      "Iteration 56, loss = 0.00345678\n",
      "Iteration 57, loss = 0.00338123\n",
      "Iteration 58, loss = 0.00332501\n",
      "Iteration 59, loss = 0.00328416\n",
      "Iteration 60, loss = 0.00323269\n",
      "Iteration 61, loss = 0.00320228\n",
      "Iteration 62, loss = 0.00314115\n",
      "Iteration 63, loss = 0.00311759\n",
      "Iteration 64, loss = 0.00306964\n",
      "Iteration 65, loss = 0.00302627\n",
      "Iteration 66, loss = 0.00298378\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "$$ Training Complete...!!!\n",
      "$$ Training Time = 0.86Min\n"
     ]
    }
   ],
   "source": [
    "print(\"@@@@@@@@@@---------Q1_e---------@@@@@@@@@@\")\n",
    "print(\"@@@@@@@@@@-----SKLEARN_ReLU-----@@@@@@@@@@\")\n",
    "'''For invscaling it is not working. therefore kept lr to 0.1 constant and 0.5 with adaptive'''\n",
    "t0 = time()\n",
    "nn = MLPClassifier(hidden_layer_sizes=(100, 100), activation='relu', solver='sgd', \n",
    "                     batch_size=100, learning_rate_init=0.1, learning_rate='constant', max_iter=400, verbose=True)\n",
    "\n",
    "print(nn)\n",
    "print(\"$$ Training Model...!!!\")\n",
    "nn.fit(DTrain, LabelsTrain)\n",
    "t1 = time()\n",
    "print(\"$$ Training Complete...!!!\")\n",
    "print(\"$$ Training Time = {}Min\".format(round((t1-t0)/60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy = 100.0%\n",
      "Test Accuracy = 92.062%\n"
     ]
    }
   ],
   "source": [
    "pred = nn.predict(DTest)\n",
    "AccuTest = 100* accuracy_score(LabelsTest, pred)\n",
    "\n",
    "pred = nn.predict(DTrain)\n",
    "AccuTrain = 100* accuracy_score(LabelsTrain, pred)\n",
    "print(\"Train Accuracy = {}%\".format(round(AccuTrain,3)))\n",
    "print(\"Test Accuracy = {}%\".format(round(AccuTest,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@@---------Q1_e---------@@@@@@@@@@\n",
      "@@@@@@@@@@-----SKLEARN_ReLU-----@@@@@@@@@@\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size=100, beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100, 100), learning_rate='adaptive',\n",
      "              learning_rate_init=0.5, max_iter=400, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "              random_state=None, shuffle=True, solver='sgd', tol=0.0001,\n",
      "              validation_fraction=0.1, verbose=True, warm_start=False)\n",
      "$$ Training Model...!!!\n",
      "Iteration 1, loss = 1.72597547\n",
      "Iteration 2, loss = 0.54860826\n",
      "Iteration 3, loss = 0.41785223\n",
      "Iteration 4, loss = 0.36795371\n",
      "Iteration 5, loss = 0.29211429\n",
      "Iteration 6, loss = 0.27620482\n",
      "Iteration 7, loss = 0.24894190\n",
      "Iteration 8, loss = 0.21342191\n",
      "Iteration 9, loss = 0.22029485\n",
      "Iteration 10, loss = 0.19913282\n",
      "Iteration 11, loss = 0.22163362\n",
      "Iteration 12, loss = 0.18978768\n",
      "Iteration 13, loss = 0.23165300\n",
      "Iteration 14, loss = 0.22735148\n",
      "Iteration 15, loss = 0.21064766\n",
      "Iteration 16, loss = 0.17673582\n",
      "Iteration 17, loss = 0.19672062\n",
      "Iteration 18, loss = 0.21256925\n",
      "Iteration 19, loss = 0.21195333\n",
      "Iteration 20, loss = 0.18504960\n",
      "Iteration 21, loss = 0.18570074\n",
      "Iteration 22, loss = 0.20968193\n",
      "Iteration 23, loss = 0.22839257\n",
      "Iteration 24, loss = 0.21493267\n",
      "Iteration 25, loss = 0.23961114\n",
      "Iteration 26, loss = 0.23252947\n",
      "Iteration 27, loss = 0.23520970\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.100000\n",
      "Iteration 28, loss = 0.09534378\n",
      "Iteration 29, loss = 0.03480552\n",
      "Iteration 30, loss = 0.02193813\n",
      "Iteration 31, loss = 0.01652524\n",
      "Iteration 32, loss = 0.01387527\n",
      "Iteration 33, loss = 0.01258784\n",
      "Iteration 34, loss = 0.01157598\n",
      "Iteration 35, loss = 0.01091740\n",
      "Iteration 36, loss = 0.01032279\n",
      "Iteration 37, loss = 0.00996957\n",
      "Iteration 38, loss = 0.00962779\n",
      "Iteration 39, loss = 0.00935642\n",
      "Iteration 40, loss = 0.00911322\n",
      "Iteration 41, loss = 0.00892188\n",
      "Iteration 42, loss = 0.00877050\n",
      "Iteration 43, loss = 0.00860983\n",
      "Iteration 44, loss = 0.00847687\n",
      "Iteration 45, loss = 0.00837120\n",
      "Iteration 46, loss = 0.00826541\n",
      "Iteration 47, loss = 0.00817795\n",
      "Iteration 48, loss = 0.00807674\n",
      "Iteration 49, loss = 0.00802986\n",
      "Iteration 50, loss = 0.00795805\n",
      "Iteration 51, loss = 0.00790006\n",
      "Iteration 52, loss = 0.00785801\n",
      "Iteration 53, loss = 0.00780515\n",
      "Iteration 54, loss = 0.00775905\n",
      "Iteration 55, loss = 0.00771848\n",
      "Iteration 56, loss = 0.00768694\n",
      "Iteration 57, loss = 0.00764693\n",
      "Iteration 58, loss = 0.00761753\n",
      "Iteration 59, loss = 0.00758512\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 60, loss = 0.00753682\n",
      "Iteration 61, loss = 0.00753013\n",
      "Iteration 62, loss = 0.00752359\n",
      "Iteration 63, loss = 0.00751831\n",
      "Iteration 64, loss = 0.00751297\n",
      "Iteration 65, loss = 0.00750868\n",
      "Iteration 66, loss = 0.00750330\n",
      "Iteration 67, loss = 0.00749864\n",
      "Iteration 68, loss = 0.00749344\n",
      "Iteration 69, loss = 0.00748906\n",
      "Iteration 70, loss = 0.00748423\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 71, loss = 0.00747636\n",
      "Iteration 72, loss = 0.00747538\n",
      "Iteration 73, loss = 0.00747449\n",
      "Iteration 74, loss = 0.00747360\n",
      "Iteration 75, loss = 0.00747276\n",
      "Iteration 76, loss = 0.00747191\n",
      "Iteration 77, loss = 0.00747103\n",
      "Iteration 78, loss = 0.00747004\n",
      "Iteration 79, loss = 0.00746919\n",
      "Iteration 80, loss = 0.00746832\n",
      "Iteration 81, loss = 0.00746747\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 82, loss = 0.00746587\n",
      "Iteration 83, loss = 0.00746569\n",
      "Iteration 84, loss = 0.00746550\n",
      "Iteration 85, loss = 0.00746533\n",
      "Iteration 86, loss = 0.00746515\n",
      "Iteration 87, loss = 0.00746500\n",
      "Iteration 88, loss = 0.00746481\n",
      "Iteration 89, loss = 0.00746466\n",
      "Iteration 90, loss = 0.00746449\n",
      "Iteration 91, loss = 0.00746429\n",
      "Iteration 92, loss = 0.00746412\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 93, loss = 0.00746382\n",
      "Iteration 94, loss = 0.00746378\n",
      "Iteration 95, loss = 0.00746375\n",
      "Iteration 96, loss = 0.00746371\n",
      "Iteration 97, loss = 0.00746367\n",
      "Iteration 98, loss = 0.00746364\n",
      "Iteration 99, loss = 0.00746361\n",
      "Iteration 100, loss = 0.00746358\n",
      "Iteration 101, loss = 0.00746354\n",
      "Iteration 102, loss = 0.00746350\n",
      "Iteration 103, loss = 0.00746347\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 104, loss = 0.00746341\n",
      "Iteration 105, loss = 0.00746340\n",
      "Iteration 106, loss = 0.00746339\n",
      "Iteration 107, loss = 0.00746339\n",
      "Iteration 108, loss = 0.00746338\n",
      "Iteration 109, loss = 0.00746337\n",
      "Iteration 110, loss = 0.00746337\n",
      "Iteration 111, loss = 0.00746336\n",
      "Iteration 112, loss = 0.00746335\n",
      "Iteration 113, loss = 0.00746334\n",
      "Iteration 114, loss = 0.00746334\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 115, loss = 0.00746332\n",
      "Iteration 116, loss = 0.00746332\n",
      "Iteration 117, loss = 0.00746332\n",
      "Iteration 118, loss = 0.00746332\n",
      "Iteration 119, loss = 0.00746332\n",
      "Iteration 120, loss = 0.00746332\n",
      "Iteration 121, loss = 0.00746332\n",
      "Iteration 122, loss = 0.00746331\n",
      "Iteration 123, loss = 0.00746331\n",
      "Iteration 124, loss = 0.00746331\n",
      "Iteration 125, loss = 0.00746331\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 126, loss = 0.00746331\n",
      "Iteration 127, loss = 0.00746331\n",
      "Iteration 128, loss = 0.00746331\n",
      "Iteration 129, loss = 0.00746331\n",
      "Iteration 130, loss = 0.00746331\n",
      "Iteration 131, loss = 0.00746331\n",
      "Iteration 132, loss = 0.00746331\n",
      "Iteration 133, loss = 0.00746331\n",
      "Iteration 134, loss = 0.00746331\n",
      "Iteration 135, loss = 0.00746331\n",
      "Iteration 136, loss = 0.00746331\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 137, loss = 0.00746331\n",
      "Iteration 138, loss = 0.00746331\n",
      "Iteration 139, loss = 0.00746330\n",
      "Iteration 140, loss = 0.00746330\n",
      "Iteration 141, loss = 0.00746330\n",
      "Iteration 142, loss = 0.00746330\n",
      "Iteration 143, loss = 0.00746330\n",
      "Iteration 144, loss = 0.00746330\n",
      "Iteration 145, loss = 0.00746330\n",
      "Iteration 146, loss = 0.00746330\n",
      "Iteration 147, loss = 0.00746330\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "$$ Training Complete...!!!\n",
      "$$ Training Time = 1.13Min\n"
     ]
    }
   ],
   "source": [
    "print(\"@@@@@@@@@@---------Q1_e---------@@@@@@@@@@\")\n",
    "print(\"@@@@@@@@@@-----SKLEARN_ReLU-----@@@@@@@@@@\")\n",
    "t0 = time()\n",
    "nn = MLPClassifier(hidden_layer_sizes=(100, 100), activation='relu', solver='sgd', \n",
    "                     batch_size=100, learning_rate_init=0.5, learning_rate='adaptive', max_iter=400, verbose=True)\n",
    "print(nn)\n",
    "print(\"$$ Training Model...!!!\")\n",
    "nn.fit(DTrain, LabelsTrain)\n",
    "t1 = time()\n",
    "print(\"$$ Training Complete...!!!\")\n",
    "print(\"$$ Training Time = {}Min\".format(round((t1-t0)/60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy = 100.0%\n",
      "Test Accuracy = 92.338%\n"
     ]
    }
   ],
   "source": [
    "pred = nn.predict(DTest)\n",
    "AccuTest = 100* accuracy_score(LabelsTest, pred)\n",
    "\n",
    "pred = nn.predict(DTrain)\n",
    "AccuTrain = 100* accuracy_score(LabelsTrain, pred)\n",
    "print(\"Train Accuracy = {}%\".format(round(AccuTrain,3)))\n",
    "print(\"Test Accuracy = {}%\".format(round(AccuTest,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
