{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "colab_type": "code",
    "id": "B83XJ_ttBEFZ",
    "outputId": "8bb3448b-0be6-43b0-8aac-ac8be59ffadc"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "val9_dFDA5BC"
   },
   "outputs": [],
   "source": [
    "'''Import modules'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "from collections import Counter\n",
    "from skimage import io, transform\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import numpy as np\n",
    "from time import time\n",
    "import collections\n",
    "import pickle\n",
    "import os\n",
    "import gensim\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "x6qEbzQNBMM-",
    "outputId": "5f53a897-eb70-4c9c-9a45-3b943847d016"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cpu\n",
      "Using 0 GPUs!\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device =\", device)\n",
    "print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "parallel = True #enable nn.DataParallel for GPU\n",
    "platform = \"local\" #colab/local\n",
    "restore = True #Restore Checkpoint\n",
    "phase = \"Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Zf92eqfBN34"
   },
   "outputs": [],
   "source": [
    "VOCAB = {}\n",
    "WORD2IDX = {}\n",
    "IDX2WORD = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vLOoOXXGBO1x"
   },
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        h, w = image.shape[:2]\n",
    "        #print(\"TA RESCALE INPUT\", image.shape)\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "        #print(\"TA RESCALE OUTPUT\", image.shape)\n",
    "        return img\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, image):\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        #print(\"TA TRANSPOSE IP\", image.shape)\n",
    "        #image = image.transpose((2, 0, 1))\n",
    "        #print(\"TA TRANSPOSE OP\", image.shape)\n",
    "        return image\n",
    "\n",
    "\n",
    "IMAGE_RESIZE = (256, 256)\n",
    "# Sequentially compose the transforms\n",
    "img_transform = transforms.Compose([Rescale(IMAGE_RESIZE), ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "F3T42fc3BQpv",
    "outputId": "e22babbf-8c19-47fd-f4e4-63a5b0f88ab6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE = 8680\n"
     ]
    }
   ],
   "source": [
    "class CaptionsPreprocessing:\n",
    "    \"\"\"Preprocess the captions, generate vocabulary and convert words to tensor tokens\n",
    "    Args:\n",
    "        captions_file_path (string): captions tsv file path\n",
    "    \"\"\"\n",
    "    def __init__(self, captions_file_path):\n",
    "        self.captions_file_path = captions_file_path\n",
    "\n",
    "        # Read raw captions\n",
    "        self.raw_captions_dict = self.read_raw_captions()\n",
    "\n",
    "        # Preprocess captions\n",
    "        self.captions_dict = self.process_captions()\n",
    "\n",
    "        # Create vocabulary\n",
    "        self.start = \"<start>\"\n",
    "        self.end = \"<end>\"\n",
    "        self.oov = \"<unk>\"\n",
    "        self.pad = \"<pad>\"\n",
    "        self.vocab = self.generate_vocabulary()\n",
    "        self.word2index = self.convert_word2index()        \n",
    "        self.index2word = self.convert_index2word()\n",
    "        \n",
    "\n",
    "    def read_raw_captions(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            Dictionary with raw captions list keyed by image ids (integers)\n",
    "        \"\"\"\n",
    "        captions_dict = {}\n",
    "        with open(self.captions_file_path, 'r', encoding='utf-8') as f:\n",
    "            for img_caption_line in f.readlines():\n",
    "                img_captions = img_caption_line.strip().split('\\t')\n",
    "                captions_dict[int(img_captions[0])] = img_captions[1:]\n",
    "\n",
    "        return captions_dict \n",
    "\n",
    "    def process_captions(self):\n",
    "        \"\"\"\n",
    "        Use this function to generate dictionary and other preprocessing on captions\n",
    "        \"\"\"\n",
    "\n",
    "        raw_captions_dict = self.raw_captions_dict \n",
    "        \n",
    "        # Do the preprocessing here                \n",
    "        captions_dict = raw_captions_dict\n",
    "\n",
    "        return captions_dict\n",
    "\n",
    " \n",
    "\n",
    "    def generate_vocabulary(self):\n",
    "        \"\"\"\n",
    "        Use this function to generate dictionary and other preprocessing on captions\n",
    "        \"\"\"\n",
    "        captions_dict = self.captions_dict\n",
    "\n",
    "        # Generate the vocabulary\n",
    "        \n",
    "        all_captions = \"\"        \n",
    "        for cap_lists in captions_dict.values():\n",
    "            all_captions += \" \".join(cap_lists)\n",
    "        all_captions = nltk.tokenize.word_tokenize(all_captions.lower())\n",
    "        \n",
    "        vocab = {self.pad :1, self.oov :1, self.start :1, self.end :1}\n",
    "        vocab_update = Counter(all_captions) \n",
    "        vocab_update = {k:v for k,v in vocab_update.items() if v >= freq_threshold}\n",
    "        vocab.update(vocab_update)        \n",
    "        vocab_size = len(vocab)\n",
    "        \n",
    "        if phase == \"Train\":\n",
    "            VOCAB.clear()\n",
    "            VOCAB.update(vocab)\n",
    "            if platform == \"colab\":\n",
    "                fname = '/content/drive/My Drive/A4/dict/VOCAB_comp.pkl'\n",
    "            else:\n",
    "                fname = '../dict/VOCAB_comp.pkl'\n",
    "            #if not os.path.isfile(fname):\n",
    "            with open(fname, 'wb') as handle:\n",
    "                pickle.dump(vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "        print(\"VOCAB SIZE =\", vocab_size)\n",
    "        return vocab\n",
    "    \n",
    "    def convert_word2index(self):\n",
    "        \"\"\"\n",
    "        word to index converter\n",
    "        \"\"\"\n",
    "        word2index = {}\n",
    "        vocab = self.vocab\n",
    "        idx = 0\n",
    "        words = vocab.keys()\n",
    "        for w in words:\n",
    "            word2index[w] = idx\n",
    "            idx +=1\n",
    "        if phase == \"Train\":\n",
    "            WORD2IDX.clear()\n",
    "            WORD2IDX.update(word2index)\n",
    "            if platform == \"colab\":\n",
    "                fname = '/content/drive/My Drive/A4/dict/WORD2IDX_comp.pkl'\n",
    "            else:\n",
    "                fname = '../dict/WORD2IDX_comp.pkl'\n",
    "            #if not os.path.isfile(fname):\n",
    "            with open(fname, 'wb') as handle:\n",
    "                pickle.dump(word2index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        return word2index\n",
    "    \n",
    "    def convert_index2word(self):\n",
    "        \"\"\"\n",
    "        index to word converter\n",
    "        \"\"\"\n",
    "        index2word = {}\n",
    "        w2i = self.word2index\n",
    "        idx = 0\n",
    "        \n",
    "        for k, v in w2i.items():\n",
    "            index2word[v] = k\n",
    "            \n",
    "        if phase == \"Train\":\n",
    "            IDX2WORD.clear()\n",
    "            IDX2WORD.update(index2word)\n",
    "            if platform == \"colab\":\n",
    "                fname = '/content/drive/My Drive/A4/dict/IDX2WORD_comp.pkl'\n",
    "            else:\n",
    "                fname = '../dict/IDX2WORD_comp.pkl'\n",
    "            #if not os.path.isfile(fname):\n",
    "            with open(fname, 'wb') as handle:\n",
    "                pickle.dump(index2word, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        return index2word\n",
    "\n",
    "    def captions_transform(self, img_caption_list):\n",
    "        \"\"\"\n",
    "        Use this function to generate tensor tokens for the text captions\n",
    "        Args:\n",
    "            img_caption_list: List of captions for a particular image\n",
    "        \"\"\"\n",
    "        if phase == \"Test\":\n",
    "            word2index = WORD2IDX\n",
    "            vocab = VOCAB\n",
    "        else:\n",
    "            word2index = self.word2index\n",
    "            vocab = self.vocab\n",
    "            \n",
    "        start = self.start\n",
    "        end = self.end\n",
    "        oov = self.oov\n",
    "        \n",
    "        processed_list = list(map(lambda x: nltk.tokenize.word_tokenize(x.lower()), img_caption_list))\n",
    "        \n",
    "        \n",
    "        #print(processed_list)\n",
    "        processed_list = list(map(lambda x: list(map(lambda y: WORD2IDX[y] if y in vocab else WORD2IDX[oov],x)),\n",
    "                                  processed_list))\n",
    "        processed_list = list(map(lambda x: [WORD2IDX['<start>']] + x + [WORD2IDX['<end>']], processed_list))\n",
    "        #print(processed_list)\n",
    "        return processed_list\n",
    "\n",
    "\n",
    "if platform == \"colab\":\n",
    "    CAPTIONS_FILE_PATH = '/content/drive/My Drive/A4/train_captions.tsv'\n",
    "else:\n",
    "    CAPTIONS_FILE_PATH = \"D:/Padhai/IIT Delhi MS(R)/2019-20 Sem II/COL774 Machine Learning/Assignment/Assignment4/train_captions.tsv\"\n",
    "    \n",
    "embedding_dim = 200\n",
    "freq_threshold = 5\n",
    "captions_preprocessing_obj = CaptionsPreprocessing(CAPTIONS_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary Loaded Successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if phase == \"Test\":\n",
    "    if platform != 'colab':\n",
    "        with open('../dict/VOCAB.pkl', 'rb') as handle:\n",
    "            VOCAB = pickle.load(handle)\n",
    "        with open('../dict/WORD2IDX.pkl', 'rb') as handle:\n",
    "            WORD2IDX = pickle.load(handle)\n",
    "        with open('../dict/IDX2WORD.pkl', 'rb') as handle:\n",
    "            IDX2WORD = pickle.load(handle)\n",
    "        print(\"Dictionary Loaded Successfully\")\n",
    "    else:\n",
    "        with open('/content/drive/My Drive/A4/dict/VOCAB.pkl', 'rb') as handle:\n",
    "            VOCAB = pickle.load(handle)\n",
    "        with open('/content/drive/My Drive/A4/dict/WORD2IDX.pkl', 'rb') as handle:\n",
    "            WORD2IDX = pickle.load(handle)\n",
    "        with open('/content/drive/My Drive/A4/dict/IDX2WORD.pkl', 'rb') as handle:\n",
    "            IDX2WORD = pickle.load(handle)\n",
    "        print(\"Dictionary Loaded Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ddnzTeRBUFd"
   },
   "outputs": [],
   "source": [
    "class ImageCaptionsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, img_dir, captions_dict, img_transform=None, captions_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dir (string): Directory with all the images.\n",
    "            captions_dict: Dictionary with captions list keyed by image ids (integers)\n",
    "            img_transform (callable, optional): Optional transform to be applied\n",
    "                on the image sample.\n",
    "\n",
    "            captions_transform: (callable, optional): Optional transform to be applied\n",
    "                on the caption sample (list).\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.captions_dict = captions_dict\n",
    "        self.img_transform = img_transform\n",
    "        self.captions_transform = captions_transform\n",
    "\n",
    "        self.image_ids = list(captions_dict.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, 'image_{}.jpg'.format(self.image_ids[idx]))\n",
    "        image = io.imread(img_name)\n",
    "        #print(\"RAW IMG\", image.shape)\n",
    "        #captions = self.captions_dict[self.image_ids[idx]]\n",
    "        if self.img_transform:\n",
    "            image = self.img_transform(image)\n",
    "            \n",
    "            image = image.transpose((2, 0, 1))\n",
    "            \n",
    "\n",
    "        '''if self.captions_transform:            \n",
    "            captions = self.captions_transform(captions)'''\n",
    "            \n",
    "        sample = {'image': image}\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    \n",
    "def custom_batch(batch):\n",
    "    batch_size = len(batch)\n",
    "    captions = []\n",
    "    normalize_img = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "       \n",
    "    x = list(map(lambda b: b['image'],batch)) \n",
    "    x = list(map(lambda i: normalize_img(torch.from_numpy(i)).unsqueeze(0),x))\n",
    "    #print(\"my after norm shape\", x[0].shape)\n",
    "    images = torch.cat(x)\n",
    "    \n",
    "    sample = {'image': images}    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cP-sf163BZEX"
   },
   "outputs": [],
   "source": [
    "#ENCODER\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        resnet50 = models.resnet50(pretrained=True, progress=True)        \n",
    "        self.resnet50 = resnet50\n",
    "        for param in self.resnet50.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(\"EMBED DIM\", embed_dim)\n",
    "        self.fc = nn.Linear(in_features=self.resnet50.fc.in_features, out_features=embed_dim, bias = True)\n",
    "        layers = list(resnet50.children())[:-1]\n",
    "        self.resnet50 = nn.Sequential(*layers)\n",
    "        '''for layer in list(self.resnet50.children())[2:]:\n",
    "            for params in layer.parameters():\n",
    "                params.requires_grad = True'''\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        print(\"resnet50 Loaded Successfully..!\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet50(x)\n",
    "        #print(\"Resnet module op\", x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print(\"Resnet module op reshape\", x.shape)\n",
    "        x = self.fc(x)\n",
    "        x = self.relu(x)\n",
    "        #print(\"Resnet FC op\", x)\n",
    "        return x\n",
    "        \n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, units, vocab_size):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.W1 = nn.Linear(in_features = embed_dim, out_features = units)\n",
    "        self.W2 = nn.Linear(in_features=units, out_features=units)\n",
    "        self.V = nn.Linear(in_features=units, out_features=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, img_features, hidden):\n",
    "        \n",
    "        hidden = hidden.unsqueeze(dim=1)\n",
    "        hidden = hidden.double()\n",
    "        #print(\"feature and hidden shape\",img_features.shape, hidden.shape)\n",
    "        combined_score = self.tanh(self.W1(img_features) + self.W2(hidden))\n",
    "        \n",
    "        attention_weights = self.softmax(self.V(combined_score))\n",
    "        context_vector = attention_weights * img_features\n",
    "        context_vector = torch.sum(context_vector, dim=1)\n",
    "        \n",
    "        return context_vector, attention_weights    \n",
    "\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_dim, lstm_hidden_size,lstm_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.vocab_size = len(VOCAB)\n",
    "        print(\"VOCAB SIZE = \", self.vocab_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = embed_dim, hidden_size = lstm_hidden_size,\n",
    "                            num_layers = lstm_layers, batch_first = True)\n",
    "        \n",
    "        self.linear = nn.Linear(lstm_hidden_size, self.vocab_size)        \n",
    "        #self.embed = nn.Embedding.from_pretrained(init_weights)\n",
    "        self.embed = nn.Embedding(self.vocab_size, embed_dim)\n",
    "        #self.attention = AttentionBlock(embed_dim, lstm_hidden_size, self.vocab_size)\n",
    "\n",
    "        \n",
    "    def forward(self, image_features, image_captions, lengths):\n",
    "        #print(\"DECODER INPUT\", image_features)\n",
    "        if phase == \"Train\":\n",
    "            #print(image)\n",
    "            image_features = torch.Tensor.repeat_interleave(image_features, repeats=5 , dim=0)\n",
    "        image_features = image_features.unsqueeze(1)\n",
    "        \n",
    "        '''Uncomment to use attention'''\n",
    "        '''hidden = torch.zeros((image_features.shape[0], self.lstm_hidden_size))\n",
    "        if device == \"cuda\":\n",
    "            hidden = hidden.to(torch.device(\"cuda:0\"))       \n",
    "        \n",
    "        #context, attention = self.attention(image_features, hidden)'''\n",
    "        \n",
    "        embedded_captions = self.embed(image_captions)\n",
    "        #print(\"EMBED SHAPE\", embedded_captions.shape)\n",
    "        #print(\"SHAPES BEFORE CONCAT\",context.unsqueeze(dim=1).shape, embedded_captions[:,:-1].shape)\n",
    "        input_lstm = torch.cat((image_features, embedded_captions[:,:-1]), dim = 1)\n",
    "        #input_lstm = pack_padded_sequence(input_lstm, lengths, batch_first=True, enforce_sorted=False)\n",
    "        lstm_outputs, _ = self.lstm(input_lstm)        \n",
    "        #lstm_outputs = self.linear(lstm_outputs[0]) \n",
    "        #print(\"lstm_outputs.shape\", lstm_outputs.shape)\n",
    "        lstm_outputs = self.linear(lstm_outputs) \n",
    "        \n",
    "        return lstm_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "colab_type": "code",
    "id": "5atlpQpiBa_n",
    "outputId": "6ea119ab-cff8-4177-c9f3-7d29e040bc7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device to CPU\n"
     ]
    }
   ],
   "source": [
    "class ImageCaptionsNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageCaptionsNet, self).__init__()        \n",
    "        ##CNN ENCODER RESNET-50        \n",
    "        self.Encoder = Encoder(embed_dim = embedding_dim)\n",
    "        ## RNN DECODER\n",
    "        self.Decoder = Decoder(embedding_dim, units, 1)    \n",
    "        \n",
    "\n",
    "    def forward(self, img_batch, cap_batch, lengths):\n",
    "        #print(\"IMG INPUT\",x)\n",
    "        x = self.Encoder(img_batch)\n",
    "        #print(\"IMG FEATURE\",x)\n",
    "        x = self.Decoder(x, cap_batch, lengths)\n",
    "        #print(\"IMG FEATURE\",x)\n",
    "        return x\n",
    "    \n",
    "units = 512\n",
    "if restore == False:\n",
    "    net = ImageCaptionsNet()\n",
    "    net = net.double()\n",
    "    \n",
    "'''    if parallel == True and device != \"cpu\":\n",
    "        print(\"Parallel Processing enabled\")\n",
    "        net = nn.DataParallel(net)'''\n",
    "\n",
    "if device == \"cpu\":\n",
    "    print(\"Device to CPU\")\n",
    "else:\n",
    "    print(\"Device to CUDA\")\n",
    "    net = net.to(torch.device(\"cuda:0\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "za24CpL-Bc2_"
   },
   "outputs": [],
   "source": [
    "'''Save and Restore Checkpoints'''\n",
    "def create_checkpoint(path,model, optim_obj, loss_obj,iteration, epoch):\n",
    "    checkpoint = {'epoch': epoch,\n",
    "                  'iteration': iteration,\n",
    "                  'model_state_dict': model.state_dict()}\n",
    "\n",
    "    if platform == \"colab\":\n",
    "        directory = '/content/drive/My Drive/A4/bkp_final_try/'\n",
    "    else:\n",
    "        directory = '../bkp_final_try/'\n",
    "\n",
    "    torch.save(checkpoint, directory + path)\n",
    "    \n",
    "def restore_checkpoint(path):\n",
    "    new_state_dict = collections.OrderedDict()\n",
    "    if platform == \"colab\":\n",
    "        directory = '/content/drive/My Drive/A4/bkp_final_try/'\n",
    "        checkpoint = torch.load(directory + path, map_location=torch.device('cpu'))\n",
    "    else:\n",
    "        directory = '../bkp_final_try/'\n",
    "        checkpoint = torch.load(directory + path, map_location=torch.device('cpu'))    \n",
    "    \n",
    "    epoch = checkpoint['epoch']\n",
    "    new_state_dict = checkpoint['model_state_dict']\n",
    "    iteration = checkpoint['iteration']\n",
    "    #optimizer_state_dict = checkpoint['optimizer_state_dict']\n",
    "    #loss_obj = checkpoint['loss']\n",
    "    print(\"Iterations = {}, Epoch = {}\".format(iteration, epoch))\n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def beam_search(img_feature, max_words=15, beam_k=3):\n",
    "    \n",
    "    #init with start token \n",
    "    init_caption = []\n",
    "    init_caption = [[[WORD2IDX[\"<start>\"]], float(0)]]\n",
    "\n",
    "    while len(init_caption[0][0]) < max_words:\n",
    "        temp_cap = []\n",
    "        \n",
    "        for c in init_caption:  \n",
    "            #print(c[0])\n",
    "            cap_pad = c[0] +  [0] * int(max_words - len(c[0]))\n",
    "            cap_pad = torch.LongTensor(cap_pad).unsqueeze(dim=0)\n",
    "            lstm_op = net.Decoder(img_feature, cap_pad)        \n",
    "            lstm_op = lstm_op.reshape(max_words, lstm_op.shape[2])\n",
    "        \n",
    "            #TOP k prob\n",
    "            #print(lstm_op.shape)\n",
    "            #print(torch.argmax(lstm_op[0], dim=0).tolist())\n",
    "            top_pred = torch.argmax(lstm_op, dim=1)\n",
    "            #top_pred = torch.argsort(top_pred)[-beam_k:]\n",
    "            print(top_pred)\n",
    "            for i in range(beam_k): \n",
    "                word_idx = top_pred[i]\n",
    "                prob = c[1] + lstm_op[0][word_idx]\n",
    "                cap = c[0][:] + [word_idx]\n",
    "                \n",
    "                temp_cap.append([cap, prob])\n",
    "                \n",
    "        init_caption = temp_cap\n",
    "        init_caption = sorted(init_caption, reverse=False, key=lambda x: x[1])[-beam_k:]\n",
    "    #print(type(init_caption[-1][0]))\n",
    "    temp_caption = list(map(lambda x: IDX2WORD[x], init_caption[-1][0]))\n",
    "    \n",
    "    pred_caption = list()\n",
    "    for w in temp_caption:\n",
    "        if w != '<end>':\n",
    "            pred_caption.append(w)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return pred_caption\n",
    "\n",
    "def caption_image(image_feature, max_words=20):\n",
    "        results = []\n",
    "        states = None\n",
    "        x = image_feature.unsqueeze(0)\n",
    "        #print(x)\n",
    "        with torch.no_grad():\n",
    "            for i in range(max_words):\n",
    "                \n",
    "                hiddens, states = net.Decoder.lstm(x, states)\n",
    "                #print(hiddens.shape)\n",
    "                decoder_op = net.Decoder.linear(hiddens.squeeze(1))\n",
    "                predicted_word = decoder_op.argmax(1)\n",
    "                prob = max(decoder_op[0].tolist())\n",
    "                #print(\"{} - {}\".format(IDX2WORD[predicted_word.item()], prob))\n",
    "                x = net.Decoder.embed(predicted_word).unsqueeze(0)\n",
    "                \n",
    "                results.append(predicted_word.item())\n",
    "                \n",
    "                '''if predicted_word == WORD2IDX[\"<end>\"]:\n",
    "                    break'''\n",
    "        \n",
    "        caption = [IDX2WORD[i] for i in  results]\n",
    "        return ' '.join(caption)\n",
    "               \n",
    "    \n",
    "# Define your hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBED DIM 200\n",
      "resnet50 Loaded Successfully..!\n",
      "VOCAB SIZE =  8680\n",
      "Iterations = 25, Epoch = 1\n",
      "EMBED DIM 200\n",
      "resnet50 Loaded Successfully..!\n",
      "VOCAB SIZE =  8680\n",
      "State Dictionary Loaded Successfully.\n",
      "1000\n",
      "['image_10005', 'image_10010', 'image_10088', 'image_10097', 'image_10106']\n",
      "10005\n",
      "Image_idx 0\n",
      "tensor([[-1.9639e-03,  2.7241e-01, -2.3129e-03,  1.3445e+00,  6.6016e-01,\n",
      "         -3.3799e-03,  9.1875e-01, -2.4428e-03, -1.5702e-03, -4.1441e-03,\n",
      "         -5.0070e-04,  4.1966e-01, -2.3454e-03, -2.7365e-03,  5.3379e-01,\n",
      "         -2.8045e-03,  9.8574e-02,  2.4870e+00, -2.3918e-03,  2.6445e-01,\n",
      "          1.8996e+00, -2.9671e-03,  1.3359e+00, -1.9331e-04, -1.3179e-03,\n",
      "         -1.9671e-03, -2.2045e-03, -2.9289e-03,  6.4827e-01,  7.1795e-01,\n",
      "         -6.9516e-04, -1.9713e-03,  2.2487e+00,  6.2337e-01,  1.1113e-01,\n",
      "         -3.1616e-03, -2.0678e-03, -8.8828e-04, -4.6308e-03,  6.9077e-02,\n",
      "         -3.5480e-03,  1.7838e+00, -3.5071e-03, -7.2820e-04, -2.7333e-03,\n",
      "          3.9954e-01, -1.1518e-03, -9.8188e-04,  5.7370e-01,  1.0444e+00,\n",
      "         -1.0997e-03,  1.7403e-01, -1.6644e-03,  7.0252e-01, -1.6604e-03,\n",
      "          2.0106e-01, -2.0087e-03,  2.7968e-02,  1.8993e+00, -1.9382e-03,\n",
      "         -2.5981e-03,  6.2031e-02,  1.1676e+00,  1.3680e+00, -4.0399e-03,\n",
      "          6.4964e-01,  2.4437e+00, -3.5539e-03,  3.3158e-01,  1.0629e+00,\n",
      "          9.3713e-02,  1.1393e+00, -4.7020e-03,  1.9222e-02, -1.6482e-03,\n",
      "         -1.5188e-03, -1.3188e-03,  1.2402e+00, -2.4124e-03, -2.2669e-03,\n",
      "         -3.7067e-03,  2.0596e+00, -2.0427e-03,  1.0359e-01,  9.7469e-01,\n",
      "         -1.9055e-03,  3.9625e-02, -8.4678e-04,  3.3950e-01,  2.7255e-01,\n",
      "          1.2946e+00, -2.1453e-03, -2.6579e-03,  1.3907e+00,  3.5096e-03,\n",
      "          5.5119e-02, -2.7482e-03, -1.6786e-03, -2.5119e-03,  3.7047e-01,\n",
      "         -3.4687e-04, -4.1552e-04, -9.4782e-04, -3.4106e-03,  1.3654e-01,\n",
      "          6.4122e-02, -1.9430e-03,  4.9558e-01,  1.1681e+00, -2.0998e-03,\n",
      "          9.9730e-01,  6.5932e-01,  2.0886e+00,  1.8597e-01, -1.3806e-03,\n",
      "          4.3192e-01, -1.4836e-03,  1.1097e+00,  1.9574e-01, -1.5850e-03,\n",
      "         -3.3591e-03,  1.1718e+00, -1.1105e-03, -2.4706e-03, -2.7413e-03,\n",
      "         -1.6585e-03, -2.9432e-03, -3.8456e-03, -3.0396e-03,  8.9776e-01,\n",
      "          7.2802e-01,  1.8205e+00,  1.0794e+00,  1.1803e+00,  1.3699e+00,\n",
      "          1.4121e-01,  9.5545e-01,  9.3733e-01, -2.5306e-03, -1.7338e-03,\n",
      "          1.1510e+00,  1.5929e+00, -4.8807e-03,  1.9931e+00,  2.4763e+00,\n",
      "         -3.6844e-03,  1.5755e+00, -1.7780e-03, -3.3488e-04,  2.1436e+00,\n",
      "          2.5305e+00, -2.2834e-03,  2.0366e+00, -5.3106e-03, -2.2435e-03,\n",
      "         -2.3019e-03,  2.5633e+00, -1.0834e-03,  4.8738e-01,  7.2597e-01,\n",
      "         -7.9633e-04,  1.3810e+00, -2.5637e-03,  7.4130e-01, -3.9064e-03,\n",
      "         -2.4533e-03,  1.8876e+00, -1.5379e-04, -2.0418e-03, -2.4048e-03,\n",
      "          2.2810e-01,  8.2893e-02,  7.0905e-01, -4.7352e-03,  1.4283e+00,\n",
      "         -1.9339e-03,  1.1799e-01, -2.6398e-03,  8.7637e-02,  9.8120e-01,\n",
      "         -9.6053e-04, -3.5648e-04, -3.0190e-03, -3.0461e-03, -2.0689e-03,\n",
      "         -7.1463e-04, -3.4688e-03,  7.3491e-01, -3.1044e-03,  1.3903e+00,\n",
      "          2.5896e-01, -9.5551e-04,  1.4902e+00,  1.1169e+00, -2.7138e-03,\n",
      "         -1.3980e-03,  6.2814e-01,  6.9557e-01,  1.3330e+00, -2.4323e-03]],\n",
      "       dtype=torch.float64, grad_fn=<LeakyReluBackward0>)\n",
      "Predicted 0 <start> un uomo uomo ragazzi corpo corpo vicine maneggiano getta inclina impedire punta.un installa zona monopolio gratta feriti dipinte <end> musicali giocavano usano alzando piuma arancioni.un accendono rampe rampa.una grigliato cream miami esaminare terminale scrivania intorno.un softball.un gruppo.un africane drenaggio drenaggio drenaggio armato <end> lavoratori agricolo femmina esotici <end> sull'autobus l'azione biglietti scrivania intorno.un softball.un gruppo.un africane drenaggio drenaggio drenaggio\n",
      "Image_idx 1\n",
      "tensor([[-1.9639e-03,  2.7241e-01, -2.3129e-03,  1.3445e+00,  6.6016e-01,\n",
      "         -3.3799e-03,  9.1875e-01, -2.4428e-03, -1.5702e-03, -4.1441e-03,\n",
      "         -5.0070e-04,  4.1966e-01, -2.3454e-03, -2.7365e-03,  5.3379e-01,\n",
      "         -2.8045e-03,  9.8574e-02,  2.4870e+00, -2.3918e-03,  2.6445e-01,\n",
      "          1.8996e+00, -2.9671e-03,  1.3359e+00, -1.9331e-04, -1.3179e-03,\n",
      "         -1.9671e-03, -2.2045e-03, -2.9289e-03,  6.4827e-01,  7.1795e-01,\n",
      "         -6.9516e-04, -1.9713e-03,  2.2487e+00,  6.2337e-01,  1.1113e-01,\n",
      "         -3.1616e-03, -2.0678e-03, -8.8828e-04, -4.6308e-03,  6.9077e-02,\n",
      "         -3.5480e-03,  1.7838e+00, -3.5071e-03, -7.2820e-04, -2.7333e-03,\n",
      "          3.9954e-01, -1.1518e-03, -9.8188e-04,  5.7370e-01,  1.0444e+00,\n",
      "         -1.0997e-03,  1.7403e-01, -1.6644e-03,  7.0252e-01, -1.6604e-03,\n",
      "          2.0106e-01, -2.0087e-03,  2.7968e-02,  1.8993e+00, -1.9382e-03,\n",
      "         -2.5981e-03,  6.2031e-02,  1.1676e+00,  1.3680e+00, -4.0399e-03,\n",
      "          6.4964e-01,  2.4437e+00, -3.5539e-03,  3.3158e-01,  1.0629e+00,\n",
      "          9.3713e-02,  1.1393e+00, -4.7020e-03,  1.9222e-02, -1.6482e-03,\n",
      "         -1.5188e-03, -1.3188e-03,  1.2402e+00, -2.4124e-03, -2.2669e-03,\n",
      "         -3.7067e-03,  2.0596e+00, -2.0427e-03,  1.0359e-01,  9.7469e-01,\n",
      "         -1.9055e-03,  3.9625e-02, -8.4678e-04,  3.3950e-01,  2.7255e-01,\n",
      "          1.2946e+00, -2.1453e-03, -2.6579e-03,  1.3907e+00,  3.5096e-03,\n",
      "          5.5119e-02, -2.7482e-03, -1.6786e-03, -2.5119e-03,  3.7047e-01,\n",
      "         -3.4687e-04, -4.1552e-04, -9.4782e-04, -3.4106e-03,  1.3654e-01,\n",
      "          6.4122e-02, -1.9430e-03,  4.9558e-01,  1.1681e+00, -2.0998e-03,\n",
      "          9.9730e-01,  6.5932e-01,  2.0886e+00,  1.8597e-01, -1.3806e-03,\n",
      "          4.3192e-01, -1.4836e-03,  1.1097e+00,  1.9574e-01, -1.5850e-03,\n",
      "         -3.3591e-03,  1.1718e+00, -1.1105e-03, -2.4706e-03, -2.7413e-03,\n",
      "         -1.6585e-03, -2.9432e-03, -3.8456e-03, -3.0396e-03,  8.9776e-01,\n",
      "          7.2802e-01,  1.8205e+00,  1.0794e+00,  1.1803e+00,  1.3699e+00,\n",
      "          1.4121e-01,  9.5545e-01,  9.3733e-01, -2.5306e-03, -1.7338e-03,\n",
      "          1.1510e+00,  1.5929e+00, -4.8807e-03,  1.9931e+00,  2.4763e+00,\n",
      "         -3.6844e-03,  1.5755e+00, -1.7780e-03, -3.3488e-04,  2.1436e+00,\n",
      "          2.5305e+00, -2.2834e-03,  2.0366e+00, -5.3106e-03, -2.2435e-03,\n",
      "         -2.3019e-03,  2.5633e+00, -1.0834e-03,  4.8738e-01,  7.2597e-01,\n",
      "         -7.9633e-04,  1.3810e+00, -2.5637e-03,  7.4130e-01, -3.9064e-03,\n",
      "         -2.4533e-03,  1.8876e+00, -1.5379e-04, -2.0418e-03, -2.4048e-03,\n",
      "          2.2810e-01,  8.2893e-02,  7.0905e-01, -4.7352e-03,  1.4283e+00,\n",
      "         -1.9339e-03,  1.1799e-01, -2.6398e-03,  8.7637e-02,  9.8120e-01,\n",
      "         -9.6053e-04, -3.5648e-04, -3.0190e-03, -3.0461e-03, -2.0689e-03,\n",
      "         -7.1463e-04, -3.4688e-03,  7.3491e-01, -3.1044e-03,  1.3903e+00,\n",
      "          2.5896e-01, -9.5551e-04,  1.4902e+00,  1.1169e+00, -2.7138e-03,\n",
      "         -1.3980e-03,  6.2814e-01,  6.9557e-01,  1.3330e+00, -2.4323e-03]],\n",
      "       dtype=torch.float64, grad_fn=<LeakyReluBackward0>)\n",
      "Predicted 1 <start> un uomo uomo ragazzi corpo corpo vicine maneggiano getta inclina impedire punta.un installa zona monopolio gratta feriti dipinte <end> musicali giocavano usano alzando piuma arancioni.un accendono rampe rampa.una grigliato cream miami esaminare terminale scrivania intorno.un softball.un gruppo.un africane drenaggio drenaggio drenaggio armato <end> lavoratori agricolo femmina esotici <end> sull'autobus l'azione biglietti scrivania intorno.un softball.un gruppo.un africane drenaggio drenaggio drenaggio\n",
      "Image_idx 2\n",
      "tensor([[-1.9639e-03,  2.7241e-01, -2.3129e-03,  1.3445e+00,  6.6016e-01,\n",
      "         -3.3799e-03,  9.1875e-01, -2.4428e-03, -1.5702e-03, -4.1441e-03,\n",
      "         -5.0070e-04,  4.1966e-01, -2.3454e-03, -2.7365e-03,  5.3379e-01,\n",
      "         -2.8045e-03,  9.8574e-02,  2.4870e+00, -2.3918e-03,  2.6445e-01,\n",
      "          1.8996e+00, -2.9671e-03,  1.3359e+00, -1.9331e-04, -1.3179e-03,\n",
      "         -1.9671e-03, -2.2045e-03, -2.9289e-03,  6.4827e-01,  7.1795e-01,\n",
      "         -6.9516e-04, -1.9713e-03,  2.2487e+00,  6.2337e-01,  1.1113e-01,\n",
      "         -3.1616e-03, -2.0678e-03, -8.8828e-04, -4.6308e-03,  6.9077e-02,\n",
      "         -3.5480e-03,  1.7838e+00, -3.5071e-03, -7.2820e-04, -2.7333e-03,\n",
      "          3.9954e-01, -1.1518e-03, -9.8188e-04,  5.7370e-01,  1.0444e+00,\n",
      "         -1.0997e-03,  1.7403e-01, -1.6644e-03,  7.0252e-01, -1.6604e-03,\n",
      "          2.0106e-01, -2.0087e-03,  2.7968e-02,  1.8993e+00, -1.9382e-03,\n",
      "         -2.5981e-03,  6.2031e-02,  1.1676e+00,  1.3680e+00, -4.0399e-03,\n",
      "          6.4964e-01,  2.4437e+00, -3.5539e-03,  3.3158e-01,  1.0629e+00,\n",
      "          9.3713e-02,  1.1393e+00, -4.7020e-03,  1.9222e-02, -1.6482e-03,\n",
      "         -1.5188e-03, -1.3188e-03,  1.2402e+00, -2.4124e-03, -2.2669e-03,\n",
      "         -3.7067e-03,  2.0596e+00, -2.0427e-03,  1.0359e-01,  9.7469e-01,\n",
      "         -1.9055e-03,  3.9625e-02, -8.4678e-04,  3.3950e-01,  2.7255e-01,\n",
      "          1.2946e+00, -2.1453e-03, -2.6579e-03,  1.3907e+00,  3.5096e-03,\n",
      "          5.5119e-02, -2.7482e-03, -1.6786e-03, -2.5119e-03,  3.7047e-01,\n",
      "         -3.4687e-04, -4.1552e-04, -9.4782e-04, -3.4106e-03,  1.3654e-01,\n",
      "          6.4122e-02, -1.9430e-03,  4.9558e-01,  1.1681e+00, -2.0998e-03,\n",
      "          9.9730e-01,  6.5932e-01,  2.0886e+00,  1.8597e-01, -1.3806e-03,\n",
      "          4.3192e-01, -1.4836e-03,  1.1097e+00,  1.9574e-01, -1.5850e-03,\n",
      "         -3.3591e-03,  1.1718e+00, -1.1105e-03, -2.4706e-03, -2.7413e-03,\n",
      "         -1.6585e-03, -2.9432e-03, -3.8456e-03, -3.0396e-03,  8.9776e-01,\n",
      "          7.2802e-01,  1.8205e+00,  1.0794e+00,  1.1803e+00,  1.3699e+00,\n",
      "          1.4121e-01,  9.5545e-01,  9.3733e-01, -2.5306e-03, -1.7338e-03,\n",
      "          1.1510e+00,  1.5929e+00, -4.8807e-03,  1.9931e+00,  2.4763e+00,\n",
      "         -3.6844e-03,  1.5755e+00, -1.7780e-03, -3.3488e-04,  2.1436e+00,\n",
      "          2.5305e+00, -2.2834e-03,  2.0366e+00, -5.3106e-03, -2.2435e-03,\n",
      "         -2.3019e-03,  2.5633e+00, -1.0834e-03,  4.8738e-01,  7.2597e-01,\n",
      "         -7.9633e-04,  1.3810e+00, -2.5637e-03,  7.4130e-01, -3.9064e-03,\n",
      "         -2.4533e-03,  1.8876e+00, -1.5379e-04, -2.0418e-03, -2.4048e-03,\n",
      "          2.2810e-01,  8.2893e-02,  7.0905e-01, -4.7352e-03,  1.4283e+00,\n",
      "         -1.9339e-03,  1.1799e-01, -2.6398e-03,  8.7637e-02,  9.8120e-01,\n",
      "         -9.6053e-04, -3.5648e-04, -3.0190e-03, -3.0461e-03, -2.0689e-03,\n",
      "         -7.1463e-04, -3.4688e-03,  7.3491e-01, -3.1044e-03,  1.3903e+00,\n",
      "          2.5896e-01, -9.5551e-04,  1.4902e+00,  1.1169e+00, -2.7138e-03,\n",
      "         -1.3980e-03,  6.2814e-01,  6.9557e-01,  1.3330e+00, -2.4323e-03]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       dtype=torch.float64, grad_fn=<LeakyReluBackward0>)\n",
      "Predicted 2 <start> un uomo uomo ragazzi corpo corpo vicine maneggiano getta inclina impedire punta.un installa zona monopolio gratta feriti dipinte <end> musicali giocavano usano alzando piuma arancioni.un accendono rampe rampa.una grigliato cream miami esaminare terminale scrivania intorno.un softball.un gruppo.un africane drenaggio drenaggio drenaggio armato <end> lavoratori agricolo femmina esotici <end> sull'autobus l'azione biglietti scrivania intorno.un softball.un gruppo.un africane drenaggio drenaggio drenaggio\n",
      "Image_idx 3\n",
      "tensor([[-1.9639e-03,  2.7241e-01, -2.3129e-03,  1.3445e+00,  6.6016e-01,\n",
      "         -3.3799e-03,  9.1875e-01, -2.4428e-03, -1.5702e-03, -4.1441e-03,\n",
      "         -5.0070e-04,  4.1966e-01, -2.3454e-03, -2.7365e-03,  5.3379e-01,\n",
      "         -2.8045e-03,  9.8574e-02,  2.4870e+00, -2.3918e-03,  2.6445e-01,\n",
      "          1.8996e+00, -2.9671e-03,  1.3359e+00, -1.9331e-04, -1.3179e-03,\n",
      "         -1.9671e-03, -2.2045e-03, -2.9289e-03,  6.4827e-01,  7.1795e-01,\n",
      "         -6.9516e-04, -1.9713e-03,  2.2487e+00,  6.2337e-01,  1.1113e-01,\n",
      "         -3.1616e-03, -2.0678e-03, -8.8828e-04, -4.6308e-03,  6.9077e-02,\n",
      "         -3.5480e-03,  1.7838e+00, -3.5071e-03, -7.2820e-04, -2.7333e-03,\n",
      "          3.9954e-01, -1.1518e-03, -9.8188e-04,  5.7370e-01,  1.0444e+00,\n",
      "         -1.0997e-03,  1.7403e-01, -1.6644e-03,  7.0252e-01, -1.6604e-03,\n",
      "          2.0106e-01, -2.0087e-03,  2.7968e-02,  1.8993e+00, -1.9382e-03,\n",
      "         -2.5981e-03,  6.2031e-02,  1.1676e+00,  1.3680e+00, -4.0399e-03,\n",
      "          6.4964e-01,  2.4437e+00, -3.5539e-03,  3.3158e-01,  1.0629e+00,\n",
      "          9.3713e-02,  1.1393e+00, -4.7020e-03,  1.9222e-02, -1.6482e-03,\n",
      "         -1.5188e-03, -1.3188e-03,  1.2402e+00, -2.4124e-03, -2.2669e-03,\n",
      "         -3.7067e-03,  2.0596e+00, -2.0427e-03,  1.0359e-01,  9.7469e-01,\n",
      "         -1.9055e-03,  3.9625e-02, -8.4678e-04,  3.3950e-01,  2.7255e-01,\n",
      "          1.2946e+00, -2.1453e-03, -2.6579e-03,  1.3907e+00,  3.5096e-03,\n",
      "          5.5119e-02, -2.7482e-03, -1.6786e-03, -2.5119e-03,  3.7047e-01,\n",
      "         -3.4687e-04, -4.1552e-04, -9.4782e-04, -3.4106e-03,  1.3654e-01,\n",
      "          6.4122e-02, -1.9430e-03,  4.9558e-01,  1.1681e+00, -2.0998e-03,\n",
      "          9.9730e-01,  6.5932e-01,  2.0886e+00,  1.8597e-01, -1.3806e-03,\n",
      "          4.3192e-01, -1.4836e-03,  1.1097e+00,  1.9574e-01, -1.5850e-03,\n",
      "         -3.3591e-03,  1.1718e+00, -1.1105e-03, -2.4706e-03, -2.7413e-03,\n",
      "         -1.6585e-03, -2.9432e-03, -3.8456e-03, -3.0396e-03,  8.9776e-01,\n",
      "          7.2802e-01,  1.8205e+00,  1.0794e+00,  1.1803e+00,  1.3699e+00,\n",
      "          1.4121e-01,  9.5545e-01,  9.3733e-01, -2.5306e-03, -1.7338e-03,\n",
      "          1.1510e+00,  1.5929e+00, -4.8807e-03,  1.9931e+00,  2.4763e+00,\n",
      "         -3.6844e-03,  1.5755e+00, -1.7780e-03, -3.3488e-04,  2.1436e+00,\n",
      "          2.5305e+00, -2.2834e-03,  2.0366e+00, -5.3106e-03, -2.2435e-03,\n",
      "         -2.3019e-03,  2.5633e+00, -1.0834e-03,  4.8738e-01,  7.2597e-01,\n",
      "         -7.9633e-04,  1.3810e+00, -2.5637e-03,  7.4130e-01, -3.9064e-03,\n",
      "         -2.4533e-03,  1.8876e+00, -1.5379e-04, -2.0418e-03, -2.4048e-03,\n",
      "          2.2810e-01,  8.2893e-02,  7.0905e-01, -4.7352e-03,  1.4283e+00,\n",
      "         -1.9339e-03,  1.1799e-01, -2.6398e-03,  8.7637e-02,  9.8120e-01,\n",
      "         -9.6053e-04, -3.5648e-04, -3.0190e-03, -3.0461e-03, -2.0689e-03,\n",
      "         -7.1463e-04, -3.4688e-03,  7.3491e-01, -3.1044e-03,  1.3903e+00,\n",
      "          2.5896e-01, -9.5551e-04,  1.4902e+00,  1.1169e+00, -2.7138e-03,\n",
      "         -1.3980e-03,  6.2814e-01,  6.9557e-01,  1.3330e+00, -2.4323e-03]],\n",
      "       dtype=torch.float64, grad_fn=<LeakyReluBackward0>)\n",
      "Predicted 3 <start> un uomo uomo ragazzi corpo corpo vicine maneggiano getta inclina impedire punta.un installa zona monopolio gratta feriti dipinte <end> musicali giocavano usano alzando piuma arancioni.un accendono rampe rampa.una grigliato cream miami esaminare terminale scrivania intorno.un softball.un gruppo.un africane drenaggio drenaggio drenaggio armato <end> lavoratori agricolo femmina esotici <end> sull'autobus l'azione biglietti scrivania intorno.un softball.un gruppo.un africane drenaggio drenaggio drenaggio\n",
      "Image_idx 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-518d3136d0db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m#print(\"Original\", [IDX2WORD[i] for i in captions_batch)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m#print(\"Cap\", [IDX2WORD[int(i)] for i in captions_batch[0]])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mimg_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m#img_features = img_features.view(-1)[torch.randperm(img_features.nelement())].view(img_features.size())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-79-6981cf76a9e3>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresnet50\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;31m#print(\"Resnet module op\", x.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    414\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    415\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[1;32m--> 416\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if platform == \"colab\":\n",
    "    IMAGE_DIR_TEST = '/content/drive/My Drive/train/'\n",
    "else:\n",
    "    IMAGE_DIR_TEST = 'D:/Padhai/IIT Delhi MS(R)/2019-20 Sem II/COL774 Machine Learning/Assignment/Assignment4/private_test_images/'\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "\n",
    "if restore == True:\n",
    "    net = ImageCaptionsNet()\n",
    "    net = net.double()\n",
    "    state_dict = collections.OrderedDict()\n",
    "    state_dict = restore_checkpoint(\"chkpt_finaltry_TOKEN.pth\")\n",
    "    net = ImageCaptionsNet()\n",
    "    net = net.double()\n",
    "    net.load_state_dict(state_dict)\n",
    "    print(\"State Dictionary Loaded Successfully.\")\n",
    "    \n",
    "    \n",
    "images_names = glob(IMAGE_DIR_TEST+\"*.jpg\")\n",
    "print(len(images_names))\n",
    "images_names = [os.path.split(i)[-1][:-4] for i in images_names]\n",
    "print(images_names[:5])\n",
    "images_names = [i.split(\"_\")[-1] for i in images_names]\n",
    "print(images_names[0])\n",
    "\n",
    "test_dataset = ImageCaptionsDataset(\n",
    "    IMAGE_DIR_TEST, captions_preprocessing_obj.captions_dict, img_transform=img_transform,\n",
    "    captions_transform=captions_preprocessing_obj.captions_transform\n",
    ")\n",
    "test_dataset.image_ids = images_names\n",
    "\n",
    "NUM_WORKERS = 0 \n",
    "MAX_WORDS = 35\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=NUM_WORKERS, collate_fn=custom_batch)\n",
    "\n",
    "if device != \"cpu\":\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "t0 = time()\n",
    "pred_caps = {}\n",
    "for batch_idx, sample in enumerate(test_loader):\n",
    "        print(\"Image_idx\", batch_idx)\n",
    "        #image_batch = sample['image']\n",
    "        #print(\"AFTER\", image_batch)\n",
    "        #print(\"Original\", [IDX2WORD[i] for i in captions_batch)\n",
    "        #print(\"Cap\", [IDX2WORD[int(i)] for i in captions_batch[0]])\n",
    "        img_features = net.Encoder(image_batch)\n",
    "        print(img_features)\n",
    "        #img_features = img_features.view(-1)[torch.randperm(img_features.nelement())].view(img_features.size())\n",
    "        #img_features = torch.FloatTensor(np.random.randn(1,300))\n",
    "        #print(img_features[0][:4].tolist(), img_features[0][-5:].tolist())\n",
    "        #print(x.shape)\n",
    "        #pred_cap = beam_search(img_features)\n",
    "        pred_cap = caption_image(img_features, 60)\n",
    "        \n",
    "        pred_caps[batch_idx] = pred_cap\n",
    "        print(\"Predicted\",batch_idx, pred_cap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W9LV14UBX1NC"
   },
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kOPtPxyTEjTw",
    "outputId": "451c89bc-0aef-491e-dcbc-60676b6d159a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBED DIM 200\n",
      "resnet50 Loaded Successfully..!\n",
      "VOCAB SIZE =  8680\n",
      "Iterations = 100, Epoch = 1\n",
      "EMBED DIM 200\n",
      "resnet50 Loaded Successfully..!\n",
      "VOCAB SIZE =  8680\n",
      "State Dictionary Loaded Successfully.\n",
      "1000\n",
      "['image_10005', 'image_10010', 'image_10088', 'image_10097', 'image_10106']\n",
      "10005\n",
      "Image_idx 0\n",
      "tensor([[-7.9731e-05,  3.2992e+00,  6.7773e+00,  1.6855e+00, -3.1742e-04,\n",
      "         -1.1180e-05,  4.9456e+00, -1.2853e-03,  5.7048e+00, -1.5602e-03,\n",
      "          4.9112e+00,  1.1732e-02,  1.1889e+00,  1.3476e+00,  3.7415e+00,\n",
      "         -2.3463e-03,  3.2826e+00, -1.7007e-03,  1.0514e+00,  8.6016e+00,\n",
      "         -1.3218e-03,  7.0165e-01,  2.7890e+00, -2.3239e-04, -3.1168e-04,\n",
      "          5.2710e-01,  2.8796e+00,  2.5961e+00,  2.0203e+00,  6.1750e-01,\n",
      "         -1.2911e-05,  1.8112e+00,  5.2715e+00, -1.6493e-03, -1.5302e-03,\n",
      "         -1.1092e-03,  6.9307e+00,  5.1040e+00,  4.1580e+00,  9.3736e-01,\n",
      "          5.1366e+00,  9.1397e-01,  3.8016e-01,  4.1835e+00, -1.9223e-04,\n",
      "          1.8982e+00,  9.8100e-02,  3.6639e+00, -5.3956e-04, -6.7262e-04,\n",
      "         -9.5458e-04,  1.5451e+00, -1.8282e-04,  3.8324e+00,  2.8794e+00,\n",
      "         -1.8036e-04,  4.7607e+00,  3.6243e-02, -2.8659e-04,  4.0475e-01,\n",
      "          4.6295e-01,  2.6002e+00,  1.0149e+00,  3.5872e+00,  5.1855e+00,\n",
      "          3.4883e+00,  3.9841e+00,  6.7841e+00,  3.6788e-01,  2.1977e+00,\n",
      "          5.3449e+00,  2.2378e-02,  5.0575e+00, -4.9228e-04,  3.1894e+00,\n",
      "         -1.6852e-03, -5.9102e-04, -1.7328e-03,  2.1925e+00,  3.9802e+00,\n",
      "         -2.3360e-04,  1.7152e-02,  1.9258e+00,  6.1379e+00, -1.2164e-03,\n",
      "          9.3698e+00, -2.0076e-03, -1.7338e-04, -5.1589e-04, -1.0734e-04,\n",
      "         -3.4890e-04, -9.0368e-04,  3.9141e+00,  7.2832e+00,  1.3591e+00,\n",
      "         -1.1568e-04,  8.8408e-02, -1.4066e-04,  4.3654e+00,  9.0558e+00,\n",
      "         -7.4516e-04,  1.9240e-02,  7.0664e-02, -4.1537e-03, -7.9159e-04,\n",
      "         -5.4655e-04, -2.9238e-03, -1.0791e-03,  2.2588e+00,  1.5580e+00,\n",
      "          3.1236e+00,  3.5690e-01,  1.9532e-02,  9.8492e-01,  1.4191e-01,\n",
      "          3.3212e-01, -3.0321e-04,  4.3640e+00,  3.5537e+00,  4.4367e+00,\n",
      "          3.2272e+00,  4.3677e+00,  4.9903e+00,  1.3655e+00,  1.1208e-01,\n",
      "         -9.8344e-04,  6.3449e-01,  1.0204e+00, -7.7756e-04,  1.2921e+00,\n",
      "          3.7249e+00, -3.9829e-04,  1.1000e+00, -7.3818e-05,  1.2296e+00,\n",
      "          2.5260e+00,  1.5465e+00,  3.0071e+00, -1.2093e-03,  6.3883e-01,\n",
      "         -1.9283e-04,  1.9180e+00,  4.5892e+00, -2.1196e-04, -3.4200e-04,\n",
      "          1.8753e+00,  4.6867e+00,  2.6001e+00, -2.6745e-03, -1.7481e-03,\n",
      "         -1.4857e-03, -1.9778e-03,  1.9455e+00,  1.4034e-02,  3.6665e+00,\n",
      "          4.6494e+00, -3.0196e-03, -1.2013e-03,  4.1916e-01, -1.3436e-04,\n",
      "          1.2418e-01,  8.4034e-01,  2.4067e+00,  5.9003e+00,  8.7377e-01,\n",
      "         -8.4834e-04, -5.1656e-04,  3.6284e-01,  4.2473e+00, -1.6115e-03,\n",
      "          2.5214e+00,  5.7289e-01, -8.2537e-05, -4.2944e-04,  5.1488e-01,\n",
      "          5.1356e+00,  3.0253e+00,  2.4671e+00,  3.8719e-01,  9.9440e-01,\n",
      "          2.4183e+00,  2.1945e+00,  1.2699e+00, -4.4481e-04,  1.2043e+00,\n",
      "          2.1488e+00, -1.7932e-03,  1.6909e+00,  2.1408e+00,  4.9911e+00,\n",
      "          5.7150e-01, -1.3487e-03,  3.2290e+00,  4.5034e+00, -7.9549e-04,\n",
      "          4.0453e+00, -2.9959e-03, -6.3273e-06,  3.5886e+00, -3.8934e-04]],\n",
      "       dtype=torch.float64, grad_fn=<LeakyReluBackward0>)\n",
      "Predicted 0 <start> un uomo con un uomo con un uomo con un uomo con un uomo con un uomo con un uomo con un uomo con un uomo . <end> . <end> <end> . <end> . <end> <end> . <end> <end> . <end> . <end> <end> . <end> <end> . <end> . <end> <end> . <end> <end> . <end> . <end>\n",
      "Image_idx 1\n",
      "tensor([[-7.9731e-05,  3.2992e+00,  6.7773e+00,  1.6855e+00, -3.1742e-04,\n",
      "         -1.1180e-05,  4.9456e+00, -1.2853e-03,  5.7048e+00, -1.5602e-03,\n",
      "          4.9112e+00,  1.1732e-02,  1.1889e+00,  1.3476e+00,  3.7415e+00,\n",
      "         -2.3463e-03,  3.2826e+00, -1.7007e-03,  1.0514e+00,  8.6016e+00,\n",
      "         -1.3218e-03,  7.0165e-01,  2.7890e+00, -2.3239e-04, -3.1168e-04,\n",
      "          5.2710e-01,  2.8796e+00,  2.5961e+00,  2.0203e+00,  6.1750e-01,\n",
      "         -1.2911e-05,  1.8112e+00,  5.2715e+00, -1.6493e-03, -1.5302e-03,\n",
      "         -1.1092e-03,  6.9307e+00,  5.1040e+00,  4.1580e+00,  9.3736e-01,\n",
      "          5.1366e+00,  9.1397e-01,  3.8016e-01,  4.1835e+00, -1.9223e-04,\n",
      "          1.8982e+00,  9.8100e-02,  3.6639e+00, -5.3956e-04, -6.7262e-04,\n",
      "         -9.5458e-04,  1.5451e+00, -1.8282e-04,  3.8324e+00,  2.8794e+00,\n",
      "         -1.8036e-04,  4.7607e+00,  3.6243e-02, -2.8659e-04,  4.0475e-01,\n",
      "          4.6295e-01,  2.6002e+00,  1.0149e+00,  3.5872e+00,  5.1855e+00,\n",
      "          3.4883e+00,  3.9841e+00,  6.7841e+00,  3.6788e-01,  2.1977e+00,\n",
      "          5.3449e+00,  2.2378e-02,  5.0575e+00, -4.9228e-04,  3.1894e+00,\n",
      "         -1.6852e-03, -5.9102e-04, -1.7328e-03,  2.1925e+00,  3.9802e+00,\n",
      "         -2.3360e-04,  1.7152e-02,  1.9258e+00,  6.1379e+00, -1.2164e-03,\n",
      "          9.3698e+00, -2.0076e-03, -1.7338e-04, -5.1589e-04, -1.0734e-04,\n",
      "         -3.4890e-04, -9.0368e-04,  3.9141e+00,  7.2832e+00,  1.3591e+00,\n",
      "         -1.1568e-04,  8.8408e-02, -1.4066e-04,  4.3654e+00,  9.0558e+00,\n",
      "         -7.4516e-04,  1.9240e-02,  7.0664e-02, -4.1537e-03, -7.9159e-04,\n",
      "         -5.4655e-04, -2.9238e-03, -1.0791e-03,  2.2588e+00,  1.5580e+00,\n",
      "          3.1236e+00,  3.5690e-01,  1.9532e-02,  9.8492e-01,  1.4191e-01,\n",
      "          3.3212e-01, -3.0321e-04,  4.3640e+00,  3.5537e+00,  4.4367e+00,\n",
      "          3.2272e+00,  4.3677e+00,  4.9903e+00,  1.3655e+00,  1.1208e-01,\n",
      "         -9.8344e-04,  6.3449e-01,  1.0204e+00, -7.7756e-04,  1.2921e+00,\n",
      "          3.7249e+00, -3.9829e-04,  1.1000e+00, -7.3818e-05,  1.2296e+00,\n",
      "          2.5260e+00,  1.5465e+00,  3.0071e+00, -1.2093e-03,  6.3883e-01,\n",
      "         -1.9283e-04,  1.9180e+00,  4.5892e+00, -2.1196e-04, -3.4200e-04,\n",
      "          1.8753e+00,  4.6867e+00,  2.6001e+00, -2.6745e-03, -1.7481e-03,\n",
      "         -1.4857e-03, -1.9778e-03,  1.9455e+00,  1.4034e-02,  3.6665e+00,\n",
      "          4.6494e+00, -3.0196e-03, -1.2013e-03,  4.1916e-01, -1.3436e-04,\n",
      "          1.2418e-01,  8.4034e-01,  2.4067e+00,  5.9003e+00,  8.7377e-01,\n",
      "         -8.4834e-04, -5.1656e-04,  3.6284e-01,  4.2473e+00, -1.6115e-03,\n",
      "          2.5214e+00,  5.7289e-01, -8.2537e-05, -4.2944e-04,  5.1488e-01,\n",
      "          5.1356e+00,  3.0253e+00,  2.4671e+00,  3.8719e-01,  9.9440e-01,\n",
      "          2.4183e+00,  2.1945e+00,  1.2699e+00, -4.4481e-04,  1.2043e+00,\n",
      "          2.1488e+00, -1.7932e-03,  1.6909e+00,  2.1408e+00,  4.9911e+00,\n",
      "          5.7150e-01, -1.3487e-03,  3.2290e+00,  4.5034e+00, -7.9549e-04,\n",
      "          4.0453e+00, -2.9959e-03, -6.3273e-06,  3.5886e+00, -3.8934e-04]],\n",
      "       dtype=torch.float64, grad_fn=<LeakyReluBackward0>)\n",
      "Predicted 1 <start> un uomo con un uomo con un uomo con un uomo con un uomo con un uomo con un uomo con un uomo con un uomo . <end> . <end> <end> . <end> . <end> <end> . <end> <end> . <end> . <end> <end> . <end> <end> . <end> . <end> <end> . <end> <end> . <end> . <end>\n",
      "Image_idx 2\n",
      "tensor([[-7.9731e-05,  3.2992e+00,  6.7773e+00,  1.6855e+00, -3.1742e-04,\n",
      "         -1.1180e-05,  4.9456e+00, -1.2853e-03,  5.7048e+00, -1.5602e-03,\n",
      "          4.9112e+00,  1.1732e-02,  1.1889e+00,  1.3476e+00,  3.7415e+00,\n",
      "         -2.3463e-03,  3.2826e+00, -1.7007e-03,  1.0514e+00,  8.6016e+00,\n",
      "         -1.3218e-03,  7.0165e-01,  2.7890e+00, -2.3239e-04, -3.1168e-04,\n",
      "          5.2710e-01,  2.8796e+00,  2.5961e+00,  2.0203e+00,  6.1750e-01,\n",
      "         -1.2911e-05,  1.8112e+00,  5.2715e+00, -1.6493e-03, -1.5302e-03,\n",
      "         -1.1092e-03,  6.9307e+00,  5.1040e+00,  4.1580e+00,  9.3736e-01,\n",
      "          5.1366e+00,  9.1397e-01,  3.8016e-01,  4.1835e+00, -1.9223e-04,\n",
      "          1.8982e+00,  9.8100e-02,  3.6639e+00, -5.3956e-04, -6.7262e-04,\n",
      "         -9.5458e-04,  1.5451e+00, -1.8282e-04,  3.8324e+00,  2.8794e+00,\n",
      "         -1.8036e-04,  4.7607e+00,  3.6243e-02, -2.8659e-04,  4.0475e-01,\n",
      "          4.6295e-01,  2.6002e+00,  1.0149e+00,  3.5872e+00,  5.1855e+00,\n",
      "          3.4883e+00,  3.9841e+00,  6.7841e+00,  3.6788e-01,  2.1977e+00,\n",
      "          5.3449e+00,  2.2378e-02,  5.0575e+00, -4.9228e-04,  3.1894e+00,\n",
      "         -1.6852e-03, -5.9102e-04, -1.7328e-03,  2.1925e+00,  3.9802e+00,\n",
      "         -2.3360e-04,  1.7152e-02,  1.9258e+00,  6.1379e+00, -1.2164e-03,\n",
      "          9.3698e+00, -2.0076e-03, -1.7338e-04, -5.1589e-04, -1.0734e-04,\n",
      "         -3.4890e-04, -9.0368e-04,  3.9141e+00,  7.2832e+00,  1.3591e+00,\n",
      "         -1.1568e-04,  8.8408e-02, -1.4066e-04,  4.3654e+00,  9.0558e+00,\n",
      "         -7.4516e-04,  1.9240e-02,  7.0664e-02, -4.1537e-03, -7.9159e-04,\n",
      "         -5.4655e-04, -2.9238e-03, -1.0791e-03,  2.2588e+00,  1.5580e+00,\n",
      "          3.1236e+00,  3.5690e-01,  1.9532e-02,  9.8492e-01,  1.4191e-01,\n",
      "          3.3212e-01, -3.0321e-04,  4.3640e+00,  3.5537e+00,  4.4367e+00,\n",
      "          3.2272e+00,  4.3677e+00,  4.9903e+00,  1.3655e+00,  1.1208e-01,\n",
      "         -9.8344e-04,  6.3449e-01,  1.0204e+00, -7.7756e-04,  1.2921e+00,\n",
      "          3.7249e+00, -3.9829e-04,  1.1000e+00, -7.3818e-05,  1.2296e+00,\n",
      "          2.5260e+00,  1.5465e+00,  3.0071e+00, -1.2093e-03,  6.3883e-01,\n",
      "         -1.9283e-04,  1.9180e+00,  4.5892e+00, -2.1196e-04, -3.4200e-04,\n",
      "          1.8753e+00,  4.6867e+00,  2.6001e+00, -2.6745e-03, -1.7481e-03,\n",
      "         -1.4857e-03, -1.9778e-03,  1.9455e+00,  1.4034e-02,  3.6665e+00,\n",
      "          4.6494e+00, -3.0196e-03, -1.2013e-03,  4.1916e-01, -1.3436e-04,\n",
      "          1.2418e-01,  8.4034e-01,  2.4067e+00,  5.9003e+00,  8.7377e-01,\n",
      "         -8.4834e-04, -5.1656e-04,  3.6284e-01,  4.2473e+00, -1.6115e-03,\n",
      "          2.5214e+00,  5.7289e-01, -8.2537e-05, -4.2944e-04,  5.1488e-01,\n",
      "          5.1356e+00,  3.0253e+00,  2.4671e+00,  3.8719e-01,  9.9440e-01,\n",
      "          2.4183e+00,  2.1945e+00,  1.2699e+00, -4.4481e-04,  1.2043e+00,\n",
      "          2.1488e+00, -1.7932e-03,  1.6909e+00,  2.1408e+00,  4.9911e+00,\n",
      "          5.7150e-01, -1.3487e-03,  3.2290e+00,  4.5034e+00, -7.9549e-04,\n",
      "          4.0453e+00, -2.9959e-03, -6.3273e-06,  3.5886e+00, -3.8934e-04]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       dtype=torch.float64, grad_fn=<LeakyReluBackward0>)\n",
      "Predicted 2 <start> un uomo con un uomo con un uomo con un uomo con un uomo con un uomo con un uomo con un uomo con un uomo . <end> . <end> <end> . <end> . <end> <end> . <end> <end> . <end> . <end> <end> . <end> <end> . <end> . <end> <end> . <end> <end> . <end> . <end>\n",
      "Image_idx 3\n",
      "tensor([[-7.9731e-05,  3.2992e+00,  6.7773e+00,  1.6855e+00, -3.1742e-04,\n",
      "         -1.1180e-05,  4.9456e+00, -1.2853e-03,  5.7048e+00, -1.5602e-03,\n",
      "          4.9112e+00,  1.1732e-02,  1.1889e+00,  1.3476e+00,  3.7415e+00,\n",
      "         -2.3463e-03,  3.2826e+00, -1.7007e-03,  1.0514e+00,  8.6016e+00,\n",
      "         -1.3218e-03,  7.0165e-01,  2.7890e+00, -2.3239e-04, -3.1168e-04,\n",
      "          5.2710e-01,  2.8796e+00,  2.5961e+00,  2.0203e+00,  6.1750e-01,\n",
      "         -1.2911e-05,  1.8112e+00,  5.2715e+00, -1.6493e-03, -1.5302e-03,\n",
      "         -1.1092e-03,  6.9307e+00,  5.1040e+00,  4.1580e+00,  9.3736e-01,\n",
      "          5.1366e+00,  9.1397e-01,  3.8016e-01,  4.1835e+00, -1.9223e-04,\n",
      "          1.8982e+00,  9.8100e-02,  3.6639e+00, -5.3956e-04, -6.7262e-04,\n",
      "         -9.5458e-04,  1.5451e+00, -1.8282e-04,  3.8324e+00,  2.8794e+00,\n",
      "         -1.8036e-04,  4.7607e+00,  3.6243e-02, -2.8659e-04,  4.0475e-01,\n",
      "          4.6295e-01,  2.6002e+00,  1.0149e+00,  3.5872e+00,  5.1855e+00,\n",
      "          3.4883e+00,  3.9841e+00,  6.7841e+00,  3.6788e-01,  2.1977e+00,\n",
      "          5.3449e+00,  2.2378e-02,  5.0575e+00, -4.9228e-04,  3.1894e+00,\n",
      "         -1.6852e-03, -5.9102e-04, -1.7328e-03,  2.1925e+00,  3.9802e+00,\n",
      "         -2.3360e-04,  1.7152e-02,  1.9258e+00,  6.1379e+00, -1.2164e-03,\n",
      "          9.3698e+00, -2.0076e-03, -1.7338e-04, -5.1589e-04, -1.0734e-04,\n",
      "         -3.4890e-04, -9.0368e-04,  3.9141e+00,  7.2832e+00,  1.3591e+00,\n",
      "         -1.1568e-04,  8.8408e-02, -1.4066e-04,  4.3654e+00,  9.0558e+00,\n",
      "         -7.4516e-04,  1.9240e-02,  7.0664e-02, -4.1537e-03, -7.9159e-04,\n",
      "         -5.4655e-04, -2.9238e-03, -1.0791e-03,  2.2588e+00,  1.5580e+00,\n",
      "          3.1236e+00,  3.5690e-01,  1.9532e-02,  9.8492e-01,  1.4191e-01,\n",
      "          3.3212e-01, -3.0321e-04,  4.3640e+00,  3.5537e+00,  4.4367e+00,\n",
      "          3.2272e+00,  4.3677e+00,  4.9903e+00,  1.3655e+00,  1.1208e-01,\n",
      "         -9.8344e-04,  6.3449e-01,  1.0204e+00, -7.7756e-04,  1.2921e+00,\n",
      "          3.7249e+00, -3.9829e-04,  1.1000e+00, -7.3818e-05,  1.2296e+00,\n",
      "          2.5260e+00,  1.5465e+00,  3.0071e+00, -1.2093e-03,  6.3883e-01,\n",
      "         -1.9283e-04,  1.9180e+00,  4.5892e+00, -2.1196e-04, -3.4200e-04,\n",
      "          1.8753e+00,  4.6867e+00,  2.6001e+00, -2.6745e-03, -1.7481e-03,\n",
      "         -1.4857e-03, -1.9778e-03,  1.9455e+00,  1.4034e-02,  3.6665e+00,\n",
      "          4.6494e+00, -3.0196e-03, -1.2013e-03,  4.1916e-01, -1.3436e-04,\n",
      "          1.2418e-01,  8.4034e-01,  2.4067e+00,  5.9003e+00,  8.7377e-01,\n",
      "         -8.4834e-04, -5.1656e-04,  3.6284e-01,  4.2473e+00, -1.6115e-03,\n",
      "          2.5214e+00,  5.7289e-01, -8.2537e-05, -4.2944e-04,  5.1488e-01,\n",
      "          5.1356e+00,  3.0253e+00,  2.4671e+00,  3.8719e-01,  9.9440e-01,\n",
      "          2.4183e+00,  2.1945e+00,  1.2699e+00, -4.4481e-04,  1.2043e+00,\n",
      "          2.1488e+00, -1.7932e-03,  1.6909e+00,  2.1408e+00,  4.9911e+00,\n",
      "          5.7150e-01, -1.3487e-03,  3.2290e+00,  4.5034e+00, -7.9549e-04,\n",
      "          4.0453e+00, -2.9959e-03, -6.3273e-06,  3.5886e+00, -3.8934e-04]],\n",
      "       dtype=torch.float64, grad_fn=<LeakyReluBackward0>)\n",
      "Predicted 3 <start> un uomo con un uomo con un uomo con un uomo con un uomo con un uomo con un uomo con un uomo con un uomo . <end> . <end> <end> . <end> . <end> <end> . <end> <end> . <end> . <end> <end> . <end> <end> . <end> . <end> <end> . <end> <end> . <end> . <end>\n",
      "Image_idx 4\n",
      "tensor([[-7.9731e-05,  3.2992e+00,  6.7773e+00,  1.6855e+00, -3.1742e-04,\n",
      "         -1.1180e-05,  4.9456e+00, -1.2853e-03,  5.7048e+00, -1.5602e-03,\n",
      "          4.9112e+00,  1.1732e-02,  1.1889e+00,  1.3476e+00,  3.7415e+00,\n",
      "         -2.3463e-03,  3.2826e+00, -1.7007e-03,  1.0514e+00,  8.6016e+00,\n",
      "         -1.3218e-03,  7.0165e-01,  2.7890e+00, -2.3239e-04, -3.1168e-04,\n",
      "          5.2710e-01,  2.8796e+00,  2.5961e+00,  2.0203e+00,  6.1750e-01,\n",
      "         -1.2911e-05,  1.8112e+00,  5.2715e+00, -1.6493e-03, -1.5302e-03,\n",
      "         -1.1092e-03,  6.9307e+00,  5.1040e+00,  4.1580e+00,  9.3736e-01,\n",
      "          5.1366e+00,  9.1397e-01,  3.8016e-01,  4.1835e+00, -1.9223e-04,\n",
      "          1.8982e+00,  9.8100e-02,  3.6639e+00, -5.3956e-04, -6.7262e-04,\n",
      "         -9.5458e-04,  1.5451e+00, -1.8282e-04,  3.8324e+00,  2.8794e+00,\n",
      "         -1.8036e-04,  4.7607e+00,  3.6243e-02, -2.8659e-04,  4.0475e-01,\n",
      "          4.6295e-01,  2.6002e+00,  1.0149e+00,  3.5872e+00,  5.1855e+00,\n",
      "          3.4883e+00,  3.9841e+00,  6.7841e+00,  3.6788e-01,  2.1977e+00,\n",
      "          5.3449e+00,  2.2378e-02,  5.0575e+00, -4.9228e-04,  3.1894e+00,\n",
      "         -1.6852e-03, -5.9102e-04, -1.7328e-03,  2.1925e+00,  3.9802e+00,\n",
      "         -2.3360e-04,  1.7152e-02,  1.9258e+00,  6.1379e+00, -1.2164e-03,\n",
      "          9.3698e+00, -2.0076e-03, -1.7338e-04, -5.1589e-04, -1.0734e-04,\n",
      "         -3.4890e-04, -9.0368e-04,  3.9141e+00,  7.2832e+00,  1.3591e+00,\n",
      "         -1.1568e-04,  8.8408e-02, -1.4066e-04,  4.3654e+00,  9.0558e+00,\n",
      "         -7.4516e-04,  1.9240e-02,  7.0664e-02, -4.1537e-03, -7.9159e-04,\n",
      "         -5.4655e-04, -2.9238e-03, -1.0791e-03,  2.2588e+00,  1.5580e+00,\n",
      "          3.1236e+00,  3.5690e-01,  1.9532e-02,  9.8492e-01,  1.4191e-01,\n",
      "          3.3212e-01, -3.0321e-04,  4.3640e+00,  3.5537e+00,  4.4367e+00,\n",
      "          3.2272e+00,  4.3677e+00,  4.9903e+00,  1.3655e+00,  1.1208e-01,\n",
      "         -9.8344e-04,  6.3449e-01,  1.0204e+00, -7.7756e-04,  1.2921e+00,\n",
      "          3.7249e+00, -3.9829e-04,  1.1000e+00, -7.3818e-05,  1.2296e+00,\n",
      "          2.5260e+00,  1.5465e+00,  3.0071e+00, -1.2093e-03,  6.3883e-01,\n",
      "         -1.9283e-04,  1.9180e+00,  4.5892e+00, -2.1196e-04, -3.4200e-04,\n",
      "          1.8753e+00,  4.6867e+00,  2.6001e+00, -2.6745e-03, -1.7481e-03,\n",
      "         -1.4857e-03, -1.9778e-03,  1.9455e+00,  1.4034e-02,  3.6665e+00,\n",
      "          4.6494e+00, -3.0196e-03, -1.2013e-03,  4.1916e-01, -1.3436e-04,\n",
      "          1.2418e-01,  8.4034e-01,  2.4067e+00,  5.9003e+00,  8.7377e-01,\n",
      "         -8.4834e-04, -5.1656e-04,  3.6284e-01,  4.2473e+00, -1.6115e-03,\n",
      "          2.5214e+00,  5.7289e-01, -8.2537e-05, -4.2944e-04,  5.1488e-01,\n",
      "          5.1356e+00,  3.0253e+00,  2.4671e+00,  3.8719e-01,  9.9440e-01,\n",
      "          2.4183e+00,  2.1945e+00,  1.2699e+00, -4.4481e-04,  1.2043e+00,\n",
      "          2.1488e+00, -1.7932e-03,  1.6909e+00,  2.1408e+00,  4.9911e+00,\n",
      "          5.7150e-01, -1.3487e-03,  3.2290e+00,  4.5034e+00, -7.9549e-04,\n",
      "          4.0453e+00, -2.9959e-03, -6.3273e-06,  3.5886e+00, -3.8934e-04]],\n",
      "       dtype=torch.float64, grad_fn=<LeakyReluBackward0>)\n",
      "Predicted 4 <start> un uomo con un uomo con un uomo con un uomo con un uomo con un uomo con un uomo con un uomo con un uomo . <end> . <end> <end> . <end> . <end> <end> . <end> <end> . <end> . <end> <end> . <end> <end> . <end> . <end> <end> . <end> <end> . <end> . <end>\n",
      "Image_idx 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-e5072dbc250c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m#print(\"Original\", [IDX2WORD[i] for i in captions_batch)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;31m#print(\"Cap\", [IDX2WORD[int(i)] for i in captions_batch[0]])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mimg_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;31m#img_features = img_features.view(-1)[torch.randperm(img_features.nelement())].view(img_features.size())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-79-6981cf76a9e3>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresnet50\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;31m#print(\"Resnet module op\", x.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    414\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    415\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[1;32m--> 416\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if platform == \"colab\":\n",
    "    IMAGE_DIR_TEST = '/content/drive/My Drive/train/'\n",
    "else:\n",
    "    IMAGE_DIR_TEST = 'D:/Padhai/IIT Delhi MS(R)/2019-20 Sem II/COL774 Machine Learning/Assignment/Assignment4/private_test_images/'\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "if restore == True:\n",
    "    if net:\n",
    "        del net\n",
    "    net = ImageCaptionsNet()\n",
    "    net = net.double()\n",
    "    state_dict = collections.OrderedDict()\n",
    "    state_dict = restore_checkpoint(\"chkpt_finaltry_TOKEN_0.01.pth\")\n",
    "    net = ImageCaptionsNet()\n",
    "    net = net.double()\n",
    "    net.load_state_dict(state_dict)\n",
    "    print(\"State Dictionary Loaded Successfully.\")\n",
    "\n",
    "images_names = glob(IMAGE_DIR_TEST+\"*.jpg\")\n",
    "print(len(images_names))\n",
    "images_names = [os.path.split(i)[-1][:-4] for i in images_names]\n",
    "print(images_names[:5])\n",
    "images_names = [i.split(\"_\")[-1] for i in images_names]\n",
    "print(images_names[0])\n",
    "\n",
    "\n",
    "# Creating the Dataset\n",
    "test_dataset = ImageCaptionsDataset(\n",
    "    IMAGE_DIR_TEST, captions_preprocessing_obj.captions_dict, img_transform=img_transform,\n",
    "    captions_transform=captions_preprocessing_obj.captions_transform\n",
    ")\n",
    "test_dataset.image_ids = images_names\n",
    "\n",
    "#print(len(img_ids))\n",
    "NUM_WORKERS = 0 # Parallel threads for dataloading\n",
    "MAX_WORDS = 35\n",
    "# Creating the DataLoader for batching purposes\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=NUM_WORKERS, collate_fn=custom_batch)\n",
    "\n",
    "if device != \"cpu\":\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "t0 = time()\n",
    "pred_caps = {}\n",
    "for batch_idx, sample in enumerate(test_loader):\n",
    "        print(\"Image_idx\", batch_idx)\n",
    "        #image_batch = sample['image']\n",
    "        #print(\"AFTER\", image_batch)\n",
    "        #print(\"Original\", [IDX2WORD[i] for i in captions_batch)\n",
    "        #print(\"Cap\", [IDX2WORD[int(i)] for i in captions_batch[0]])\n",
    "        img_features = net.Encoder(image_batch)\n",
    "        print(img_features)\n",
    "        #img_features = img_features.view(-1)[torch.randperm(img_features.nelement())].view(img_features.size())\n",
    "        #img_features = torch.FloatTensor(np.random.randn(1,300))\n",
    "        #print(img_features[0][:4].tolist(), img_features[0][-5:].tolist())\n",
    "        #print(x.shape)\n",
    "        #pred_cap = beam_search(img_features)\n",
    "        pred_cap = caption_image(img_features, 60)\n",
    "        \n",
    "        pred_caps[batch_idx] = pred_cap\n",
    "        print(\"Predicted\",batch_idx, pred_cap.replace(\"<unk>\", \"*\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "op_str = \"blu.tre grillare trekking sinistra.una valigie pista.una tastiera estrarre jumpsuit azienda metropolitana.un accanto bilanciarsi coraggioso free canon canon dama canon dama dama 100 comodamente casco rocce accanto.una neve.il birra.una accanto.una accanto.una viso bassi mezz'aria.una sollevati dell'azienda lanciarsi sollevati letto.una afroamericana personale controllato birra.una orgoglioso carro l'esecuzione bandiera.un momenti l'operaio portapranzo pausa ferrovia.un l'attività pasta miei decide hockey.una addormentato cantava cubo fatta.\"\n",
    "op_str = \"un.uomo in camicia rossa e un gilet blu e una donna.\"\n",
    "op_str = [op_str]*len(images_names)\n",
    "\n",
    "op_dict = dict(zip(images_names, op_str))\n",
    "df = pd.DataFrame.from_dict(op_dict, orient='index', columns=None)\n",
    "df.to_csv( \"../2019SIY7580_2019CSZ8763/2019SIY7580_2019CSZ8763_public.tsv\", sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0044,  0.0037,  0.0051,  ...,  0.0081,  0.0126, -0.0021],\n",
       "        [ 0.0051, -0.0143,  0.0178,  ..., -0.0196,  0.0201,  0.0193],\n",
       "        [ 0.0086, -0.0086,  0.0099,  ...,  0.0163,  0.0085, -0.0196],\n",
       "        ...,\n",
       "        [-0.0101,  0.0161,  0.0168,  ..., -0.0196, -0.0078, -0.0193],\n",
       "        [-0.0136, -0.0130, -0.0190,  ...,  0.0151, -0.0174, -0.0115],\n",
       "        [-0.0103,  0.0192,  0.0010,  ..., -0.0193, -0.0207,  0.0131]],\n",
       "       dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.Encoder.fc.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fkfskWPHhtAi"
   },
   "source": [
    "### TRAIN LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "RA1o2PSgBfKF",
    "outputId": "d2b12bc8-ee6b-410e-acf6-02e19afefa08",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBED DIM 200\n",
      "resnet50 Loaded Successfully..!\n",
      "VOCAB SIZE =  8680\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../bkp_final_try/caption_chkpt_multi.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-f6fc73298c78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mnew_state_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mnew_state_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrestore_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"caption_chkpt_multi.pth\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"State Dictionary Loaded Successfully.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-81-ae220dfa03fc>\u001b[0m in \u001b[0;36mrestore_checkpoint\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mdirectory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'../bkp_final_try/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 571\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    572\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m             \u001b[1;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../bkp_final_try/caption_chkpt_multi.pth'"
     ]
    }
   ],
   "source": [
    "if platform == \"colab\":\n",
    "    IMAGE_DIR = '/content/drive/My Drive/train_images/'\n",
    "else:\n",
    "    IMAGE_DIR = 'D:/Padhai/IIT Delhi MS(R)/2019-20 Sem II/COL774 Machine Learning/Assignment/Assignment4/train_images/'\n",
    "\n",
    "if restore == True:\n",
    "    net = ImageCaptionsNet()\n",
    "    net = net.double()\n",
    "    new_state_dict = collections.OrderedDict()\n",
    "    new_state_dict = restore_checkpoint(\"caption_chkpt_multi.pth\")    \n",
    "    \n",
    "    print(\"State Dictionary Loaded Successfully.\")\n",
    "    #net = nn.DataParallel(net)\n",
    "    net = net.to(torch.device(\"cuda:0\"))\n",
    "\n",
    "# Creating the Dataset\n",
    "train_dataset = ImageCaptionsDataset(\n",
    "    IMAGE_DIR, captions_preprocessing_obj.captions_dict, img_transform=img_transform,\n",
    "    captions_transform=captions_preprocessing_obj.captions_transform\n",
    ")\n",
    "\n",
    "# Define your hyperparameters\n",
    "NUMBER_OF_EPOCHS = 3\n",
    "LEARNING_RATE = 2e-3\n",
    "BATCH_SIZE = 1\n",
    "NUM_WORKERS = 0 # Parallel threads for dataloading\n",
    "\n",
    "'''cw = torch.ones(len(VOCAB), dtype=torch.double)\n",
    "cw[WORD2IDX[\"<pad>\"]] = 0\n",
    "cw = cw.to(torch.device(\"cuda:0\"))'''\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=WORD2IDX[\"<pad>\"])\n",
    "\n",
    "paramaters = list(net.Decoder.parameters()) + list(net.Encoder.fc.parameters())\n",
    "\n",
    "optimizer = optim.Adam(paramaters, lr=LEARNING_RATE,weight_decay=0.001)\n",
    "\n",
    "total_params = sum(p.numel() for p in net.parameters())\n",
    "trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "\n",
    "params_for_adam = sum(p.numel() for p in paramaters)\n",
    "print(\"TOTAL PARAMS: {}, TOTAL TRAINABLE PARAMS NET: {}, TOTAL ADAM PARAMS: {}\".format(total_params,trainable_params,params_for_adam))\n",
    "print(\"TOTAL EPOCHS: {}, BATCH SIZE: {}, OPTIMIZER: {}\".format(NUMBER_OF_EPOCHS, BATCH_SIZE, optimizer))\n",
    "loss_list = []\n",
    "# Creating the DataLoader for batching purposes\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS,\n",
    "                          collate_fn=custom_batch)\n",
    "\n",
    "if device != \"cpu\":\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    #torch.cuda.set_device(1)\n",
    "t0 = time()\n",
    "for epoch in range(NUMBER_OF_EPOCHS):\n",
    "    print(\"$$$$$----EPOCH {}----$$$$$$\".format(epoch+1))\n",
    "    iteration = 0\n",
    "    \n",
    "    '''if epoch == 1:\n",
    "        LEARNING_RATE = 8e-4\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = LEARNING_RATE\n",
    "        print(\"\\nLEARNING RATE =\", LEARNING_RATE, optimizer)\n",
    "    elif epoch == 2:\n",
    "        LEARNING_RATE = 5e-4\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = LEARNING_RATE\n",
    "        print(\"\\nLEARNING RATE =\", LEARNING_RATE, optimizer)'''\n",
    "    \n",
    "\n",
    "    for batch_idx, sample in enumerate(train_loader):\n",
    "        iteration +=1\n",
    "        if iteration%25 == 0:\n",
    "            LEARNING_RATE *= 0.94\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = LEARNING_RATE\n",
    "            print(\"\\nLEARNING RATE =\", LEARNING_RATE, optimizer)\n",
    "\n",
    "        #net.zero_grad()\n",
    "        \n",
    "        net.Encoder.zero_grad()\n",
    "        net.Decoder.zero_grad()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        image_batch, captions_batch, lengths = sample['image'], sample['captions'], sample['lengths']\n",
    "        #print(lengths)\n",
    "        #print(\"image_shape\", image_batch.shape)\n",
    "        #print(\"batch_shape\", captions_batch.shape)\n",
    "        \n",
    "        #print(\"MY CAP\", captions_batch)\n",
    "\n",
    "        # If GPU training required\n",
    "        if device != \"cpu\":\n",
    "          #print(\"cuda\")\n",
    "          image_batch, captions_batch = image_batch.to(torch.device(\"cuda:0\")), captions_batch.to(torch.device(\"cuda:0\"))\n",
    "        \n",
    "        output_captions = net(image_batch, captions_batch, lengths)\n",
    "        #ground_truth = pack_padded_sequence(captions_batch, lengths, batch_first=True, enforce_sorted=False)\n",
    "        #ground_truth = ground_truth[0]\n",
    "        #print(\"GT\", captions_batch.reshape(-1))\n",
    "        #print(\"size for loss\", output_captions.shape, captions_batch.shape)\n",
    "        #torch.Size([10, 26, 9934]) torch.Size([10, 26])\n",
    "        #print(\"BEFORE LOSS\", output_captions.shape, ground_truth.shape)\n",
    "        #loss = loss_function(output_captions, ground_truth)\n",
    "        loss = loss_function(output_captions.reshape(-1, output_captions.shape[2]), captions_batch.reshape(-1))\n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    " \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if iteration%25 == 0:\n",
    "            create_checkpoint(\"chkpt_finaltry_pad.pth\", net, optimizer, loss, iteration, epoch+1)\n",
    "        print(\"ITERATION:[{}/{}] | LOSS: {} | EPOCH = [{}/{}] | TIME ELAPSED ={}Mins\".format(iteration, round(29000/BATCH_SIZE)+1,\n",
    "              round(loss.item(), 6), epoch+1, NUMBER_OF_EPOCHS, round((time()-t0)/60,2)))\n",
    "    print(\"\\n$$Loss = {},EPOCH: [{}/{}]\\n\\n\".format(round(loss.item(), 6), epoch+1, NUMBER_OF_EPOCHS))\n",
    "    create_checkpoint(\"Epoch_finaltry_pad.pth\", net, optimizer, loss, iteration, epoch+1)\n",
    "\n",
    "create_checkpoint(\"Full_finaltry_pad.pth\", net, optimizer, loss, iteration, epoch+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "oOKS9DIUfkor",
    "outputId": "83f91197-22ae-4065-c02e-927f84829c3a"
   },
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uOXPnRtdv2E0"
   },
   "outputs": [],
   "source": [
    "reference = [['this', 'is', 'a', 'test'], ['this', 'is' 'test']]\n",
    "candidate = ['un', 'uomo', 'in', 'camicia', 'rossa', 'e', 'un', 'gilet', 'blu', 'e', 'una', 'donna', '.']\n",
    "score = sentence_bleu(reference, candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gpyvqp2LRSLY"
   },
   "outputs": [],
   "source": [
    "'''if platform == \"colab\":\n",
    "    embed_path = '/content/drive/My Drive/A4/embeddings/trained_embed.pkl'\n",
    "else:\n",
    "    embed_path = '../embeddings/trained_embed.pkl'\n",
    "with open(embed_path, 'rb') as handle:\n",
    "    vocab_dump = pickle.load(handle)\n",
    "\n",
    "init_weights = torch.randn(len(VOCAB), 300)\n",
    "idx = 0\n",
    "words = WORD2IDX.keys()\n",
    "for i in range(len(words)):\n",
    "    init_weights[i] = vocab_dump[IDX2WORD[i]]\n",
    "init_weights.shape'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nt32kDSARJG6"
   },
   "outputs": [],
   "source": [
    "models.resnet34()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H_KQSLLVX5DA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "guOc4YBptLX7"
   },
   "outputs": [],
   "source": [
    "pred = '<start> un.uomo in camicia rossa e un gilet blu e una donna , <unk> <unk> <unk> . <end>'\n",
    "#[VOCAB[i] for  i in pred.split(\" \")]\n",
    "pred.replace(\"\\%.\\%\", \" \").split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j980Ey5B-kUv"
   },
   "outputs": [],
   "source": [
    "y = torch.Tensor.repeat_interleave(x, repeats=5 , dim=0)\n",
    "\n",
    "z= y\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Pack_comp_final_try.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
