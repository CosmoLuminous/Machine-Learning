{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "colab_type": "code",
    "id": "B83XJ_ttBEFZ",
    "outputId": "8bb3448b-0be6-43b0-8aac-ac8be59ffadc"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "val9_dFDA5BC"
   },
   "outputs": [],
   "source": [
    "'''Import modules'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "from collections import Counter\n",
    "from skimage import io, transform\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import numpy as np\n",
    "from time import time\n",
    "import collections\n",
    "import pickle\n",
    "import os\n",
    "import gensim\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "x6qEbzQNBMM-",
    "outputId": "5f53a897-eb70-4c9c-9a45-3b943847d016"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cpu\n",
      "Using 0 GPUs!\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device =\", device)\n",
    "print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "parallel = True #enable nn.DataParallel for GPU\n",
    "platform = \"local\" #colab/local\n",
    "restore = True #Restore Checkpoint\n",
    "phase = \"Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Zf92eqfBN34"
   },
   "outputs": [],
   "source": [
    "VOCAB = {}\n",
    "WORD2IDX = {}\n",
    "IDX2WORD = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vLOoOXXGBO1x"
   },
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        h, w = image.shape[:2]\n",
    "        #print(\"TA RESCALE INPUT\", image.shape)\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "        #print(\"TA RESCALE OUTPUT\", image.shape)\n",
    "        return img\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, image):\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        #print(\"TA TRANSPOSE IP\", image.shape)\n",
    "        #image = image.transpose((2, 0, 1))\n",
    "        #print(\"TA TRANSPOSE OP\", image.shape)\n",
    "        return image\n",
    "\n",
    "\n",
    "IMAGE_RESIZE = (256, 256)\n",
    "# Sequentially compose the transforms\n",
    "img_transform = transforms.Compose([Rescale(IMAGE_RESIZE), ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "F3T42fc3BQpv",
    "outputId": "e22babbf-8c19-47fd-f4e4-63a5b0f88ab6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE = 8680\n"
     ]
    }
   ],
   "source": [
    "class CaptionsPreprocessing:\n",
    "    \"\"\"Preprocess the captions, generate vocabulary and convert words to tensor tokens\n",
    "    Args:\n",
    "        captions_file_path (string): captions tsv file path\n",
    "    \"\"\"\n",
    "    def __init__(self, captions_file_path):\n",
    "        self.captions_file_path = captions_file_path\n",
    "\n",
    "        # Read raw captions\n",
    "        self.raw_captions_dict = self.read_raw_captions()\n",
    "\n",
    "        # Preprocess captions\n",
    "        self.captions_dict = self.process_captions()\n",
    "\n",
    "        # Create vocabulary\n",
    "        self.start = \"<start>\"\n",
    "        self.end = \"<end>\"\n",
    "        self.oov = \"<unk>\"\n",
    "        self.pad = \"<pad>\"\n",
    "        self.vocab = self.generate_vocabulary()\n",
    "        self.word2index = self.convert_word2index()        \n",
    "        self.index2word = self.convert_index2word()\n",
    "        \n",
    "\n",
    "    def read_raw_captions(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            Dictionary with raw captions list keyed by image ids (integers)\n",
    "        \"\"\"\n",
    "        captions_dict = {}\n",
    "        with open(self.captions_file_path, 'r', encoding='utf-8') as f:\n",
    "            for img_caption_line in f.readlines():\n",
    "                img_captions = img_caption_line.strip().split('\\t')\n",
    "                captions_dict[int(img_captions[0])] = img_captions[1:]\n",
    "\n",
    "        return captions_dict \n",
    "\n",
    "    def process_captions(self):\n",
    "        \"\"\"\n",
    "        Use this function to generate dictionary and other preprocessing on captions\n",
    "        \"\"\"\n",
    "\n",
    "        raw_captions_dict = self.raw_captions_dict \n",
    "        \n",
    "        # Do the preprocessing here                \n",
    "        captions_dict = raw_captions_dict\n",
    "\n",
    "        return captions_dict\n",
    "\n",
    " \n",
    "\n",
    "    def generate_vocabulary(self):\n",
    "        \"\"\"\n",
    "        Use this function to generate dictionary and other preprocessing on captions\n",
    "        \"\"\"\n",
    "        captions_dict = self.captions_dict\n",
    "\n",
    "        # Generate the vocabulary\n",
    "        \n",
    "        all_captions = \"\"        \n",
    "        for cap_lists in captions_dict.values():\n",
    "            all_captions += \" \".join(cap_lists)\n",
    "        all_captions = nltk.tokenize.word_tokenize(all_captions.lower())\n",
    "        \n",
    "        vocab = {self.pad :1, self.oov :1, self.start :1, self.end :1}\n",
    "        vocab_update = Counter(all_captions) \n",
    "        vocab_update = {k:v for k,v in vocab_update.items() if v >= freq_threshold}\n",
    "        vocab.update(vocab_update)        \n",
    "        vocab_size = len(vocab)\n",
    "        \n",
    "        if phase == \"Train\":\n",
    "            VOCAB.clear()\n",
    "            VOCAB.update(vocab)\n",
    "            if platform == \"colab\":\n",
    "                fname = '/content/drive/My Drive/A4/dict/VOCAB_comp.pkl'\n",
    "            else:\n",
    "                fname = '../dict/VOCAB_comp.pkl'\n",
    "            #if not os.path.isfile(fname):\n",
    "            with open(fname, 'wb') as handle:\n",
    "                pickle.dump(vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "        print(\"VOCAB SIZE =\", vocab_size)\n",
    "        return vocab\n",
    "    \n",
    "    def convert_word2index(self):\n",
    "        \"\"\"\n",
    "        word to index converter\n",
    "        \"\"\"\n",
    "        word2index = {}\n",
    "        vocab = self.vocab\n",
    "        idx = 0\n",
    "        words = vocab.keys()\n",
    "        for w in words:\n",
    "            word2index[w] = idx\n",
    "            idx +=1\n",
    "        if phase == \"Train\":\n",
    "            WORD2IDX.clear()\n",
    "            WORD2IDX.update(word2index)\n",
    "            if platform == \"colab\":\n",
    "                fname = '/content/drive/My Drive/A4/dict/WORD2IDX_comp.pkl'\n",
    "            else:\n",
    "                fname = '../dict/WORD2IDX_comp.pkl'\n",
    "            #if not os.path.isfile(fname):\n",
    "            with open(fname, 'wb') as handle:\n",
    "                pickle.dump(word2index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        return word2index\n",
    "    \n",
    "    def convert_index2word(self):\n",
    "        \"\"\"\n",
    "        index to word converter\n",
    "        \"\"\"\n",
    "        index2word = {}\n",
    "        w2i = self.word2index\n",
    "        idx = 0\n",
    "        \n",
    "        for k, v in w2i.items():\n",
    "            index2word[v] = k\n",
    "            \n",
    "        if phase == \"Train\":\n",
    "            IDX2WORD.clear()\n",
    "            IDX2WORD.update(index2word)\n",
    "            if platform == \"colab\":\n",
    "                fname = '/content/drive/My Drive/A4/dict/IDX2WORD_comp.pkl'\n",
    "            else:\n",
    "                fname = '../dict/IDX2WORD_comp.pkl'\n",
    "            #if not os.path.isfile(fname):\n",
    "            with open(fname, 'wb') as handle:\n",
    "                pickle.dump(index2word, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        return index2word\n",
    "\n",
    "    def captions_transform(self, img_caption_list):\n",
    "        \"\"\"\n",
    "        Use this function to generate tensor tokens for the text captions\n",
    "        Args:\n",
    "            img_caption_list: List of captions for a particular image\n",
    "        \"\"\"\n",
    "        if phase == \"Test\":\n",
    "            word2index = WORD2IDX\n",
    "            vocab = VOCAB\n",
    "        else:\n",
    "            word2index = self.word2index\n",
    "            vocab = self.vocab\n",
    "            \n",
    "        start = self.start\n",
    "        end = self.end\n",
    "        oov = self.oov\n",
    "        \n",
    "        processed_list = list(map(lambda x: nltk.tokenize.word_tokenize(x.lower()), img_caption_list))\n",
    "        \n",
    "        \n",
    "        #print(processed_list)\n",
    "        processed_list = list(map(lambda x: list(map(lambda y: WORD2IDX[y] if y in vocab else WORD2IDX[oov],x)),\n",
    "                                  processed_list))\n",
    "        processed_list = list(map(lambda x: [WORD2IDX['<start>']] + x + [WORD2IDX['<end>']], processed_list))\n",
    "        #print(processed_list)\n",
    "        return processed_list\n",
    "\n",
    "\n",
    "if platform == \"colab\":\n",
    "    CAPTIONS_FILE_PATH = '/content/drive/My Drive/A4/train_captions.tsv'\n",
    "else:\n",
    "    CAPTIONS_FILE_PATH = \"D:/Padhai/IIT Delhi MS(R)/2019-20 Sem II/COL774 Machine Learning/Assignment/Assignment4/train_captions.tsv\"\n",
    "    \n",
    "embedding_dim = 256\n",
    "freq_threshold = 5\n",
    "captions_preprocessing_obj = CaptionsPreprocessing(CAPTIONS_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary Loaded Successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if phase == \"Test\":\n",
    "    VOCAB.clear()\n",
    "    WORD2IDX.clear()\n",
    "    IDX2WORD.clear()\n",
    "    if platform != 'colab':\n",
    "        with open('../dict/VOCAB.pkl', 'rb') as handle:\n",
    "            VOCAB = pickle.load(handle)\n",
    "        with open('../dict/WORD2IDX.pkl', 'rb') as handle:\n",
    "            WORD2IDX = pickle.load(handle)\n",
    "        with open('../dict/IDX2WORD.pkl', 'rb') as handle:\n",
    "            IDX2WORD = pickle.load(handle)\n",
    "        print(\"Dictionary Loaded Successfully\")\n",
    "    else:\n",
    "        with open('/content/drive/My Drive/A4/dict/VOCAB.pkl', 'rb') as handle:\n",
    "            VOCAB = pickle.load(handle)\n",
    "        with open('/content/drive/My Drive/A4/dict/WORD2IDX.pkl', 'rb') as handle:\n",
    "            WORD2IDX = pickle.load(handle)\n",
    "        with open('/content/drive/My Drive/A4/dict/IDX2WORD.pkl', 'rb') as handle:\n",
    "            IDX2WORD = pickle.load(handle)\n",
    "        print(\"Dictionary Loaded Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ddnzTeRBUFd"
   },
   "outputs": [],
   "source": [
    "class ImageCaptionsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, img_dir, captions_dict, img_transform=None, captions_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dir (string): Directory with all the images.\n",
    "            captions_dict: Dictionary with captions list keyed by image ids (integers)\n",
    "            img_transform (callable, optional): Optional transform to be applied\n",
    "                on the image sample.\n",
    "\n",
    "            captions_transform: (callable, optional): Optional transform to be applied\n",
    "                on the caption sample (list).\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.captions_dict = captions_dict\n",
    "        self.img_transform = img_transform\n",
    "        self.captions_transform = captions_transform\n",
    "\n",
    "        self.image_ids = list(captions_dict.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, 'image_{}.jpg'.format(self.image_ids[idx]))\n",
    "        image = io.imread(img_name)\n",
    "        #print(\"RAW IMG\", image.shape)\n",
    "        #captions = self.captions_dict[self.image_ids[idx]]\n",
    "        if self.img_transform:\n",
    "            image = self.img_transform(image)\n",
    "            \n",
    "            image = image.transpose((2, 0, 1))\n",
    "            \n",
    "\n",
    "        '''if self.captions_transform:            \n",
    "            captions = self.captions_transform(captions)'''\n",
    "            \n",
    "        sample = {'image': image}\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    \n",
    "def custom_batch(batch):\n",
    "    batch_size = len(batch)\n",
    "    captions = []\n",
    "    normalize_img = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "       \n",
    "    x = list(map(lambda b: b['image'],batch)) \n",
    "    x = list(map(lambda i: normalize_img(torch.from_numpy(i)).unsqueeze(0),x))\n",
    "    #print(\"my after norm shape\", x[0].shape)\n",
    "    images = torch.cat(x)\n",
    "    \n",
    "    sample = {'image': images}    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cP-sf163BZEX"
   },
   "outputs": [],
   "source": [
    "#ENCODER\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size, filters, stride=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            channels: Int: Number of Input channels to 1st convolutional layer\n",
    "            kernel_size: integer, Symmetric Conv Window = (kernel_size, kernel_size)\n",
    "            filters: python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "            stride: Tuple: (stride, stride)\n",
    "        \"\"\"\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        F1, F2, F3 = filters\n",
    "        #N, in_channels , H, W = shape\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "        padding = (1,1)\n",
    "        stride = (stride, stride)\n",
    "        self.conv1 = nn.Conv2d(in_channels = channels, out_channels = F1, kernel_size=(1,1), stride=stride, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(F1)\n",
    "        self.relu = nn.ReLU(inplace=True) \n",
    "        self.conv2 = nn.Conv2d(in_channels = F1, out_channels = F2, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.bn2 = nn.BatchNorm2d(F2)\n",
    "        self.conv3 = nn.Conv2d(in_channels = F2, out_channels = F3, kernel_size=(1,1), stride=stride, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(F3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_residual = x #backup x for residual connection\n",
    "        \n",
    "        #stage 1 main path\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        #print(\"RESI:\", x.shape)\n",
    "        \n",
    "        #stage 2 main path\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        #print(\"RESI:\", x.shape)\n",
    "        \n",
    "        #stage 3 main path\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        #print(\"RESI:\", x.shape)\n",
    "        \n",
    "        x += x_residual #add output with residual connection\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class ConvolutionalBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size, filters, stride=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            channels: Int: Number of Input channels to 1st convolutional layer\n",
    "            kernel_size: integer, Symmetric Conv Window = (kernel_size, kernel_size)\n",
    "            filters: python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "            stride: Tuple: (stride, stride)\n",
    "        \"\"\"\n",
    "        super(ConvolutionalBlock, self).__init__()\n",
    "        F1, F2, F3 = filters\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "        padding = (1,1)\n",
    "        stride = (stride, stride)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels = channels, out_channels = F1, kernel_size=(1,1), stride=stride, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(F1)\n",
    "        self.relu = nn.ReLU(inplace=True) \n",
    "        self.conv2 = nn.Conv2d(in_channels = F1, out_channels = F2, kernel_size=kernel_size, stride=(1,1), padding=padding)\n",
    "        self.bn2 = nn.BatchNorm2d(F2)\n",
    "        self.conv3 = nn.Conv2d(in_channels = F2, out_channels = F3, kernel_size=(1,1), stride=(1,1), padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(F3)\n",
    "        self.conv4 = nn.Conv2d(in_channels = channels, out_channels = F3, kernel_size=(1,1), stride=stride, padding=0)\n",
    "        self.bn4 = nn.BatchNorm2d(F3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x_residual = x #backup x for residual connection\n",
    "        \n",
    "        #stage 1 main path\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        #print(\"CONV:\", x.shape)\n",
    "        \n",
    "        #stage 2 main path\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        #print(\"CONV:\", x.shape)\n",
    "        \n",
    "        #stage 3 main path\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        #print(\"CONV:\", x.shape)\n",
    "        \n",
    "        #residual connection\n",
    "        x_residual = self.conv4(x_residual)\n",
    "        x_residual = self.bn4(x_residual)\n",
    "        x += x_residual #add output with residual connection\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, input_shape = (256, 256, 3), classes = 5):\n",
    "        \"\"\"\n",
    "        It Implements Famous Resnet50 Architecture\n",
    "        Args:\n",
    "            input_shape(tuple):(callable, optional): dimensions of image sample\n",
    "            classes(int):(callable, optional): Final output classes of softmax layer.\n",
    "        \"\"\"\n",
    "        super(ResNet50, self).__init__()\n",
    "        \n",
    "        self.pad = nn.ZeroPad2d((1, 1, 3, 3))        \n",
    "        ###STAGE1\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels=64, kernel_size=(7,7), stride = (2,2), padding=1) # convolve each of our 3-channel images with 6 different 5x5 kernels, giving us 6 feature maps\n",
    "        self.batch_norm1 = nn.BatchNorm2d(64) #BatchNorm\n",
    "        self.pool1 = nn.MaxPool2d((3,3), stride=(2,2), padding=1, dilation=1)\n",
    "        \n",
    "        ###STAGE2 channels, kernel_size=3, filters, stride=1, stage\n",
    "        self.conv_block1 = ConvolutionalBlock(channels = 64, kernel_size = 3, filters = [64, 64, 256],stride = 1)\n",
    "        self.residual_block1 = ResidualBlock(channels = 256, kernel_size = 3, filters = [64, 64, 256])\n",
    "        \n",
    "        ###STAGE3 \n",
    "        self.conv_block2 = ConvolutionalBlock(channels = 256, kernel_size = 3, filters = [128, 128, 512],stride = 2)\n",
    "        self.residual_block2 = ResidualBlock(channels = 512, kernel_size = 3, filters = [128, 128, 512],)\n",
    "        \n",
    "        ###STAGE4 \n",
    "        self.conv_block3 = ConvolutionalBlock(channels = 512, kernel_size = 3, filters = [256, 256, 1024], stride = 2)\n",
    "        self.residual_block3 = ResidualBlock(channels = 1024, kernel_size = 3, filters = [256, 256, 1024])\n",
    "        \n",
    "        ###STAGE5 \n",
    "        self.conv_block4 = ConvolutionalBlock(channels = 1024, kernel_size = 3, filters = [512, 512, 2048], stride = 2)\n",
    "        self.residual_block4 = ResidualBlock(channels = 2048, kernel_size = 3, filters = [512, 512, 2048])\n",
    "        \n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d(output_size = (1,1))\n",
    "        self.fc1 = nn.Linear(in_features=2048, out_features=classes, bias = True)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(\"IP_SIZE:\", x.shape)\n",
    "        \n",
    "        ###STAGE1        \n",
    "        #print(\"\\n STAGE1\")\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.pool1(x)\n",
    "        #print(\"OP_STAGE1_SIZE:\", x.shape)\n",
    "        \n",
    "        ###STAGE2 \n",
    "        #print(\"\\n STAGE2\")\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.residual_block1(x)\n",
    "        x = self.residual_block1(x)\n",
    "        #print(\"OP_STAGE2_SIZE:\", x.shape)\n",
    "        \n",
    "        ###STAGE3 \n",
    "        #print(\"\\n STAGE3\")\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.residual_block2(x)\n",
    "        x = self.residual_block2(x)\n",
    "        x = self.residual_block2(x)\n",
    "        #print(\"OP_STAGE3_SIZE:\", x.shape)\n",
    "        \n",
    "        ###STAGE4  \n",
    "        #print(\"\\n STAGE4\")\n",
    "        x = self.conv_block3(x)\n",
    "        x = self.residual_block3(x)\n",
    "        x = self.residual_block3(x)\n",
    "        x = self.residual_block3(x)\n",
    "        x = self.residual_block3(x)\n",
    "        x = self.residual_block3(x)\n",
    "        #print(\"OP_STAGE4_SIZE:\", x.shape)\n",
    "        \n",
    "        ###STAGE5  \n",
    "        #print(\"\\n STAGE5\")\n",
    "        x = self.conv_block4(x)\n",
    "        x = self.residual_block4(x)\n",
    "        x = self.residual_block4(x)\n",
    "        #print(\"OP_STAGE5_SIZE:\", x.shape)\n",
    "        \n",
    "        x = self.adaptive_pool(x)\n",
    "        #print(\"OP_ADAPTIVEPOOL_SHAPE\", x.shape)\n",
    "        \n",
    "        x = x.view(x.size(0), -1) # Flatten Vector\n",
    "        x = self.fc1(x)\n",
    "        #print(\"OP_FC1_SIZE:\", x.shape)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class Encoder(nn.Module):    \n",
    "    def __init__(self, embed_dim):\n",
    "        \"\"\"\n",
    "        CNN ENCODER\n",
    "        Args:\n",
    "            embed_dim(int): embedding dimension ie output dimension of last FC Layer\n",
    "        Returns:\n",
    "            x: Feature vector of size(BatchSize, embed_dim)\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.resnet50 = ResNet50(classes = embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.resnet50(x)\n",
    "'''    \n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, units, vocab_size):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.W1 = nn.Linear(in_features = embed_dim, out_features = units)\n",
    "        self.W2 = nn.Linear(in_features=units, out_features=units)\n",
    "        self.V = nn.Linear(in_features=units, out_features=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, img_features, hidden):\n",
    "        \n",
    "        hidden = hidden.unsqueeze(dim=1)\n",
    "        hidden = hidden.double()\n",
    "        #print(\"feature and hidden shape\",img_features.shape, hidden.shape)\n",
    "        combined_score = self.tanh(self.W1(img_features) + self.W2(hidden))\n",
    "        \n",
    "        attention_weights = self.softmax(self.V(combined_score))\n",
    "        context_vector = attention_weights * img_features\n",
    "        context_vector = torch.sum(context_vector, dim=1)\n",
    "        \n",
    "        return context_vector, attention_weights    '''\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_dim, lstm_hidden_size,lstm_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.vocab_size = len(VOCAB)\n",
    "        print(\"VOCAB SIZE = \", self.vocab_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = embed_dim, hidden_size = lstm_hidden_size,\n",
    "                            num_layers = lstm_layers, batch_first = True)\n",
    "        \n",
    "        self.linear = nn.Linear(lstm_hidden_size, self.vocab_size)        \n",
    "        #self.embed = nn.Embedding.from_pretrained(init_weights)\n",
    "        self.embed = nn.Embedding(self.vocab_size, embed_dim)\n",
    "        #self.attention = AttentionBlock(embed_dim, lstm_hidden_size, self.vocab_size)\n",
    "\n",
    "        \n",
    "    def forward(self, image_features, image_captions, lengths):\n",
    "        #print(\"DECODER INPUT\", image_features)\n",
    "        if phase == \"Train\":\n",
    "            #print(image)\n",
    "            image_features = torch.Tensor.repeat_interleave(image_features, repeats=5 , dim=0)\n",
    "        image_features = image_features.unsqueeze(1)\n",
    "        \n",
    "        \n",
    "        embedded_captions = self.embed(image_captions)\n",
    "        #print(\"EMBED SHAPE\", embedded_captions.shape)\n",
    "        #print(\"SHAPES BEFORE CONCAT\",context.unsqueeze(dim=1).shape, embedded_captions[:,:-1].shape)\n",
    "        input_lstm = torch.cat((image_features, embedded_captions[:,:-1]), dim = 1)\n",
    "        #input_lstm = pack_padded_sequence(input_lstm, lengths, batch_first=True, enforce_sorted=False)\n",
    "        lstm_outputs, _ = self.lstm(input_lstm)        \n",
    "        #lstm_outputs = self.linear(lstm_outputs[0]) \n",
    "        #print(\"lstm_outputs.shape\", lstm_outputs.shape)\n",
    "        lstm_outputs = self.linear(lstm_outputs) \n",
    "        \n",
    "        return lstm_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "colab_type": "code",
    "id": "5atlpQpiBa_n",
    "outputId": "6ea119ab-cff8-4177-c9f3-7d29e040bc7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device to CPU\n"
     ]
    }
   ],
   "source": [
    "class ImageCaptionsNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageCaptionsNet, self).__init__()        \n",
    "        ##CNN ENCODER RESNET-50        \n",
    "        self.Encoder = Encoder(embed_dim = embedding_dim)\n",
    "        ## RNN DECODER\n",
    "        self.Decoder = Decoder(embedding_dim, units, 1)    \n",
    "        \n",
    "\n",
    "    def forward(self, img_batch, cap_batch, lengths):\n",
    "        #print(\"IMG INPUT\",x)\n",
    "        x = self.Encoder(img_batch)\n",
    "        #print(\"IMG FEATURE\",x)\n",
    "        x = self.Decoder(x, cap_batch, lengths)\n",
    "        #print(\"IMG FEATURE\",x)\n",
    "        return x\n",
    "    \n",
    "units = 512\n",
    "if restore == False:\n",
    "    net = ImageCaptionsNet()\n",
    "    net = net.double()\n",
    "    \n",
    "'''    if parallel == True and device != \"cpu\":\n",
    "        print(\"Parallel Processing enabled\")\n",
    "        net = nn.DataParallel(net)'''\n",
    "\n",
    "if device == \"cpu\":\n",
    "    print(\"Device to CPU\")\n",
    "else:\n",
    "    print(\"Device to CUDA\")\n",
    "    net = net.to(torch.device(\"cuda:0\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "za24CpL-Bc2_"
   },
   "outputs": [],
   "source": [
    "'''Save and Restore Checkpoints'''\n",
    "def create_checkpoint(path,model, optim_obj, loss_obj,iteration, epoch):\n",
    "    checkpoint = {'epoch': epoch,\n",
    "                  'iteration': iteration,\n",
    "                  'model_state_dict': model.state_dict()}\n",
    "\n",
    "    if platform == \"colab\":\n",
    "        directory = '/content/drive/My Drive/A4/bkp_final_try/'\n",
    "    else:\n",
    "        directory = '../bkp_final_try/'\n",
    "\n",
    "    torch.save(checkpoint, directory + path)\n",
    "    \n",
    "def restore_checkpoint(path):\n",
    "    new_state_dict = collections.OrderedDict()\n",
    "    if platform == \"colab\":\n",
    "        directory = '/content/drive/My Drive/A4/bkp_final_try/'\n",
    "        checkpoint = torch.load(directory + path, map_location=torch.device('cpu'))\n",
    "    else:\n",
    "        directory = '../bkp_final_try/'\n",
    "        checkpoint = torch.load(directory + path, map_location=torch.device('cpu'))    \n",
    "    \n",
    "    epoch = checkpoint['epoch']\n",
    "    new_state_dict = checkpoint['model_state_dict']\n",
    "    iteration = checkpoint['iteration']\n",
    "    #optimizer_state_dict = checkpoint['optimizer_state_dict']\n",
    "    #loss_obj = checkpoint['loss']\n",
    "    print(\"Iterations = {}, Epoch = {}\".format(iteration, epoch))\n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_image(image_feature, max_words=20):\n",
    "        results = []\n",
    "        states = None\n",
    "        x = image_feature.unsqueeze(0)\n",
    "        #print(x)\n",
    "        with torch.no_grad():\n",
    "            for i in range(max_words):\n",
    "                \n",
    "                hiddens, states = net.Decoder.lstm(x, states)\n",
    "                #print(hiddens.shape)\n",
    "                decoder_op = net.Decoder.linear(hiddens.squeeze(1))\n",
    "                predicted_word = decoder_op.argmax(1)\n",
    "                prob = max(decoder_op[0].tolist())\n",
    "                #print(\"{} - {}\".format(IDX2WORD[predicted_word.item()], prob))\n",
    "                x = net.Decoder.embed(predicted_word).unsqueeze(0)\n",
    "                \n",
    "                results.append(predicted_word.item())\n",
    "                \n",
    "                '''if predicted_word == WORD2IDX[\"<end>\"]:\n",
    "                    break'''\n",
    "        \n",
    "        caption = [IDX2WORD[i] for i in  results]\n",
    "        cap = ' '.join(caption)\n",
    "        cap = cap.replace(\"<start>\",\"\").replace(\"<unk>\",\"\").replace(\"<end>\",\"\").replace(\"end\",\"\")\n",
    "        return cap\n",
    "               \n",
    "    \n",
    "# Define your hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE =  8680\n",
      "Iterations = 1209, Epoch = 3\n",
      "VOCAB SIZE =  8680\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ImageCaptionsNet:\n\tUnexpected key(s) in state_dict: \"Decoder.attention.W1.weight\", \"Decoder.attention.W1.bias\", \"Decoder.attention.W2.weight\", \"Decoder.attention.W2.bias\", \"Decoder.attention.V.weight\", \"Decoder.attention.V.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-296b70edf801>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageCaptionsNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"State Dictionary Loaded Successfully.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1043\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m-> 1045\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m   1046\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ImageCaptionsNet:\n\tUnexpected key(s) in state_dict: \"Decoder.attention.W1.weight\", \"Decoder.attention.W1.bias\", \"Decoder.attention.W2.weight\", \"Decoder.attention.W2.bias\", \"Decoder.attention.V.weight\", \"Decoder.attention.V.bias\". "
     ]
    }
   ],
   "source": [
    "if platform == \"colab\":\n",
    "    IMAGE_DIR_TEST = '/content/drive/My Drive/train/'\n",
    "else:\n",
    "    IMAGE_DIR_TEST = 'D:/Padhai/IIT Delhi MS(R)/2019-20 Sem II/COL774 Machine Learning/Assignment/Assignment4/private_test_images/'\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "\n",
    "if restore == True:\n",
    "    net = ImageCaptionsNet()\n",
    "    net = net.double()\n",
    "    state_dict = collections.OrderedDict()\n",
    "    state_dict = restore_checkpoint(\"Full_Model_own_finaltry.pth\")\n",
    "    net = ImageCaptionsNet()\n",
    "    net = net.double()\n",
    "    net.load_state_dict(state_dict)\n",
    "    print(\"State Dictionary Loaded Successfully.\")\n",
    "    \n",
    "    \n",
    "images_names = glob(IMAGE_DIR_TEST+\"*.jpg\")\n",
    "print(len(images_names))\n",
    "images_names = [os.path.split(i)[-1][:-4] for i in images_names]\n",
    "print(images_names[:5])\n",
    "images_names = [i.split(\"_\")[-1] for i in images_names]\n",
    "print(images_names[0])\n",
    "\n",
    "test_dataset = ImageCaptionsDataset(\n",
    "    IMAGE_DIR_TEST, captions_preprocessing_obj.captions_dict, img_transform=img_transform,\n",
    "    captions_transform=captions_preprocessing_obj.captions_transform\n",
    ")\n",
    "test_dataset.image_ids = images_names\n",
    "\n",
    "NUM_WORKERS = 0 \n",
    "MAX_WORDS = 35\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=NUM_WORKERS, collate_fn=custom_batch)\n",
    "\n",
    "if device != \"cpu\":\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "t0 = time()\n",
    "pred_caps = {}\n",
    "for batch_idx, sample in enumerate(test_loader):\n",
    "        print(\"Image_idx\", batch_idx)\n",
    "        image_batch = sample['image']\n",
    "        #print(\"AFTER\", image_batch)\n",
    "        #print(\"Original\", [IDX2WORD[i] for i in captions_batch)\n",
    "        #print(\"Cap\", [IDX2WORD[int(i)] for i in captions_batch[0]])\n",
    "        img_features = net.Encoder(image_batch)\n",
    "        #print(img_features)\n",
    "        #img_features = img_features.view(-1)[torch.randperm(img_features.nelement())].view(img_features.size())\n",
    "        #img_features = torch.FloatTensor(np.random.randn(1,300))\n",
    "        #print(img_features[0][:4].tolist(), img_features[0][-5:].tolist())\n",
    "        #print(x.shape)\n",
    "        #pred_cap = beam_search(img_features)\n",
    "        pred_cap = caption_image(img_features, 60)\n",
    "        \n",
    "        pred_caps[batch_idx] = pred_cap\n",
    "        print(\"Predicted\",batch_idx, pred_cap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "IMAGE_DIR_TEST = 'D:/Padhai/IIT Delhi MS(R)/2019-20 Sem II/COL774 Machine Learning/Assignment/Assignment4/private_test_images/'\n",
    "images_names = glob(IMAGE_DIR_TEST+\"*.jpg\")\n",
    "print(len(images_names))\n",
    "images_names = [os.path.split(i)[-1][:-4] for i in images_names]\n",
    "print(images_names[:5])\n",
    "images_names = [i.split(\"_\")[-1] for i in images_names]\n",
    "print(images_names[0])\n",
    "\n",
    "\n",
    "op_str = \"un uomo con una camicia edile e rifiuti sole.un è in piedi su una scala e ponte.un un altro uomo che tiene in mano un canta di spesa.un . \"\n",
    "op_str = [op_str]*len(images_names)\n",
    "\n",
    "op_dict = dict(zip(images_names, op_str))\n",
    "df = pd.DataFrame.from_dict(op_dict, orient='index', columns=None)\n",
    "df.to_csv( \"../2019SIY7580_2019CSZ8763/2019SIY7580_2019CSZ8763_private.tsv\", sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "IMAGE_DIR_TEST = 'D:/Padhai/IIT Delhi MS(R)/2019-20 Sem II/COL774 Machine Learning/Assignment/Assignment4/public_test_images/'\n",
    "images = glob(IMAGE_DIR_TEST+\"*.jpg\")\n",
    "print(len(images))\n",
    "images = [os.path.split(i)[-1][:-4] for i in images]\n",
    "print(images[:5])\n",
    "images = [i.split(\"_\")[-1] for i in images]\n",
    "print(images[0])\n",
    "\n",
    "\n",
    "op_str = \"un uomo con una camicia edile e rifiuti sole.un è in piedi su una scala e ponte.un un altro uomo che tiene in mano un canta di spesa.un . \"\n",
    "op_str = [op_str]*len(images)\n",
    "\n",
    "op_dict = dict(zip(images, op_str))\n",
    "df = pd.DataFrame.from_dict(op_dict, orient='index', columns=None)\n",
    "df.to_csv( \"../2019SIY7580_2019CSZ8763/2019SIY7580_2019CSZ8763_public.tsv\", sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1014\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "pub_test = os.path.join(\"D:/Padhai/IIT Delhi MS(R)/2019-20 Sem II/COL774 Machine Learning/Assignment/Assignment4/public_test_captions.tsv\")\n",
    "df = pd.read_csv(pub_test,sep=\"\\t\", header=None)\n",
    "names = df.iloc[:, 0]\n",
    "print(len(names))\n",
    "df.head()\n",
    "\n",
    "op_str = \"un uomo con una camicia edile e rifiuti sole.un è in piedi su una scala e ponte.un un altro uomo che tiene in mano un canta di spesa.un . \"\n",
    "op_str = [op_str]*len(names)\n",
    "\n",
    "op_dict = dict(zip(names, op_str))\n",
    "df_op = pd.DataFrame.from_dict(op_dict, orient='index', columns=None)\n",
    "df_op.to_csv( \"../2019SIY7580_2019CSZ8763/2019SIY7580_2019CSZ8763_public.tsv\", sep='\\t', header=False)\n",
    "#img_list = glob(pub_test)\n",
    "#img_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'image_10005.jpg'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "private_test = os.path.join(\"D:/Padhai/IIT Delhi MS(R)/2019-20 Sem II/COL774 Machine Learning/Assignment/Assignment4/private_test_images/\")\n",
    "pvt_names = os.listdir(private_test) \n",
    "pvt_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0044,  0.0037,  0.0051,  ...,  0.0081,  0.0126, -0.0021],\n",
       "        [ 0.0051, -0.0143,  0.0178,  ..., -0.0196,  0.0201,  0.0193],\n",
       "        [ 0.0086, -0.0086,  0.0099,  ...,  0.0163,  0.0085, -0.0196],\n",
       "        ...,\n",
       "        [-0.0101,  0.0161,  0.0168,  ..., -0.0196, -0.0078, -0.0193],\n",
       "        [-0.0136, -0.0130, -0.0190,  ...,  0.0151, -0.0174, -0.0115],\n",
       "        [-0.0103,  0.0192,  0.0010,  ..., -0.0193, -0.0207,  0.0131]],\n",
       "       dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.Encoder.fc.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "oOKS9DIUfkor",
    "outputId": "83f91197-22ae-4065-c02e-927f84829c3a"
   },
   "outputs": [],
   "source": [
    "op_str = \"blu.tre grillare trekking sinistra.una valigie pista.una tastiera estrarre jumpsuit azienda metropolitana.un accanto bilanciarsi coraggioso free canon canon dama canon dama dama 100 comodamente casco rocce accanto.una neve.il birra.una accanto.una accanto.una viso bassi mezz'aria.una sollevati dell'azienda lanciarsi sollevati letto.una afroamericana personale controllato birra.una orgoglioso carro l'esecuzione bandiera.un momenti l'operaio portapranzo pausa ferrovia.un l'attività pasta miei decide hockey.una addormentato cantava cubo fatta.\"\n",
    "op_str = \"un.uomo in camicia rossa e un gilet blu e una donna.\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Pack_comp_final_try.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
