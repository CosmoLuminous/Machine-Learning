{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnWhNJC8M5d5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from skimage import io, transform\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import numpy as np\n",
    "\n",
    "#Other Imports\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kC9JUlQMM5d9"
   },
   "source": [
    "### Image Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g6ClqmTUM5d9"
   },
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "        return img\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, image):\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return image\n",
    "\n",
    "\n",
    "IMAGE_RESIZE = (256, 256)\n",
    "# Sequentially compose the transforms\n",
    "img_transform = transforms.Compose([Rescale(IMAGE_RESIZE), ToTensor()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K5CM4cBhM5eA"
   },
   "source": [
    "### Captions Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rbEQS053M5eA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE = 127 {'<UNK>': 1, '<PAD>': 1, '<START>': 1, '<END>': 1, 'due': 32, 'ragazzi': 8, 'con': 106, 'i': 16, 'capelli': 9, 'le': 14, 'mentre': 25, 'sono': 14, 'in': 164, 'fuori': 15, 'da': 27, 'uomini': 21, 'magliette': 5, 'un': 315, 'uomo': 105, 'maglietta': 9, 'blu': 20, 'il': 32, 'cappello': 17, 'stanno': 15, 'di': 134, 'su': 65, 'che': 103, 'una': 183, 'quattro': 13, 'ad': 19, 'tre': 8, 'grande': 7, 'bambino': 7, 'vestito': 6, 'sta': 39, 'scale': 6, 'per': 32, 'bambina': 7, 'sale': 5, 'la': 45, 'giochi': 5, 'si': 38, 'parco': 8, 'ragazza': 8, 'camicia': 23, 'e': 80, 'Ã¨': 14, 'piedi': 18, 'finestra': 5, 'scala': 6, 'a': 68, 'sulla': 22, 'uno': 15, 'vicino': 13, 'al': 16, 'persone': 25, 'nella': 5, 'gli': 17, 'verde': 5, 'tiene': 12, 'del': 10, 'altro': 5, 'suonano': 6, 'siede': 9, 'seduto': 7, 'mano': 13, 'giocattolo': 5, 'fa': 6, 'parla': 6, 'cellulare': 5, 'strada': 31, 'giovane': 9, 'indossa': 11, 'sul': 12, 'donna': 21, 'nera': 10, 'accanto': 7, 'vestiti': 5, 'nero': 10, 'saltano': 11, 'della': 9, 'sole': 11, 'cinque': 7, 'dietro': 7, 'muro': 5, 'cane': 31, 'bianco': 16, 'verso': 5, 'guida': 7, 'arancione': 8, 'donne': 9, 'fronte': 14, 'gruppo': 16, 'occhiali': 7, 'bianca': 10, 'testa': 5, 'giacca': 12, 'sopra': 5, 'gioca': 8, 'panchina': 5, 'suo': 9, 'gente': 6, 'loro': 7, 'facendo': 5, 'persona': 13, 'guarda': 13, 'folla': 11, 'davanti': 9, 'tatuaggio': 5, 'ragazzo': 17, 'bambini': 11, 'piccolo': 6, 'bandiera': 5, 'rossa': 12, 'marrone': 5, 'pantaloni': 6, 'spinge': 5, 'cammina': 5, 'tazza': 8, 'camminano': 8, 'neve': 10, 'bicicletta': 11, 'corre': 8, 'abito': 6, 'veicolo': 5, 'ragazzino': 7, 'salta': 6}\n"
     ]
    }
   ],
   "source": [
    "class CaptionsPreprocessing:\n",
    "    \"\"\"Preprocess the captions, generate vocabulary and convert words to tensor tokens\n",
    "\n",
    "    Args:\n",
    "        captions_file_path (string): captions tsv file path\n",
    "    \"\"\"\n",
    "    def __init__(self, captions_file_path):\n",
    "        self.captions_file_path = captions_file_path\n",
    "\n",
    "        # Read raw captions\n",
    "        self.raw_captions_dict = self.read_raw_captions()\n",
    "\n",
    "        # Preprocess captions\n",
    "        self.captions_dict = self.process_captions()\n",
    "\n",
    "        # Create vocabulary\n",
    "        self.start = \"<START>\"\n",
    "        self.end = \"<END>\"\n",
    "        self.oov = \"<UNK>\"\n",
    "        self.pad = \"<PAD>\"\n",
    "        self.vocab = self.generate_vocabulary()\n",
    "        self.word2index = self.convert_word2index()        \n",
    "        self.index2word = self.convert_index2word()\n",
    "        self.embed = nn.Embedding(len(self.vocab), embedding_dim)\n",
    "        \n",
    "\n",
    "    def read_raw_captions(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            Dictionary with raw captions list keyed by image ids (integers)\n",
    "        \"\"\"\n",
    "\n",
    "        captions_dict = {}\n",
    "        with open(self.captions_file_path, 'r', encoding='utf-8') as f:\n",
    "            for img_caption_line in f.readlines():\n",
    "                img_captions = img_caption_line.strip().split('\\t')\n",
    "                captions_dict[int(img_captions[0])] = img_captions[1:]\n",
    "\n",
    "        return captions_dict\n",
    "\n",
    "    def process_captions(self):\n",
    "        \"\"\"\n",
    "        Use this function to generate dictionary and other preprocessing on captions\n",
    "        \"\"\"\n",
    "\n",
    "        raw_captions_dict = self.raw_captions_dict\n",
    "        \n",
    "        # Do the preprocessing here        \n",
    "        \n",
    "        captions_dict = raw_captions_dict\n",
    "\n",
    "        return captions_dict\n",
    "\n",
    "    def generate_vocabulary(self):\n",
    "        \"\"\"\n",
    "        Use this function to generate dictionary and other preprocessing on captions\n",
    "        \"\"\"\n",
    "\n",
    "        captions_dict = self.captions_dict\n",
    "\n",
    "        # Generate the vocabulary\n",
    "        \n",
    "        all_captions = \"\"        \n",
    "        for cap_lists in captions_dict.values():\n",
    "            all_captions += \" \".join(cap_lists)\n",
    "        all_captions = all_captions.lower().replace(\".\", \"\").split(\" \")\n",
    "        \n",
    "        vocab = { self.oov :1, self.pad :1, self.start :1, self.end :1}\n",
    "        vocab_update = Counter(all_captions) \n",
    "        vocab_update = {k:v for k,v in vocab_update.items() if v >= freq_threshold}\n",
    "        vocab.update(vocab_update)\n",
    "        vocab_size = len(vocab)        \n",
    "        \n",
    "        print(\"VOCAB SIZE =\", vocab_size, vocab)\n",
    "\n",
    "        return vocab\n",
    "    \n",
    "    def convert_word2index(self):\n",
    "        word2index = {}\n",
    "        vocab = self.vocab\n",
    "        idx = 0\n",
    "        for k, v in vocab.items():\n",
    "            word2index[k] = idx\n",
    "            idx +=1\n",
    "        \n",
    "        return word2index\n",
    "    \n",
    "    def convert_index2word(self):\n",
    "        index2word = {}\n",
    "        vocab = self.vocab\n",
    "        idx = 0\n",
    "        \n",
    "        for k, v in vocab.items():\n",
    "            index2word[idx] = k\n",
    "            idx +=1\n",
    "        \n",
    "        return index2word\n",
    "\n",
    "    def captions_transform(self, img_caption_list):\n",
    "        \"\"\"\n",
    "        Use this function to generate tensor tokens for the text captions\n",
    "        Args:\n",
    "            img_caption_list: List of captions for a particular image\n",
    "        \"\"\"\n",
    "        word2index = self.word2index\n",
    "        vocab = self.vocab\n",
    "        #index2word = self.index2word        \n",
    "        embed = self.embed\n",
    "        start = self.start\n",
    "        end = self.end\n",
    "        oov = self.oov\n",
    "                \n",
    "        processed_list = list(map(lambda x: start + \" \"+ x + \" \" + end, img_caption_list))\n",
    "        print(processed_list)\n",
    "        processed_list = list(map(lambda x: x.lower().replace(\".\", \"\").split(\" \"), processed_list))\n",
    "        print(processed_list)\n",
    "        processed_list = list(map(lambda x: list(map(lambda y: word2index[y] if y in vocab else word2index[oov],x)),\n",
    "                                  processed_list))\n",
    "        \n",
    "        \"\"\"YAHA SE START KARNA HAI. TENSOR ME CONVERT NAHI HO RAHA\"\"\"\n",
    "        processed_list = torch.Tensor(processed_list)\n",
    "        print(processed_list)\n",
    "        \n",
    "        processed_captions = embed(processed_list)   \n",
    "        print(processed_captions)\n",
    "\n",
    "        # Generate tensors\n",
    "\n",
    "        #return torch.zeros(len(img_caption_list), 10)\n",
    "        return processed_captions\n",
    "\n",
    "# Set the captions tsv file path\n",
    "CAPTIONS_FILE_PATH = '../data/train_cap64.tsv'\n",
    "embedding_dim = 256\n",
    "freq_threshold = 5\n",
    "captions_preprocessing_obj = CaptionsPreprocessing(CAPTIONS_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "a = [\"a\",\"b\"]\n",
    "a.extend([2,3,4])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lzmf-nlhM5eC"
   },
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bUuWkFPpM5eD"
   },
   "outputs": [],
   "source": [
    "class ImageCaptionsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, img_dir, captions_dict, img_transform=None, captions_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dir (string): Directory with all the images.\n",
    "            captions_dict: Dictionary with captions list keyed by image ids (integers)\n",
    "            img_transform (callable, optional): Optional transform to be applied\n",
    "                on the image sample.\n",
    "\n",
    "            captions_transform: (callable, optional): Optional transform to be applied\n",
    "                on the caption sample (list).\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.captions_dict = captions_dict\n",
    "        self.img_transform = img_transform\n",
    "        self.captions_transform = captions_transform\n",
    "\n",
    "        self.image_ids = list(captions_dict.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, 'image_{}.jpg'.format(self.image_ids[idx]))\n",
    "        image = io.imread(img_name)\n",
    "        captions = self.captions_dict[self.image_ids[idx]]\n",
    "\n",
    "        if self.img_transform:\n",
    "            image = self.img_transform(image)\n",
    "\n",
    "        if self.captions_transform:\n",
    "            captions = self.captions_transform(captions)\n",
    "\n",
    "        sample = {'image': image, 'captions': captions}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENCODER\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size, filters, stride=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            channels: Int: Number of Input channels to 1st convolutional layer\n",
    "            kernel_size: integer, Symmetric Conv Window = (kernel_size, kernel_size)\n",
    "            filters: python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "            stride: Tuple: (stride, stride)\n",
    "        \"\"\"\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        F1, F2, F3 = filters\n",
    "        #N, in_channels , H, W = shape\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "        padding = (1,1)\n",
    "        stride = (stride, stride)\n",
    "        self.conv1 = nn.Conv2d(in_channels = channels, out_channels = F1, kernel_size=(1,1), stride=stride, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(F1)\n",
    "        self.relu = nn.ReLU(inplace=True) \n",
    "        self.conv2 = nn.Conv2d(in_channels = F1, out_channels = F2, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.bn2 = nn.BatchNorm2d(F2)\n",
    "        self.conv3 = nn.Conv2d(in_channels = F2, out_channels = F3, kernel_size=(1,1), stride=stride, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(F3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_residual = x #backup x for residual connection\n",
    "        \n",
    "        #stage 1 main path\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        #print(\"RESI:\", x.shape)\n",
    "        \n",
    "        #stage 2 main path\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        #print(\"RESI:\", x.shape)\n",
    "        \n",
    "        #stage 3 main path\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        #print(\"RESI:\", x.shape)\n",
    "        \n",
    "        x += x_residual #add output with residual connection\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class ConvolutionalBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size, filters, stride=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            channels: Int: Number of Input channels to 1st convolutional layer\n",
    "            kernel_size: integer, Symmetric Conv Window = (kernel_size, kernel_size)\n",
    "            filters: python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "            stride: Tuple: (stride, stride)\n",
    "        \"\"\"\n",
    "        super(ConvolutionalBlock, self).__init__()\n",
    "        F1, F2, F3 = filters\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "        padding = (1,1)\n",
    "        stride = (stride, stride)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels = channels, out_channels = F1, kernel_size=(1,1), stride=stride, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(F1)\n",
    "        self.relu = nn.ReLU(inplace=True) \n",
    "        self.conv2 = nn.Conv2d(in_channels = F1, out_channels = F2, kernel_size=kernel_size, stride=(1,1), padding=padding)\n",
    "        self.bn2 = nn.BatchNorm2d(F2)\n",
    "        self.conv3 = nn.Conv2d(in_channels = F2, out_channels = F3, kernel_size=(1,1), stride=(1,1), padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(F3)\n",
    "        self.conv4 = nn.Conv2d(in_channels = channels, out_channels = F3, kernel_size=(1,1), stride=stride, padding=0)\n",
    "        self.bn4 = nn.BatchNorm2d(F3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x_residual = x #backup x for residual connection\n",
    "        \n",
    "        #stage 1 main path\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        #print(\"CONV:\", x.shape)\n",
    "        \n",
    "        #stage 2 main path\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        #print(\"CONV:\", x.shape)\n",
    "        \n",
    "        #stage 3 main path\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        #print(\"CONV:\", x.shape)\n",
    "        \n",
    "        #residual connection\n",
    "        x_residual = self.conv4(x_residual)\n",
    "        x_residual = self.bn4(x_residual)\n",
    "        print(x.shape, x_residual.shape)\n",
    "        x += x_residual #add output with residual connection\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, input_shape = (256, 256, 3), classes = 5):\n",
    "        \"\"\"\n",
    "        It Implements Famous Resnet50 Architecture\n",
    "        Args:\n",
    "            input_shape(tuple):(callable, optional): dimensions of image sample\n",
    "            classes(int):(callable, optional): Final output classes of softmax layer.\n",
    "        \"\"\"\n",
    "        super(ResNet50, self).__init__()\n",
    "        \n",
    "        self.pad = nn.ZeroPad2d((1, 1, 3, 3))        \n",
    "        ###STAGE1\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels=64, kernel_size=(7,7), stride = (2,2), padding=1) # convolve each of our 3-channel images with 6 different 5x5 kernels, giving us 6 feature maps\n",
    "        self.batch_norm1 = nn.BatchNorm2d(64) #BatchNorm\n",
    "        self.pool1 = nn.MaxPool2d((3,3), stride=(2,2), padding=1, dilation=1)\n",
    "        \n",
    "        ###STAGE2 channels, kernel_size=3, filters, stride=1, stage\n",
    "        self.conv_block1 = ConvolutionalBlock(channels = 64, kernel_size = 3, filters = [64, 64, 256],stride = 1)\n",
    "        self.residual_block1 = ResidualBlock(channels = 256, kernel_size = 3, filters = [64, 64, 256])\n",
    "        \n",
    "        ###STAGE3 \n",
    "        self.conv_block2 = ConvolutionalBlock(channels = 256, kernel_size = 3, filters = [128, 128, 512],stride = 2)\n",
    "        self.residual_block2 = ResidualBlock(channels = 512, kernel_size = 3, filters = [128, 128, 512],)\n",
    "        \n",
    "        ###STAGE4 \n",
    "        self.conv_block3 = ConvolutionalBlock(channels = 512, kernel_size = 3, filters = [256, 256, 1024], stride = 2)\n",
    "        self.residual_block3 = ResidualBlock(channels = 1024, kernel_size = 3, filters = [256, 256, 1024])\n",
    "        \n",
    "        ###STAGE5 \n",
    "        self.conv_block4 = ConvolutionalBlock(channels = 1024, kernel_size = 3, filters = [512, 512, 2048], stride = 2)\n",
    "        self.residual_block4 = ResidualBlock(channels = 2048, kernel_size = 3, filters = [512, 512, 2048])\n",
    "        \n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
    "        self.fc1 = nn.Linear(1 , classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(\"IP_SIZE:\", x.shape)\n",
    "        \n",
    "        ###STAGE1        \n",
    "        #print(\"\\n STAGE1\")\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.pool1(x)\n",
    "        print(\"OP_STAGE1_SIZE:\", x.shape)\n",
    "        \n",
    "        ###STAGE2 \n",
    "        #print(\"\\n STAGE2\")\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.residual_block1(x)\n",
    "        x = self.residual_block1(x)\n",
    "        print(\"OP_STAGE2_SIZE:\", x.shape)\n",
    "        \n",
    "        ###STAGE3 \n",
    "        #print(\"\\n STAGE3\")\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.residual_block2(x)\n",
    "        x = self.residual_block2(x)\n",
    "        x = self.residual_block2(x)\n",
    "        print(\"OP_STAGE3_SIZE:\", x.shape)\n",
    "        \n",
    "        ###STAGE4  \n",
    "        #print(\"\\n STAGE4\")\n",
    "        x = self.conv_block3(x)\n",
    "        x = self.residual_block3(x)\n",
    "        x = self.residual_block3(x)\n",
    "        x = self.residual_block3(x)\n",
    "        x = self.residual_block3(x)\n",
    "        x = self.residual_block3(x)\n",
    "        print(\"OP_STAGE4_SIZE:\", x.shape)\n",
    "        \n",
    "        ###STAGE5  \n",
    "        #print(\"\\n STAGE5\")\n",
    "        x = self.conv_block4(x)\n",
    "        x = self.residual_block4(x)\n",
    "        x = self.residual_block4(x)\n",
    "        print(\"OP_STAGE5_SIZE:\", x.shape)\n",
    "        \n",
    "        x = self.adaptive_pool(x)\n",
    "        #print(\"Post Pool\", x.shape)\n",
    "        x = self.fc1(x)\n",
    "        print(\"OP_FC1_SIZE:\", x.shape)\n",
    "        \n",
    "        \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.resnet50 = ResNet50(classes = embed_dim)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.resnet50(x)\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6eaz6MgvM5eG"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn' has no attribute 'Softm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-82c65ed033d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcaptions_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageCaptionsNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# If GPU training is required\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-82c65ed033d0>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;31m##CNN ENCODER RESNET-50\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEncoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-d940c9bec1f1>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, embed_dim)\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEncoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresnet50\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mResNet50\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSoftm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'Softm'"
     ]
    }
   ],
   "source": [
    "class ImageCaptionsNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageCaptionsNet, self).__init__()\n",
    "\n",
    "        # Define your architecture here\n",
    "        \n",
    "        ##CNN ENCODER RESNET-50        \n",
    "        self.Encoder = Encoder(embedding_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = image_batch, captions_batch\n",
    "\n",
    "        # Forward Propogation\n",
    "        x = self.Encoder(image_batch)\n",
    "        #print(x.shape)\n",
    "\n",
    "        return captions_batch\n",
    "\n",
    "net = ImageCaptionsNet()\n",
    "net = net.double()\n",
    "# If GPU training is required\n",
    "# net = net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rUQtOQ5JM5eI"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZmqfKZIPM5eI"
   },
   "outputs": [],
   "source": [
    "IMAGE_DIR = '../data/train/'\n",
    "\n",
    "# Creating the Dataset\n",
    "train_dataset = ImageCaptionsDataset(\n",
    "    IMAGE_DIR, captions_preprocessing_obj.captions_dict, img_transform=img_transform,\n",
    "    captions_transform=captions_preprocessing_obj.captions_transform\n",
    ")\n",
    "\n",
    "# Define your hyperparameters\n",
    "embedding_dim = 256\n",
    "words_threshold = 5\n",
    "\n",
    "NUMBER_OF_EPOCHS = 3\n",
    "LEARNING_RATE = 1e-1\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 0 # Parallel threads for dataloading\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Creating the DataLoader for batching purposes\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "import os\n",
    "for epoch in range(NUMBER_OF_EPOCHS):\n",
    "    for batch_idx, sample in enumerate(train_loader):\n",
    "        net.zero_grad()\n",
    "\n",
    "        image_batch, captions_batch = sample['image'], sample['captions']\n",
    "        \n",
    "\n",
    "        # If GPU training required\n",
    "        # image_batch, captions_batch = image_batch.cuda(), captions_batch.cuda()\n",
    "\n",
    "        output_captions = net((image_batch, captions_batch))\n",
    "        #loss = loss_function(output_captions, captions_batch)\n",
    "        #loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Iteration: \" + str(epoch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W0pi-EQYM5eF"
   },
   "source": [
    "### Model Architecture\n",
    "\n",
    "\n",
    "#### 3 - Building  first ResNet model (50 layers)\n",
    "You now have the necessary blocks to build a very deep ResNet. The following figure describes in detail the architecture of this neural network. \"ID BLOCK\" in the diagram stands for \"Identity block,\" and \"ID BLOCK x3\" means you should stack 3 identity blocks together.\n",
    "\n",
    "\n",
    "The details of this ResNet-50 model are:\n",
    "- Zero-padding pads the input with a pad of (3,3)\n",
    "- Stage 1:\n",
    "    - The 2D Convolution has 64 filters of shape (7,7) and uses a stride of (2,2). Its name is \"conv1\".\n",
    "    - BatchNorm is applied to the 'channels' axis of the input.\n",
    "    - MaxPooling uses a (3,3) window and a (2,2) stride.\n",
    "- Stage 2:\n",
    "    - The convolutional block uses three sets of filters of size [64,64,256], \"f\" is 3, \"s\" is 1 and the block is \"a\".\n",
    "    - The 2 identity blocks use three sets of filters of size [64,64,256], \"f\" is 3 and the blocks are \"b\" and \"c\".\n",
    "- Stage 3:\n",
    "    - The convolutional block uses three sets of filters of size [128,128,512], \"f\" is 3, \"s\" is 2 and the block is \"a\".\n",
    "    - The 3 identity blocks use three sets of filters of size [128,128,512], \"f\" is 3 and the blocks are \"b\", \"c\" and \"d\".\n",
    "- Stage 4:\n",
    "    - The convolutional block uses three sets of filters of size [256, 256, 1024], \"f\" is 3, \"s\" is 2 and the block is \"a\".\n",
    "    - The 5 identity blocks use three sets of filters of size [256, 256, 1024], \"f\" is 3 and the blocks are \"b\", \"c\", \"d\", \"e\" and \"f\".\n",
    "- Stage 5:\n",
    "    - The convolutional block uses three sets of filters of size [512, 512, 2048], \"f\" is 3, \"s\" is 2 and the block is \"a\".\n",
    "    - The 2 identity blocks use three sets of filters of size [512, 512, 2048], \"f\" is 3 and the blocks are \"b\" and \"c\".\n",
    "- The 2D Average Pooling uses a window of shape (2,2) and its name is \"avg_pool\".\n",
    "- The 'flatten' layer doesn't have any hyperparameters or name.\n",
    "- The Fully Connected (Dense) layer reduces its input to the number of classes using a softmax activation. Its name should be `'fc' + str(classes)`.\n",
    "\n",
    "**Exercise**: Implement the ResNet with 50 layers described in the figure above. We have implemented Stages 1 and 2. Please implement the rest. (The syntax for implementing Stages 3-5 should be quite similar to that of Stage 2.) Make sure you follow the naming convention in the text above. \n",
    "\n",
    "You'll need to use this function: \n",
    "- Average pooling [see reference](https://keras.io/layers/pooling/#averagepooling2d)\n",
    "\n",
    "Here are some other functions we used in the code below:\n",
    "- Conv2D: [See reference](https://keras.io/layers/convolutional/#conv2d)\n",
    "- BatchNorm: [See reference](https://keras.io/layers/normalization/#batchnormalization) (axis: Integer, the axis that should be normalized (typically the features axis))\n",
    "- Zero padding: [See reference](https://keras.io/layers/convolutional/#zeropadding2d)\n",
    "- Max pooling: [See reference](https://keras.io/layers/pooling/#maxpooling2d)\n",
    "- Fully connected layer: [See reference](https://keras.io/layers/core/#dense)\n",
    "- Addition: [See reference](https://keras.io/layers/merge/#add)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pytorchtut.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
