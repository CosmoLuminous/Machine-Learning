{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "B83XJ_ttBEFZ",
    "outputId": "f9c162ed-0bed-4be6-9975-19122078819e"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "val9_dFDA5BC"
   },
   "outputs": [],
   "source": [
    "'''Import modules'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "from collections import Counter\n",
    "from skimage import io, transform\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import numpy as np\n",
    "from time import time\n",
    "import collections\n",
    "import pickle\n",
    "import os\n",
    "import gensim\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "x6qEbzQNBMM-",
    "outputId": "c7919f50-8312-4e18-9123-abad84770b21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cpu\n",
      "Using 0 GPUs!\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device =\", device)\n",
    "print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "parallel = True #enable nn.DataParallel for GPU\n",
    "platform = \"local\" #colab/local\n",
    "restore = True #Restore Checkpoint\n",
    "phase = \"Train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Zf92eqfBN34"
   },
   "outputs": [],
   "source": [
    "VOCAB = {}\n",
    "WORD2IDX = {}\n",
    "IDX2WORD = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ly-_2DZHX1Mf"
   },
   "outputs": [],
   "source": [
    "if platform == \"colab\":\n",
    "    path_gensim = \"/content/drive/My Drive/A4/wordvec/train_captions.bin\"\n",
    "    path_fasttext = '/content/drive/My Drive/wordvec/cc.it.300.vec.gz'\n",
    "else:\n",
    "    path_gensim = \"../embeddings/train_captions.bin\"\n",
    "    path_fasttext = ''\n",
    "\n",
    "#fasttext_model = gensim.models.KeyedVectors.load_word2vec_format(path_fasttext, binary=False, unicode_errors='replace')\n",
    "#print(fasttext_model)\n",
    "#gensim_model = gensim.models.Word2Vec.load(path_gensim)\n",
    "#print(gensim_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "oOKS9DIUfkor",
    "outputId": "5a45424c-673e-4348-c0f0-5cbb3d5dc488"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not found 0\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "keys_found = 0\n",
    "not_found = []\n",
    "vocab_dump = VOCAB.copy()\n",
    "for k in VOCAB.keys():\n",
    "    if k in fasttext_model.vocab:\n",
    "        keys_found += 1\n",
    "        vocab_dump[k] = torch.FloatTensor(fasttext_model.wv.get_vector(k))\n",
    "    else:\n",
    "        vocab_dump[k] = torch.randn(300)\n",
    "        not_found.append(k)\n",
    "\n",
    "print(\"not found\", len(not_found))\n",
    "\n",
    "#with open('/content/drive/My Drive/A4/embeddings/trained_embed.pkl', 'wb') as handle:\n",
    "#  pickle.dump(vocab_dump, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vLOoOXXGBO1x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class Rescale(object):\\n    \"\"\"Rescale the image in a sample to a given size.\\n\\n    Args:\\n        output_size (tuple or int): Desired output size. If tuple, output is\\n            matched to output_size. If int, smaller of image edges is matched\\n            to output_size keeping aspect ratio the same.\\n    \"\"\"\\n\\n    def __init__(self, output_size):\\n        assert isinstance(output_size, (int, tuple))\\n        self.output_size = output_size\\n\\n    def __call__(self, image):\\n        h, w = image.shape[:2]\\n        if isinstance(self.output_size, int):\\n            if h > w:\\n                new_h, new_w = self.output_size * h / w, self.output_size\\n            else:\\n                new_h, new_w = self.output_size, self.output_size * w / h\\n        else:\\n            new_h, new_w = self.output_size\\n\\n        new_h, new_w = int(new_h), int(new_w)\\n        img = transform.resize(image, (new_h, new_w))\\n        return img\\n\\n\\nclass ToTensor(object):\\n    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\\n\\n    def __call__(self, image):\\n        # swap color axis because\\n        # numpy image: H x W x C\\n        # torch image: C X H X W\\n        image = image.transpose((2, 0, 1))\\n        return image\\n\\n\\nIMAGE_RESIZE = (256, 256)\\n# Sequentially compose the transforms\\nimg_transform = transforms.Compose([Rescale(IMAGE_RESIZE), ToTensor()])'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "        return img\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, image):\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return image\n",
    "\n",
    "\n",
    "IMAGE_RESIZE = (256, 256)\n",
    "# Sequentially compose the transforms\n",
    "img_transform = transforms.Compose([Rescale(IMAGE_RESIZE), ToTensor()])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resize(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        img = transforms.Resize(self.output_size)(image)\n",
    "        return img\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, image):\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return image\n",
    "\n",
    "class Normalize(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        img_norm = transforms.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n",
    "        image = img_norm(img_norm)\n",
    "        return image\n",
    "\n",
    "IMAGE_RESIZE = (256, 256)\n",
    "# Sequentially compose the transforms\n",
    "img_transform = transforms.Compose([Resize(256), RandomHorizontalFlip(), transforms.ToTensor(), Normalize()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "F3T42fc3BQpv",
    "outputId": "1f7ff66e-65bb-4c6c-8f2d-f658dde62b00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE = 8900\n"
     ]
    }
   ],
   "source": [
    "class CaptionsPreprocessing:\n",
    "    \"\"\"Preprocess the captions, generate vocabulary and convert words to tensor tokens\n",
    "    Args:\n",
    "        captions_file_path (string): captions tsv file path\n",
    "    \"\"\"\n",
    "    def __init__(self, captions_file_path):\n",
    "        self.captions_file_path = captions_file_path\n",
    "\n",
    "        # Read raw captions\n",
    "        self.raw_captions_dict = self.read_raw_captions()\n",
    "\n",
    "        # Preprocess captions\n",
    "        self.captions_dict = self.process_captions()\n",
    "\n",
    "        # Create vocabulary\n",
    "        self.start = \"<start>\"\n",
    "        self.end = \"<end>\"\n",
    "        self.oov = \"<unk>\"\n",
    "        self.pad = \"<pad>\"\n",
    "        self.vocab = self.generate_vocabulary()\n",
    "        self.word2index = self.convert_word2index()        \n",
    "        self.index2word = self.convert_index2word()\n",
    "        \n",
    "\n",
    "    def read_raw_captions(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            Dictionary with raw captions list keyed by image ids (integers)\n",
    "        \"\"\"\n",
    "        captions_dict = {}\n",
    "        with open(self.captions_file_path, 'r', encoding='utf-8') as f:\n",
    "            for img_caption_line in f.readlines():\n",
    "                img_captions = img_caption_line.strip().split('\\t')\n",
    "                captions_dict[int(img_captions[0])] = img_captions[1:]\n",
    "\n",
    "        return captions_dict \n",
    "\n",
    "    def process_captions(self):\n",
    "        \"\"\"\n",
    "        Use this function to generate dictionary and other preprocessing on captions\n",
    "        \"\"\"\n",
    "\n",
    "        raw_captions_dict = self.raw_captions_dict \n",
    "        \n",
    "        # Do the preprocessing here                \n",
    "        captions_dict = raw_captions_dict\n",
    "\n",
    "        return captions_dict\n",
    "\n",
    " \n",
    "\n",
    "    def generate_vocabulary(self):\n",
    "        \"\"\"\n",
    "        Use this function to generate dictionary and other preprocessing on captions\n",
    "        \"\"\"\n",
    "        captions_dict = self.captions_dict\n",
    "\n",
    "        # Generate the vocabulary\n",
    "        \n",
    "        all_captions = \"\"        \n",
    "        for cap_lists in captions_dict.values():\n",
    "            all_captions += \" \".join(cap_lists)\n",
    "        all_captions = all_captions.lower().replace(\",\",\" ,\").replace(\".\",\" .\").split(\" \")\n",
    "        \n",
    "        vocab = {self.pad :1, self.oov :1, self.start :1, self.end :1}\n",
    "        vocab_update = Counter(all_captions) \n",
    "        vocab_update = {k:v for k,v in vocab_update.items() if v >= freq_threshold}\n",
    "        vocab.update(vocab_update)        \n",
    "        vocab_size = len(vocab)\n",
    "        \n",
    "        if phase == \"Train\":\n",
    "            VOCAB.update(vocab)\n",
    "            if platform == \"colab\":\n",
    "                fname = '/content/drive/My Drive/A4/dict/VOCAB.pkl'\n",
    "            else:\n",
    "                fname = '../dict/VOCAB.pkl'\n",
    "            #if not os.path.isfile(fname):\n",
    "            with open(fname, 'wb') as handle:\n",
    "                pickle.dump(vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "        print(\"VOCAB SIZE =\", vocab_size)\n",
    "        return vocab\n",
    "    \n",
    "    def convert_word2index(self):\n",
    "        \"\"\"\n",
    "        word to index converter\n",
    "        \"\"\"\n",
    "        word2index = {}\n",
    "        vocab = self.vocab\n",
    "        idx = 0\n",
    "        for k, v in vocab.items():\n",
    "            word2index[k] = idx\n",
    "            idx +=1\n",
    "        if phase == \"Train\":\n",
    "            WORD2IDX.update(word2index)\n",
    "            if platform == \"colab\":\n",
    "                fname = '/content/drive/My Drive/A4/dict/WORD2IDX.pkl'\n",
    "            else:\n",
    "                fname = '../dict/WORD2IDX.pkl'\n",
    "            #if not os.path.isfile(fname):\n",
    "            with open(fname, 'wb') as handle:\n",
    "                pickle.dump(word2index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        return word2index\n",
    "    \n",
    "    def convert_index2word(self):\n",
    "        \"\"\"\n",
    "        index to word converter\n",
    "        \"\"\"\n",
    "        index2word = {}\n",
    "        vocab = self.vocab\n",
    "        idx = 0\n",
    "        \n",
    "        for k, v in vocab.items():\n",
    "            index2word[idx] = k\n",
    "            idx +=1\n",
    "        if phase == \"Train\":\n",
    "            IDX2WORD.update(index2word)\n",
    "            if platform == \"colab\":\n",
    "                fname = '/content/drive/My Drive/A4/dict/IDX2WORD.pkl'\n",
    "            else:\n",
    "                fname = '../dict/IDX2WORD.pkl'\n",
    "            #if not os.path.isfile(fname):\n",
    "            with open(fname, 'wb') as handle:\n",
    "                pickle.dump(index2word, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        return index2word\n",
    "\n",
    " \n",
    "\n",
    "    def captions_transform(self, img_caption_list):\n",
    "        \"\"\"\n",
    "        Use this function to generate tensor tokens for the text captions\n",
    "        Args:\n",
    "            img_caption_list: List of captions for a particular image\n",
    "        \"\"\"\n",
    "        if phase == \"Test\":\n",
    "            word2index = WORD2IDX\n",
    "            vocab = VOCAB\n",
    "        else:\n",
    "            word2index = self.word2index\n",
    "            vocab = self.vocab\n",
    "            \n",
    "        start = self.start\n",
    "        end = self.end\n",
    "        oov = self.oov\n",
    "        \n",
    "        processed_list = list(map(lambda x: start + \" \"+ x + \" \" + end, img_caption_list))\n",
    "        processed_list = list(map(lambda x: x.lower().replace(\".\", \" .\").replace(\",\", \" ,\").split(\" \"), processed_list))\n",
    "        processed_list = list(map(lambda x: list(map(lambda y: word2index[y] if y in vocab else word2index[oov],x)),\n",
    "                                  processed_list))\n",
    "        return processed_list\n",
    "\n",
    "\n",
    "if platform == \"colab\":\n",
    "    CAPTIONS_FILE_PATH = '/content/drive/My Drive/A4/train_captions.tsv'\n",
    "else:\n",
    "    CAPTIONS_FILE_PATH = \"D:/Padhai/IIT Delhi MS(R)/2019-20 Sem II/COL774 Machine Learning/Assignment/Assignment4/train_captions.tsv\"\n",
    "    \n",
    "embedding_dim = 300\n",
    "freq_threshold = 4\n",
    "captions_preprocessing_obj = CaptionsPreprocessing(CAPTIONS_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gpyvqp2LRSLY"
   },
   "outputs": [],
   "source": [
    "if platform == \"colab\":\n",
    "    embed_path = '/content/drive/My Drive/A4/embeddings/trained_embed.pkl'\n",
    "else:\n",
    "    embed_path = '../embeddings/trained_embed.pkl'\n",
    "with open(embed_path , 'rb') as handle:\n",
    "    vocab_dump = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "bMogMbywOhKv",
    "outputId": "dc86317a-864d-4ceb-8f00-29164b690ea4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'init_weights = torch.randn(len(VOCAB), 300)\\nidx = 0\\nwords = WORD2IDX.keys()\\nfor i in range(len(words)):\\n    init_weights[i] = vocab_dump[IDX2WORD[i]]\\ninit_weights.shape'"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''init_weights = torch.randn(len(VOCAB), 300)\n",
    "idx = 0\n",
    "words = WORD2IDX.keys()\n",
    "for i in range(len(words)):\n",
    "    init_weights[i] = vocab_dump[IDX2WORD[i]]\n",
    "init_weights.shape'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ddnzTeRBUFd"
   },
   "outputs": [],
   "source": [
    "class ImageCaptionsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, img_dir, captions_dict, img_transform=None, captions_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dir (string): Directory with all the images.\n",
    "            captions_dict: Dictionary with captions list keyed by image ids (integers)\n",
    "            img_transform (callable, optional): Optional transform to be applied\n",
    "                on the image sample.\n",
    "\n",
    "            captions_transform: (callable, optional): Optional transform to be applied\n",
    "                on the caption sample (list).\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.captions_dict = captions_dict\n",
    "        self.img_transform = transforms.Compose([transforms.Resize(256), \n",
    "                                                  transforms.RandomCrop(224),transforms.RandomHorizontalFlip(),\n",
    "                                                  transforms.ToTensor(),transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                                                                                             (0.229, 0.224, 0.225))])\n",
    "        self.captions_transform = captions_transform\n",
    "\n",
    "        self.image_ids = list(captions_dict.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, 'image_{}.jpg'.format(self.image_ids[idx]))\n",
    "        #image = io.imread(img_name)\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        print(type(image))\n",
    "        image = self.img_transform(image)\n",
    "        captions = self.captions_dict[self.image_ids[idx]]\n",
    "        if self.img_transform:\n",
    "            image = self.img_transform(image)\n",
    "\n",
    "        if self.captions_transform:            \n",
    "            captions = self.captions_transform(captions)\n",
    "            \n",
    "        sample = {'image': image, 'captions': captions}\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    \n",
    "def custom_batch(batch):\n",
    "    batch_size = len(batch)\n",
    "    captions = []\n",
    "    \n",
    "    z = list(map(lambda b: captions.extend(b['captions']),batch))    \n",
    "    x = list(map(lambda b: b['image'],batch))\n",
    "    print(x[0].shape)\n",
    "    captions = list(map(lambda c: torch.LongTensor(c),captions))\n",
    "    captions = pad_sequence(captions, batch_first=True)\n",
    "    images = torch.cat(x)\n",
    "    \n",
    "    sample = {'image': images, 'captions': captions}    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cP-sf163BZEX"
   },
   "outputs": [],
   "source": [
    "#ENCODER\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        resnet101 = models.resnet34(pretrained=True, progress=True)\n",
    "        for param in resnet101.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(\"ResNet152 Loaded Successfully..!\")\n",
    "        self.resnet101 = resnet101\n",
    "        #self.adaptive_pool = nn.AdaptiveAvgPool2d(output_size = (1,1))\n",
    "        self.fc = nn.Linear(in_features=self.resnet101.fc.in_features, out_features=embed_dim, bias = True)\n",
    "        layers = list(resnet101.children())[:-1]\n",
    "        self.resnet101 = nn.Sequential(*layers)\n",
    "        '''for layer in list(self.resnet101.children())[2:]:\n",
    "            for params in layer.parameters():\n",
    "                params.requires_grad = True'''\n",
    "        self.relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet101(x)\n",
    "        #x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #x = self.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "        \n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, units, vocab_size):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.W1 = nn.Linear(in_features = embed_dim, out_features = units)\n",
    "        self.W2 = nn.Linear(in_features=units, out_features=units)\n",
    "        self.V = nn.Linear(in_features=units, out_features=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, img_features, hidden):\n",
    "        \n",
    "        hidden = hidden.unsqueeze(dim=1)\n",
    "        hidden = hidden.double()\n",
    "        #print(\"feature and hidden shape\",img_features.shape, hidden.shape)\n",
    "        combined_score = self.tanh(self.W1(img_features) + self.W2(hidden))\n",
    "        \n",
    "        attention_weights = self.softmax(self.V(combined_score))\n",
    "        context_vector = attention_weights * img_features\n",
    "        context_vector = torch.sum(context_vector, dim=1)\n",
    "        \n",
    "        return context_vector, attention_weights    \n",
    "\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_dim, lstm_hidden_size,lstm_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.vocab_size = len(VOCAB)\n",
    "        print(\"VOCAB SIZE = \", self.vocab_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = embed_dim, hidden_size = lstm_hidden_size,\n",
    "                            num_layers = lstm_layers, batch_first = True)\n",
    "        \n",
    "        self.linear = nn.Linear(lstm_hidden_size, self.vocab_size)        \n",
    "        #self.embed = nn.Embedding.from_pretrained(init_weights)\n",
    "        self.embed = nn.Embedding(self.vocab_size, embed_dim)\n",
    "        \n",
    "        #self.attention = AttentionBlock(embed_dim, lstm_hidden_size, self.vocab_size)\n",
    "\n",
    "        \n",
    "    def forward(self, image_features, image_captions):\n",
    "        \n",
    "        if phase == \"Train\":\n",
    "            #print(image)\n",
    "            image_features = torch.Tensor.repeat_interleave(image_features, repeats=5 , dim=0)\n",
    "        image_features = image_features.unsqueeze(1)\n",
    "        \n",
    "        #hidden = torch.zeros((image_features.shape[0], self.lstm_hidden_size))\n",
    "        #if device == \"cuda\":\n",
    "        #    hidden = hidden.to(torch.device(\"cuda:0\"))\n",
    "        \n",
    "        #context, attention = self.attention(image_features, hidden)\n",
    "        \n",
    "        embedded_captions = self.embed(image_captions)\n",
    "        #print(\"EMBED SHAPE\", embedded_captions.shape)\n",
    "        #print(\"SHAPES BEFORE CONCAT\",context.unsqueeze(dim=1).shape, embedded_captions[:,:-1].shape)\n",
    "        input_lstm = torch.cat((image_features, embedded_captions[:,:-1]), dim = 1)\n",
    "        \n",
    "        lstm_outputs, _ = self.lstm(input_lstm)        \n",
    "        lstm_outputs = self.linear(lstm_outputs)\n",
    "        #print(\"lstm_outputs.shape\", lstm_outputs.shape)\n",
    "        \n",
    "        \n",
    "        return lstm_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "id": "5atlpQpiBa_n",
    "outputId": "b794ef15-a315-4476-bee9-d072c868c3f6"
   },
   "outputs": [],
   "source": [
    "class ImageCaptionsNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageCaptionsNet, self).__init__()        \n",
    "        ##CNN ENCODER RESNET-50        \n",
    "        self.Encoder = Encoder(embed_dim = embedding_dim)\n",
    "        ## RNN DECODER\n",
    "        self.Decoder = Decoder(embedding_dim, units, 1)    \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        image_batch, captions_batch = x\n",
    "        x = self.Encoder(image_batch)\n",
    "        x = self.Decoder(x, captions_batch)\n",
    "        return x\n",
    "    \n",
    "units = 512\n",
    "if restore == False:\n",
    "    net = ImageCaptionsNet()\n",
    "    net = net.double()\n",
    "    \n",
    "    if parallel == True and device != \"cpu\":\n",
    "        print(\"Parallel Processing enabled\")\n",
    "        net = nn.DataParallel(net)\n",
    "\n",
    "    if device == \"cpu\":\n",
    "        print(\"Device to CPU\")\n",
    "    else:\n",
    "        print(\"Device to CUDA\")\n",
    "        net = net.to(torch.device(\"cuda:0\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "za24CpL-Bc2_"
   },
   "outputs": [],
   "source": [
    "'''Save and Restore Checkpoints'''\n",
    "def create_checkpoint(path,model, optim_obj, loss_obj,iteration, epoch):\n",
    "    checkpoint = {'epoch': epoch,\n",
    "                  'iteration': iteration,\n",
    "                  'model_state_dict': model.module.state_dict()}\n",
    "\n",
    "    if platform == \"colab\":\n",
    "        directory = '/content/drive/My Drive/A4/comp_checkpoint/'\n",
    "    else:\n",
    "        directory = '../comp_checkpoint/'\n",
    "\n",
    "    torch.save(checkpoint, directory + path)\n",
    "    \n",
    "def restore_checkpoint(path):\n",
    "    new_state_dict = collections.OrderedDict()\n",
    "    if platform == \"colab\":\n",
    "        directory = '/content/drive/My Drive/A4/comp_checkpoint/'\n",
    "        checkpoint = torch.load(directory + path, map_location=torch.device('cpu'))\n",
    "    else:\n",
    "        directory = '../comp_checkpoint/'\n",
    "        checkpoint = torch.load(directory + path, map_location=torch.device('cpu'))    \n",
    "    \n",
    "    epoch = checkpoint['epoch']\n",
    "    new_state_dict = checkpoint['model_state_dict']\n",
    "    iteration = checkpoint['iteration']\n",
    "    #optimizer_state_dict = checkpoint['optimizer_state_dict']\n",
    "    #loss_obj = checkpoint['loss']\n",
    "    print(\"Iterations = {}, Epoch = {}\".format(iteration, epoch))\n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-a2e9cf3db975>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpytorch_total_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpytorch_trainable_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpytorch_total_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpytorch_trainable_params\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in net.parameters())\n",
    "pytorch_trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "pytorch_total_params, pytorch_trainable_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W9LV14UBX1NC"
   },
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 928
    },
    "colab_type": "code",
    "id": "kOPtPxyTEjTw",
    "outputId": "4cd94302-b359-40f2-ff9a-48cbae8895bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary Loaded Successfully\n",
      "ResNet152 Loaded Successfully..!\n",
      "VOCAB SIZE =  8680\n",
      "Iterations = 1209, Epoch = 3\n",
      "ResNet152 Loaded Successfully..!\n",
      "VOCAB SIZE =  8680\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ImageCaptionsNet:\n\tMissing key(s) in state_dict: \"Encoder.resnet101.0.weight\", \"Encoder.resnet101.1.weight\", \"Encoder.resnet101.1.bias\", \"Encoder.resnet101.1.running_mean\", \"Encoder.resnet101.1.running_var\", \"Encoder.resnet101.4.0.conv1.weight\", \"Encoder.resnet101.4.0.bn1.weight\", \"Encoder.resnet101.4.0.bn1.bias\", \"Encoder.resnet101.4.0.bn1.running_mean\", \"Encoder.resnet101.4.0.bn1.running_var\", \"Encoder.resnet101.4.0.conv2.weight\", \"Encoder.resnet101.4.0.bn2.weight\", \"Encoder.resnet101.4.0.bn2.bias\", \"Encoder.resnet101.4.0.bn2.running_mean\", \"Encoder.resnet101.4.0.bn2.running_var\", \"Encoder.resnet101.4.1.conv1.weight\", \"Encoder.resnet101.4.1.bn1.weight\", \"Encoder.resnet101.4.1.bn1.bias\", \"Encoder.resnet101.4.1.bn1.running_mean\", \"Encoder.resnet101.4.1.bn1.running_var\", \"Encoder.resnet101.4.1.conv2.weight\", \"Encoder.resnet101.4.1.bn2.weight\", \"Encoder.resnet101.4.1.bn2.bias\", \"Encoder.resnet101.4.1.bn2.running_mean\", \"Encoder.resnet101.4.1.bn2.running_var\", \"Encoder.resnet101.4.2.conv1.weight\", \"Encoder.resnet101.4.2.bn1.weight\", \"Encoder.resnet101.4.2.bn1.bias\", \"Encoder.resnet101.4.2.bn1.running_mean\", \"Encoder.resnet101.4.2.bn1.running_var\", \"Encoder.resnet101.4.2.conv2.weight\", \"Encoder.resnet101.4.2.bn2.weight\", \"Encoder.resnet101.4.2.bn2.bias\", \"Encoder.resnet101.4.2.bn2.running_mean\", \"Encoder.resnet101.4.2.bn2.running_var\", \"Encoder.resnet101.5.0.conv1.weight\", \"Encoder.resnet101.5.0.bn1.weight\", \"Encoder.resnet101.5.0.bn1.bias\", \"Encoder.resnet101.5.0.bn1.running_mean\", \"Encoder.resnet101.5.0.bn1.running_var\", \"Encoder.resnet101.5.0.conv2.weight\", \"Encoder.resnet101.5.0.bn2.weight\", \"Encoder.resnet101.5.0.bn2.bias\", \"Encoder.resnet101.5.0.bn2.running_mean\", \"Encoder.resnet101.5.0.bn2.running_var\", \"Encoder.resnet101.5.0.downsample.0.weight\", \"Encoder.resnet101.5.0.downsample.1.weight\", \"Encoder.resnet101.5.0.downsample.1.bias\", \"Encoder.resnet101.5.0.downsample.1.running_mean\", \"Encoder.resnet101.5.0.downsample.1.running_var\", \"Encoder.resnet101.5.1.conv1.weight\", \"Encoder.resnet101.5.1.bn1.weight\", \"Encoder.resnet101.5.1.bn1.bias\", \"Encoder.resnet101.5.1.bn1.running_mean\", \"Encoder.resnet101.5.1.bn1.running_var\", \"Encoder.resnet101.5.1.conv2.weight\", \"Encoder.resnet101.5.1.bn2.weight\", \"Encoder.resnet101.5.1.bn2.bias\", \"Encoder.resnet101.5.1.bn2.running_mean\", \"Encoder.resnet101.5.1.bn2.running_var\", \"Encoder.resnet101.5.2.conv1.weight\", \"Encoder.resnet101.5.2.bn1.weight\", \"Encoder.resnet101.5.2.bn1.bias\", \"Encoder.resnet101.5.2.bn1.running_mean\", \"Encoder.resnet101.5.2.bn1.running_var\", \"Encoder.resnet101.5.2.conv2.weight\", \"Encoder.resnet101.5.2.bn2.weight\", \"Encoder.resnet101.5.2.bn2.bias\", \"Encoder.resnet101.5.2.bn2.running_mean\", \"Encoder.resnet101.5.2.bn2.running_var\", \"Encoder.resnet101.5.3.conv1.weight\", \"Encoder.resnet101.5.3.bn1.weight\", \"Encoder.resnet101.5.3.bn1.bias\", \"Encoder.resnet101.5.3.bn1.running_mean\", \"Encoder.resnet101.5.3.bn1.running_var\", \"Encoder.resnet101.5.3.conv2.weight\", \"Encoder.resnet101.5.3.bn2.weight\", \"Encoder.resnet101.5.3.bn2.bias\", \"Encoder.resnet101.5.3.bn2.running_mean\", \"Encoder.resnet101.5.3.bn2.running_var\", \"Encoder.resnet101.6.0.conv1.weight\", \"Encoder.resnet101.6.0.bn1.weight\", \"Encoder.resnet101.6.0.bn1.bias\", \"Encoder.resnet101.6.0.bn1.running_mean\", \"Encoder.resnet101.6.0.bn1.running_var\", \"Encoder.resnet101.6.0.conv2.weight\", \"Encoder.resnet101.6.0.bn2.weight\", \"Encoder.resnet101.6.0.bn2.bias\", \"Encoder.resnet101.6.0.bn2.running_mean\", \"Encoder.resnet101.6.0.bn2.running_var\", \"Encoder.resnet101.6.0.downsample.0.weight\", \"Encoder.resnet101.6.0.downsample.1.weight\", \"Encoder.resnet101.6.0.downsample.1.bias\", \"Encoder.resnet101.6.0.downsample.1.running_mean\", \"Encoder.resnet101.6.0.downsample.1.running_var\", \"Encoder.resnet101.6.1.conv1.weight\", \"Encoder.resnet101.6.1.bn1.weight\", \"Encoder.resnet101.6.1.bn1.bias\", \"Encoder.resnet101.6.1.bn1.running_mean\", \"Encoder.resnet101.6.1.bn1.running_var\", \"Encoder.resnet101.6.1.conv2.weight\", \"Encoder.resnet101.6.1.bn2.weight\", \"Encoder.resnet101.6.1.bn2.bias\", \"Encoder.resnet101.6.1.bn2.running_mean\", \"Encoder.resnet101.6.1.bn2.running_var\", \"Encoder.resnet101.6.2.conv1.weight\", \"Encoder.resnet101.6.2.bn1.weight\", \"Encoder.resnet101.6.2.bn1.bias\", \"Encoder.resnet101.6.2.bn1.running_mean\", \"Encoder.resnet101.6.2.bn1.running_var\", \"Encoder.resnet101.6.2.conv2.weight\", \"Encoder.resnet101.6.2.bn2.weight\", \"Encoder.resnet101.6.2.bn2.bias\", \"Encoder.resnet101.6.2.bn2.running_mean\", \"Encoder.resnet101.6.2.bn2.running_var\", \"Encoder.resnet101.6.3.conv1.weight\", \"Encoder.resnet101.6.3.bn1.weight\", \"Encoder.resnet101.6.3.bn1.bias\", \"Encoder.resnet101.6.3.bn1.running_mean\", \"Encoder.resnet101.6.3.bn1.running_var\", \"Encoder.resnet101.6.3.conv2.weight\", \"Encoder.resnet101.6.3.bn2.weight\", \"Encoder.resnet101.6.3.bn2.bias\", \"Encoder.resnet101.6.3.bn2.running_mean\", \"Encoder.resnet101.6.3.bn2.running_var\", \"Encoder.resnet101.6.4.conv1.weight\", \"Encoder.resnet101.6.4.bn1.weight\", \"Encoder.resnet101.6.4.bn1.bias\", \"Encoder.resnet101.6.4.bn1.running_mean\", \"Encoder.resnet101.6.4.bn1.running_var\", \"Encoder.resnet101.6.4.conv2.weight\", \"Encoder.resnet101.6.4.bn2.weight\", \"Encoder.resnet101.6.4.bn2.bias\", \"Encoder.resnet101.6.4.bn2.running_mean\", \"Encoder.resnet101.6.4.bn2.running_var\", \"Encoder.resnet101.6.5.conv1.weight\", \"Encoder.resnet101.6.5.bn1.weight\", \"Encoder.resnet101.6.5.bn1.bias\", \"Encoder.resnet101.6.5.bn1.running_mean\", \"Encoder.resnet101.6.5.bn1.running_var\", \"Encoder.resnet101.6.5.conv2.weight\", \"Encoder.resnet101.6.5.bn2.weight\", \"Encoder.resnet101.6.5.bn2.bias\", \"Encoder.resnet101.6.5.bn2.running_mean\", \"Encoder.resnet101.6.5.bn2.running_var\", \"Encoder.resnet101.7.0.conv1.weight\", \"Encoder.resnet101.7.0.bn1.weight\", \"Encoder.resnet101.7.0.bn1.bias\", \"Encoder.resnet101.7.0.bn1.running_mean\", \"Encoder.resnet101.7.0.bn1.running_var\", \"Encoder.resnet101.7.0.conv2.weight\", \"Encoder.resnet101.7.0.bn2.weight\", \"Encoder.resnet101.7.0.bn2.bias\", \"Encoder.resnet101.7.0.bn2.running_mean\", \"Encoder.resnet101.7.0.bn2.running_var\", \"Encoder.resnet101.7.0.downsample.0.weight\", \"Encoder.resnet101.7.0.downsample.1.weight\", \"Encoder.resnet101.7.0.downsample.1.bias\", \"Encoder.resnet101.7.0.downsample.1.running_mean\", \"Encoder.resnet101.7.0.downsample.1.running_var\", \"Encoder.resnet101.7.1.conv1.weight\", \"Encoder.resnet101.7.1.bn1.weight\", \"Encoder.resnet101.7.1.bn1.bias\", \"Encoder.resnet101.7.1.bn1.running_mean\", \"Encoder.resnet101.7.1.bn1.running_var\", \"Encoder.resnet101.7.1.conv2.weight\", \"Encoder.resnet101.7.1.bn2.weight\", \"Encoder.resnet101.7.1.bn2.bias\", \"Encoder.resnet101.7.1.bn2.running_mean\", \"Encoder.resnet101.7.1.bn2.running_var\", \"Encoder.resnet101.7.2.conv1.weight\", \"Encoder.resnet101.7.2.bn1.weight\", \"Encoder.resnet101.7.2.bn1.bias\", \"Encoder.resnet101.7.2.bn1.running_mean\", \"Encoder.resnet101.7.2.bn1.running_var\", \"Encoder.resnet101.7.2.conv2.weight\", \"Encoder.resnet101.7.2.bn2.weight\", \"Encoder.resnet101.7.2.bn2.bias\", \"Encoder.resnet101.7.2.bn2.running_mean\", \"Encoder.resnet101.7.2.bn2.running_var\", \"Encoder.fc.weight\", \"Encoder.fc.bias\". \n\tUnexpected key(s) in state_dict: \"Encoder.resnet50.conv1.weight\", \"Encoder.resnet50.conv1.bias\", \"Encoder.resnet50.batch_norm1.weight\", \"Encoder.resnet50.batch_norm1.bias\", \"Encoder.resnet50.batch_norm1.running_mean\", \"Encoder.resnet50.batch_norm1.running_var\", \"Encoder.resnet50.batch_norm1.num_batches_tracked\", \"Encoder.resnet50.conv_block1.conv1.weight\", \"Encoder.resnet50.conv_block1.conv1.bias\", \"Encoder.resnet50.conv_block1.bn1.weight\", \"Encoder.resnet50.conv_block1.bn1.bias\", \"Encoder.resnet50.conv_block1.bn1.running_mean\", \"Encoder.resnet50.conv_block1.bn1.running_var\", \"Encoder.resnet50.conv_block1.bn1.num_batches_tracked\", \"Encoder.resnet50.conv_block1.conv2.weight\", \"Encoder.resnet50.conv_block1.conv2.bias\", \"Encoder.resnet50.conv_block1.bn2.weight\", \"Encoder.resnet50.conv_block1.bn2.bias\", \"Encoder.resnet50.conv_block1.bn2.running_mean\", \"Encoder.resnet50.conv_block1.bn2.running_var\", \"Encoder.resnet50.conv_block1.bn2.num_batches_tracked\", \"Encoder.resnet50.conv_block1.conv3.weight\", \"Encoder.resnet50.conv_block1.conv3.bias\", \"Encoder.resnet50.conv_block1.bn3.weight\", \"Encoder.resnet50.conv_block1.bn3.bias\", \"Encoder.resnet50.conv_block1.bn3.running_mean\", \"Encoder.resnet50.conv_block1.bn3.running_var\", \"Encoder.resnet50.conv_block1.bn3.num_batches_tracked\", \"Encoder.resnet50.conv_block1.conv4.weight\", \"Encoder.resnet50.conv_block1.conv4.bias\", \"Encoder.resnet50.conv_block1.bn4.weight\", \"Encoder.resnet50.conv_block1.bn4.bias\", \"Encoder.resnet50.conv_block1.bn4.running_mean\", \"Encoder.resnet50.conv_block1.bn4.running_var\", \"Encoder.resnet50.conv_block1.bn4.num_batches_tracked\", \"Encoder.resnet50.residual_block1.conv1.weight\", \"Encoder.resnet50.residual_block1.conv1.bias\", \"Encoder.resnet50.residual_block1.bn1.weight\", \"Encoder.resnet50.residual_block1.bn1.bias\", \"Encoder.resnet50.residual_block1.bn1.running_mean\", \"Encoder.resnet50.residual_block1.bn1.running_var\", \"Encoder.resnet50.residual_block1.bn1.num_batches_tracked\", \"Encoder.resnet50.residual_block1.conv2.weight\", \"Encoder.resnet50.residual_block1.conv2.bias\", \"Encoder.resnet50.residual_block1.bn2.weight\", \"Encoder.resnet50.residual_block1.bn2.bias\", \"Encoder.resnet50.residual_block1.bn2.running_mean\", \"Encoder.resnet50.residual_block1.bn2.running_var\", \"Encoder.resnet50.residual_block1.bn2.num_batches_tracked\", \"Encoder.resnet50.residual_block1.conv3.weight\", \"Encoder.resnet50.residual_block1.conv3.bias\", \"Encoder.resnet50.residual_block1.bn3.weight\", \"Encoder.resnet50.residual_block1.bn3.bias\", \"Encoder.resnet50.residual_block1.bn3.running_mean\", \"Encoder.resnet50.residual_block1.bn3.running_var\", \"Encoder.resnet50.residual_block1.bn3.num_batches_tracked\", \"Encoder.resnet50.conv_block2.conv1.weight\", \"Encoder.resnet50.conv_block2.conv1.bias\", \"Encoder.resnet50.conv_block2.bn1.weight\", \"Encoder.resnet50.conv_block2.bn1.bias\", \"Encoder.resnet50.conv_block2.bn1.running_mean\", \"Encoder.resnet50.conv_block2.bn1.running_var\", \"Encoder.resnet50.conv_block2.bn1.num_batches_tracked\", \"Encoder.resnet50.conv_block2.conv2.weight\", \"Encoder.resnet50.conv_block2.conv2.bias\", \"Encoder.resnet50.conv_block2.bn2.weight\", \"Encoder.resnet50.conv_block2.bn2.bias\", \"Encoder.resnet50.conv_block2.bn2.running_mean\", \"Encoder.resnet50.conv_block2.bn2.running_var\", \"Encoder.resnet50.conv_block2.bn2.num_batches_tracked\", \"Encoder.resnet50.conv_block2.conv3.weight\", \"Encoder.resnet50.conv_block2.conv3.bias\", \"Encoder.resnet50.conv_block2.bn3.weight\", \"Encoder.resnet50.conv_block2.bn3.bias\", \"Encoder.resnet50.conv_block2.bn3.running_mean\", \"Encoder.resnet50.conv_block2.bn3.running_var\", \"Encoder.resnet50.conv_block2.bn3.num_batches_tracked\", \"Encoder.resnet50.conv_block2.conv4.weight\", \"Encoder.resnet50.conv_block2.conv4.bias\", \"Encoder.resnet50.conv_block2.bn4.weight\", \"Encoder.resnet50.conv_block2.bn4.bias\", \"Encoder.resnet50.conv_block2.bn4.running_mean\", \"Encoder.resnet50.conv_block2.bn4.running_var\", \"Encoder.resnet50.conv_block2.bn4.num_batches_tracked\", \"Encoder.resnet50.residual_block2.conv1.weight\", \"Encoder.resnet50.residual_block2.conv1.bias\", \"Encoder.resnet50.residual_block2.bn1.weight\", \"Encoder.resnet50.residual_block2.bn1.bias\", \"Encoder.resnet50.residual_block2.bn1.running_mean\", \"Encoder.resnet50.residual_block2.bn1.running_var\", \"Encoder.resnet50.residual_block2.bn1.num_batches_tracked\", \"Encoder.resnet50.residual_block2.conv2.weight\", \"Encoder.resnet50.residual_block2.conv2.bias\", \"Encoder.resnet50.residual_block2.bn2.weight\", \"Encoder.resnet50.residual_block2.bn2.bias\", \"Encoder.resnet50.residual_block2.bn2.running_mean\", \"Encoder.resnet50.residual_block2.bn2.running_var\", \"Encoder.resnet50.residual_block2.bn2.num_batches_tracked\", \"Encoder.resnet50.residual_block2.conv3.weight\", \"Encoder.resnet50.residual_block2.conv3.bias\", \"Encoder.resnet50.residual_block2.bn3.weight\", \"Encoder.resnet50.residual_block2.bn3.bias\", \"Encoder.resnet50.residual_block2.bn3.running_mean\", \"Encoder.resnet50.residual_block2.bn3.running_var\", \"Encoder.resnet50.residual_block2.bn3.num_batches_tracked\", \"Encoder.resnet50.conv_block3.conv1.weight\", \"Encoder.resnet50.conv_block3.conv1.bias\", \"Encoder.resnet50.conv_block3.bn1.weight\", \"Encoder.resnet50.conv_block3.bn1.bias\", \"Encoder.resnet50.conv_block3.bn1.running_mean\", \"Encoder.resnet50.conv_block3.bn1.running_var\", \"Encoder.resnet50.conv_block3.bn1.num_batches_tracked\", \"Encoder.resnet50.conv_block3.conv2.weight\", \"Encoder.resnet50.conv_block3.conv2.bias\", \"Encoder.resnet50.conv_block3.bn2.weight\", \"Encoder.resnet50.conv_block3.bn2.bias\", \"Encoder.resnet50.conv_block3.bn2.running_mean\", \"Encoder.resnet50.conv_block3.bn2.running_var\", \"Encoder.resnet50.conv_block3.bn2.num_batches_tracked\", \"Encoder.resnet50.conv_block3.conv3.weight\", \"Encoder.resnet50.conv_block3.conv3.bias\", \"Encoder.resnet50.conv_block3.bn3.weight\", \"Encoder.resnet50.conv_block3.bn3.bias\", \"Encoder.resnet50.conv_block3.bn3.running_mean\", \"Encoder.resnet50.conv_block3.bn3.running_var\", \"Encoder.resnet50.conv_block3.bn3.num_batches_tracked\", \"Encoder.resnet50.conv_block3.conv4.weight\", \"Encoder.resnet50.conv_block3.conv4.bias\", \"Encoder.resnet50.conv_block3.bn4.weight\", \"Encoder.resnet50.conv_block3.bn4.bias\", \"Encoder.resnet50.conv_block3.bn4.running_mean\", \"Encoder.resnet50.conv_block3.bn4.running_var\", \"Encoder.resnet50.conv_block3.bn4.num_batches_tracked\", \"Encoder.resnet50.residual_block3.conv1.weight\", \"Encoder.resnet50.residual_block3.conv1.bias\", \"Encoder.resnet50.residual_block3.bn1.weight\", \"Encoder.resnet50.residual_block3.bn1.bias\", \"Encoder.resnet50.residual_block3.bn1.running_mean\", \"Encoder.resnet50.residual_block3.bn1.running_var\", \"Encoder.resnet50.residual_block3.bn1.num_batches_tracked\", \"Encoder.resnet50.residual_block3.conv2.weight\", \"Encoder.resnet50.residual_block3.conv2.bias\", \"Encoder.resnet50.residual_block3.bn2.weight\", \"Encoder.resnet50.residual_block3.bn2.bias\", \"Encoder.resnet50.residual_block3.bn2.running_mean\", \"Encoder.resnet50.residual_block3.bn2.running_var\", \"Encoder.resnet50.residual_block3.bn2.num_batches_tracked\", \"Encoder.resnet50.residual_block3.conv3.weight\", \"Encoder.resnet50.residual_block3.conv3.bias\", \"Encoder.resnet50.residual_block3.bn3.weight\", \"Encoder.resnet50.residual_block3.bn3.bias\", \"Encoder.resnet50.residual_block3.bn3.running_mean\", \"Encoder.resnet50.residual_block3.bn3.running_var\", \"Encoder.resnet50.residual_block3.bn3.num_batches_tracked\", \"Encoder.resnet50.conv_block4.conv1.weight\", \"Encoder.resnet50.conv_block4.conv1.bias\", \"Encoder.resnet50.conv_block4.bn1.weight\", \"Encoder.resnet50.conv_block4.bn1.bias\", \"Encoder.resnet50.conv_block4.bn1.running_mean\", \"Encoder.resnet50.conv_block4.bn1.running_var\", \"Encoder.resnet50.conv_block4.bn1.num_batches_tracked\", \"Encoder.resnet50.conv_block4.conv2.weight\", \"Encoder.resnet50.conv_block4.conv2.bias\", \"Encoder.resnet50.conv_block4.bn2.weight\", \"Encoder.resnet50.conv_block4.bn2.bias\", \"Encoder.resnet50.conv_block4.bn2.running_mean\", \"Encoder.resnet50.conv_block4.bn2.running_var\", \"Encoder.resnet50.conv_block4.bn2.num_batches_tracked\", \"Encoder.resnet50.conv_block4.conv3.weight\", \"Encoder.resnet50.conv_block4.conv3.bias\", \"Encoder.resnet50.conv_block4.bn3.weight\", \"Encoder.resnet50.conv_block4.bn3.bias\", \"Encoder.resnet50.conv_block4.bn3.running_mean\", \"Encoder.resnet50.conv_block4.bn3.running_var\", \"Encoder.resnet50.conv_block4.bn3.num_batches_tracked\", \"Encoder.resnet50.conv_block4.conv4.weight\", \"Encoder.resnet50.conv_block4.conv4.bias\", \"Encoder.resnet50.conv_block4.bn4.weight\", \"Encoder.resnet50.conv_block4.bn4.bias\", \"Encoder.resnet50.conv_block4.bn4.running_mean\", \"Encoder.resnet50.conv_block4.bn4.running_var\", \"Encoder.resnet50.conv_block4.bn4.num_batches_tracked\", \"Encoder.resnet50.residual_block4.conv1.weight\", \"Encoder.resnet50.residual_block4.conv1.bias\", \"Encoder.resnet50.residual_block4.bn1.weight\", \"Encoder.resnet50.residual_block4.bn1.bias\", \"Encoder.resnet50.residual_block4.bn1.running_mean\", \"Encoder.resnet50.residual_block4.bn1.running_var\", \"Encoder.resnet50.residual_block4.bn1.num_batches_tracked\", \"Encoder.resnet50.residual_block4.conv2.weight\", \"Encoder.resnet50.residual_block4.conv2.bias\", \"Encoder.resnet50.residual_block4.bn2.weight\", \"Encoder.resnet50.residual_block4.bn2.bias\", \"Encoder.resnet50.residual_block4.bn2.running_mean\", \"Encoder.resnet50.residual_block4.bn2.running_var\", \"Encoder.resnet50.residual_block4.bn2.num_batches_tracked\", \"Encoder.resnet50.residual_block4.conv3.weight\", \"Encoder.resnet50.residual_block4.conv3.bias\", \"Encoder.resnet50.residual_block4.bn3.weight\", \"Encoder.resnet50.residual_block4.bn3.bias\", \"Encoder.resnet50.residual_block4.bn3.running_mean\", \"Encoder.resnet50.residual_block4.bn3.running_var\", \"Encoder.resnet50.residual_block4.bn3.num_batches_tracked\", \"Encoder.resnet50.fc1.weight\", \"Encoder.resnet50.fc1.bias\", \"Decoder.attention.W1.weight\", \"Decoder.attention.W1.bias\", \"Decoder.attention.W2.weight\", \"Decoder.attention.W2.bias\", \"Decoder.attention.V.weight\", \"Decoder.attention.V.bias\". \n\tsize mismatch for Decoder.lstm.weight_ih_l0: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([2048, 300]).\n\tsize mismatch for Decoder.embed.weight: copying a param with shape torch.Size([8680, 256]) from checkpoint, the shape in current model is torch.Size([8680, 300]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-8f8a584fcfd6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageCaptionsNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"State Dictionary Loaded Successfully.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1043\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m-> 1045\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m   1046\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ImageCaptionsNet:\n\tMissing key(s) in state_dict: \"Encoder.resnet101.0.weight\", \"Encoder.resnet101.1.weight\", \"Encoder.resnet101.1.bias\", \"Encoder.resnet101.1.running_mean\", \"Encoder.resnet101.1.running_var\", \"Encoder.resnet101.4.0.conv1.weight\", \"Encoder.resnet101.4.0.bn1.weight\", \"Encoder.resnet101.4.0.bn1.bias\", \"Encoder.resnet101.4.0.bn1.running_mean\", \"Encoder.resnet101.4.0.bn1.running_var\", \"Encoder.resnet101.4.0.conv2.weight\", \"Encoder.resnet101.4.0.bn2.weight\", \"Encoder.resnet101.4.0.bn2.bias\", \"Encoder.resnet101.4.0.bn2.running_mean\", \"Encoder.resnet101.4.0.bn2.running_var\", \"Encoder.resnet101.4.1.conv1.weight\", \"Encoder.resnet101.4.1.bn1.weight\", \"Encoder.resnet101.4.1.bn1.bias\", \"Encoder.resnet101.4.1.bn1.running_mean\", \"Encoder.resnet101.4.1.bn1.running_var\", \"Encoder.resnet101.4.1.conv2.weight\", \"Encoder.resnet101.4.1.bn2.weight\", \"Encoder.resnet101.4.1.bn2.bias\", \"Encoder.resnet101.4.1.bn2.running_mean\", \"Encoder.resnet101.4.1.bn2.running_var\", \"Encoder.resnet101.4.2.conv1.weight\", \"Encoder.resnet101.4.2.bn1.weight\", \"Encoder.resnet101.4.2.bn1.bias\", \"Encoder.resnet101.4.2.bn1.running_mean\", \"Encoder.resnet101.4.2.bn1.running_var\", \"Encoder.resnet101.4.2.conv2.weight\", \"Encoder.resnet101.4.2.bn2.weight\", \"Encoder.resnet101.4.2.bn2.bias\", \"Encoder.resnet101.4.2.bn2.running_mean\", \"Encoder.resnet101.4.2.bn2.running_var\", \"Encoder.resnet101.5.0.conv1.weight\", \"Encoder.resnet101.5.0.bn1.weight\", \"Encoder.resnet101.5.0.bn1.bias\", \"Encoder.resnet101.5.0.bn1.running_mean\", \"Encoder.resnet101.5.0.bn1.running_var\", \"Encoder.resnet101.5.0.conv2.weight\", \"Encoder.resnet101.5.0.bn2.weight\", \"Encoder.resnet101.5.0.bn2.bias\", \"Encoder.resnet101.5.0.bn2.running_mean\", \"Encoder.resnet101.5.0.bn2.running_var\", \"Encoder.resnet101.5.0.downsample.0.weight\", \"Encoder.resnet101.5.0.downsample.1.weight\", \"Encoder.resnet101.5.0.downsample.1.bias\", \"Encoder.resnet101.5.0.downsample.1.running_mean\", \"Encoder.resnet101.5.0.downsample.1.running_var\", \"Encoder.resnet101.5.1.conv1.weight\", \"Encoder.resnet101.5.1.bn1.weight\", \"Encoder.resnet101.5.1.bn1.bias\", \"Encoder.resnet101.5.1.bn1.running_mean\", \"Encoder.resnet101.5.1.bn1.running_var\", \"Encoder.resnet101.5.1.conv2.weight\", \"Encoder.resnet101.5.1.bn2.weight\", \"Encoder.resnet101.5.1.bn2.bias\", \"Encoder.resnet101.5.1.bn2.running_mean\", \"Encoder.resnet101.5.1.bn2.running_var\", \"Encoder.resnet101.5.2.conv1.weight\", \"Encoder.resnet101.5.2.bn1.weight\", \"Encoder.resnet101.5.2.bn1.bias\", \"Encoder.resnet101.5.2.bn1.running_mean\", \"Encoder.resnet101.5.2.bn1.running_var\", \"Encoder.resnet101.5.2.conv2.weight\", \"Encoder.resnet101.5.2.bn2.weight\", \"Encoder.resnet101.5.2.bn2.bias\", \"Encoder.resnet101.5.2.bn2.running_mean\", \"Encoder.resnet101.5.2.bn2.running_var\", \"Encoder.resnet101.5.3.conv1.weight\", \"Encoder.resnet101.5.3.bn1.weight\", \"Encoder.resnet101.5.3.bn1.bias\", \"Encoder.resnet101.5.3.bn1.running_mean\", \"Encoder.resnet101.5.3.bn1.running_var\", \"Encoder.resnet101.5.3.conv2.weight\", \"Encoder.resnet101.5.3.bn2.weight\", \"Encoder.resnet101.5.3.bn2.bias\", \"Encoder.resnet101.5.3.bn2.running_mean\", \"Encoder.resnet101.5.3.bn2.running_var\", \"Encoder.resnet101.6.0.conv1.weight\", \"Encoder.resnet101.6.0.bn1.weight\", \"Encoder.resnet101.6.0.bn1.bias\", \"Encoder.resnet101.6.0.bn1.running_mean\", \"Encoder.resnet101.6.0.bn1.running_var\", \"Encoder.resnet101.6.0.conv2.weight\", \"Encoder.resnet101.6.0.bn2.weight\", \"Encoder.resnet101.6.0.bn2.bias\", \"Encoder.resnet101.6.0.bn2.running_mean\", \"Encoder.resnet101.6.0.bn2.running_var\", \"Encoder.resnet101.6.0.downsample.0.weight\", \"Encoder.resnet101.6.0.downsample.1.weight\", \"Encoder.resnet101.6.0.downsample.1.bias\", \"Encoder.resnet101.6.0.downsample.1.running_mean\", \"Encoder.resnet101.6.0.downsample.1.running_var\", \"Encoder.resnet101.6.1.conv1.weight\", \"Encoder.resnet101.6.1.bn1.weight\", \"Encoder.resnet101.6.1.bn1.bias\", \"Encoder.resnet101.6.1.bn1.running_mean\", \"Encoder.resnet101.6.1.bn1.running_var\", \"Encoder.resnet101.6.1.conv2.weight\", \"Encoder.resnet101.6.1.bn2.weight\", \"Encoder.resnet101.6.1.bn2.bias\", \"Encoder.resnet101.6.1.bn2.running_mean\", \"Encoder.resnet101.6.1.bn2.running_var\", \"Encoder.resnet101.6.2.conv1.weight\", \"Encoder.resnet101.6.2.bn1.weight\", \"Encoder.resnet101.6.2.bn1.bias\", \"Encoder.resnet101.6.2.bn1.running_mean\", \"Encoder.resnet101.6.2.bn1.running_var\", \"Encoder.resnet101.6.2.conv2.weight\", \"Encoder.resnet101.6.2.bn2.weight\", \"Encoder.resnet101.6.2.bn2.bias\", \"Encoder.resnet101.6.2.bn2.running_mean\", \"Encoder.resnet101.6.2.bn2.running_var\", \"Encoder.resnet101.6.3.conv1.weight\", \"Encoder.resnet101.6.3.bn1.weight\", \"Encoder.resnet101.6.3.bn1.bias\", \"Encoder.resnet101.6.3.bn1.running_mean\", \"Encoder.resnet101.6.3.bn1.running_var\", \"Encoder.resnet101.6.3.conv2.weight\", \"Encoder.resnet101.6.3.bn2.weight\", \"Encoder.resnet101.6.3.bn2.bias\", \"Encoder.resnet101.6.3.bn2.running_mean\", \"Encoder.resnet101.6.3.bn2.running_var\", \"Encoder.resnet101.6.4.conv1.weight\", \"Encoder.resnet101.6.4.bn1.weight\", \"Encoder.resnet101.6.4.bn1.bias\", \"Encoder.resnet101.6.4.bn1.running_mean\", \"Encoder.resnet101.6.4.bn1.running_var\", \"Encoder.resnet101.6.4.conv2.weight\", \"Encoder.resnet101.6.4.bn2.weight\", \"Encoder.resnet101.6.4.bn2.bias\", \"Encoder.resnet101.6.4.bn2.running_mean\", \"Encoder.resnet101.6.4.bn2.running_var\", \"Encoder.resnet101.6.5.conv1.weight\", \"Encoder.resnet101.6.5.bn1.weight\", \"Encoder.resnet101.6.5.bn1.bias\", \"Encoder.resnet101.6.5.bn1.running_mean\", \"Encoder.resnet101.6.5.bn1.running_var\", \"Encoder.resnet101.6.5.conv2.weight\", \"Encoder.resnet101.6.5.bn2.weight\", \"Encoder.resnet101.6.5.bn2.bias\", \"Encoder.resnet101.6.5.bn2.running_mean\", \"Encoder.resnet101.6.5.bn2.running_var\", \"Encoder.resnet101.7.0.conv1.weight\", \"Encoder.resnet101.7.0.bn1.weight\", \"Encoder.resnet101.7.0.bn1.bias\", \"Encoder.resnet101.7.0.bn1.running_mean\", \"Encoder.resnet101.7.0.bn1.running_var\", \"Encoder.resnet101.7.0.conv2.weight\", \"Encoder.resnet101.7.0.bn2.weight\", \"Encoder.resnet101.7.0.bn2.bias\", \"Encoder.resnet101.7.0.bn2.running_mean\", \"Encoder.resnet101.7.0.bn2.running_var\", \"Encoder.resnet101.7.0.downsample.0.weight\", \"Encoder.resnet101.7.0.downsample.1.weight\", \"Encoder.resnet101.7.0.downsample.1.bias\", \"Encoder.resnet101.7.0.downsample.1.running_mean\", \"Encoder.resnet101.7.0.downsample.1.running_var\", \"Encoder.resnet101.7.1.conv1.weight\", \"Encoder.resnet101.7.1.bn1.weight\", \"Encoder.resnet101.7.1.bn1.bias\", \"Encoder.resnet101.7.1.bn1.running_mean\", \"Encoder.resnet101.7.1.bn1.running_var\", \"Encoder.resnet101.7.1.conv2.weight\", \"Encoder.resnet101.7.1.bn2.weight\", \"Encoder.resnet101.7.1.bn2.bias\", \"Encoder.resnet101.7.1.bn2.running_mean\", \"Encoder.resnet101.7.1.bn2.running_var\", \"Encoder.resnet101.7.2.conv1.weight\", \"Encoder.resnet101.7.2.bn1.weight\", \"Encoder.resnet101.7.2.bn1.bias\", \"Encoder.resnet101.7.2.bn1.running_mean\", \"Encoder.resnet101.7.2.bn1.running_var\", \"Encoder.resnet101.7.2.conv2.weight\", \"Encoder.resnet101.7.2.bn2.weight\", \"Encoder.resnet101.7.2.bn2.bias\", \"Encoder.resnet101.7.2.bn2.running_mean\", \"Encoder.resnet101.7.2.bn2.running_var\", \"Encoder.fc.weight\", \"Encoder.fc.bias\". \n\tUnexpected key(s) in state_dict: \"Encoder.resnet50.conv1.weight\", \"Encoder.resnet50.conv1.bias\", \"Encoder.resnet50.batch_norm1.weight\", \"Encoder.resnet50.batch_norm1.bias\", \"Encoder.resnet50.batch_norm1.running_mean\", \"Encoder.resnet50.batch_norm1.running_var\", \"Encoder.resnet50.batch_norm1.num_batches_tracked\", \"Encoder.resnet50.conv_block1.conv1.weight\", \"Encoder.resnet50.conv_block1.conv1.bias\", \"Encoder.resnet50.conv_block1.bn1.weight\", \"Encoder.resnet50.conv_block1.bn1.bias\", \"Encoder.resnet50.conv_block1.bn1.running_mean\", \"Encoder.resnet50.conv_block1.bn1.running_var\", \"Encoder.resnet50.conv_block1.bn1.num_batches_tracked\", \"Encoder.resnet50.conv_block1.conv2.weight\", \"Encoder.resnet50.conv_block1.conv2.bias\", \"Encoder.resnet50.conv_block1.bn2.weight\", \"Encoder.resnet50.conv_block1.bn2.bias\", \"Encoder.resnet50.conv_block1.bn2.running_mean\", \"Encoder.resnet50.conv_block1.bn2.running_var\", \"Encoder.resnet50.conv_block1.bn2.num_batches_tracked\", \"Encoder.resnet50.conv_block1.conv3.weight\", \"Encoder.resnet50.conv_block1.conv3.bias\", \"Encoder.resnet50.conv_block1.bn3.weight\", \"Encoder.resnet50.conv_block1.bn3.bias\", \"Encoder.resnet50.conv_block1.bn3.running_mean\", \"Encoder.resnet50.conv_block1.bn3.running_var\", \"Encoder.resnet50.conv_block1.bn3.num_batches_tracked\", \"Encoder.resnet50.conv_block1.conv4.weight\", \"Encoder.resnet50.conv_block1.conv4.bias\", \"Encoder.resnet50.conv_block1.bn4.weight\", \"Encoder.resnet50.conv_block1.bn4.bias\", \"Encoder.resnet50.conv_block1.bn4.running_mean\", \"Encoder.resnet50.conv_block1.bn4.running_var\", \"Encoder.resnet50.conv_block1.bn4.num_batches_tracked\", \"Encoder.resnet50.residual_block1.conv1.weight\", \"Encoder.resnet50.residual_block1.conv1.bias\", \"Encoder.resnet50.residual_block1.bn1.weight\", \"Encoder.resnet50.residual_block1.bn1.bias\", \"Encoder.resnet50.residual_block1.bn1.running_mean\", \"Encoder.resnet50.residual_block1.bn1.running_var\", \"Encoder.resnet50.residual_block1.bn1.num_batches_tracked\", \"Encoder.resnet50.residual_block1.conv2.weight\", \"Encoder.resnet50.residual_block1.conv2.bias\", \"Encoder.resnet50.residual_block1.bn2.weight\", \"Encoder.resnet50.residual_block1.bn2.bias\", \"Encoder.resnet50.residual_block1.bn2.running_mean\", \"Encoder.resnet50.residual_block1.bn2.running_var\", \"Encoder.resnet50.residual_block1.bn2.num_batches_tracked\", \"Encoder.resnet50.residual_block1.conv3.weight\", \"Encoder.resnet50.residual_block1.conv3.bias\", \"Encoder.resnet50.residual_block1.bn3.weight\", \"Encoder.resnet50.residual_block1.bn3.bias\", \"Encoder.resnet50.residual_block1.bn3.running_mean\", \"Encoder.resnet50.residual_block1.bn3.running_var\", \"Encoder.resnet50.residual_block1.bn3.num_batches_tracked\", \"Encoder.resnet50.conv_block2.conv1.weight\", \"Encoder.resnet50.conv_block2.conv1.bias\", \"Encoder.resnet50.conv_block2.bn1.weight\", \"Encoder.resnet50.conv_block2.bn1.bias\", \"Encoder.resnet50.conv_block2.bn1.running_mean\", \"Encoder.resnet50.conv_block2.bn1.running_var\", \"Encoder.resnet50.conv_block2.bn1.num_batches_tracked\", \"Encoder.resnet50.conv_block2.conv2.weight\", \"Encoder.resnet50.conv_block2.conv2.bias\", \"Encoder.resnet50.conv_block2.bn2.weight\", \"Encoder.resnet50.conv_block2.bn2.bias\", \"Encoder.resnet50.conv_block2.bn2.running_mean\", \"Encoder.resnet50.conv_block2.bn2.running_var\", \"Encoder.resnet50.conv_block2.bn2.num_batches_tracked\", \"Encoder.resnet50.conv_block2.conv3.weight\", \"Encoder.resnet50.conv_block2.conv3.bias\", \"Encoder.resnet50.conv_block2.bn3.weight\", \"Encoder.resnet50.conv_block2.bn3.bias\", \"Encoder.resnet50.conv_block2.bn3.running_mean\", \"Encoder.resnet50.conv_block2.bn3.running_var\", \"Encoder.resnet50.conv_block2.bn3.num_batches_tracked\", \"Encoder.resnet50.conv_block2.conv4.weight\", \"Encoder.resnet50.conv_block2.conv4.bias\", \"Encoder.resnet50.conv_block2.bn4.weight\", \"Encoder.resnet50.conv_block2.bn4.bias\", \"Encoder.resnet50.conv_block2.bn4.running_mean\", \"Encoder.resnet50.conv_block2.bn4.running_var\", \"Encoder.resnet50.conv_block2.bn4.num_batches_tracked\", \"Encoder.resnet50.residual_block2.conv1.weight\", \"Encoder.resnet50.residual_block2.conv1.bias\", \"Encoder.resnet50.residual_block2.bn1.weight\", \"Encoder.resnet50.residual_block2.bn1.bias\", \"Encoder.resnet50.residual_block2.bn1.running_mean\", \"Encoder.resnet50.residual_block2.bn1.running_var\", \"Encoder.resnet50.residual_block2.bn1.num_batches_tracked\", \"Encoder.resnet50.residual_block2.conv2.weight\", \"Encoder.resnet50.residual_block2.conv2.bias\", \"Encoder.resnet50.residual_block2.bn2.weight\", \"Encoder.resnet50.residual_block2.bn2.bias\", \"Encoder.resnet50.residual_block2.bn2.running_mean\", \"Encoder.resnet50.residual_block2.bn2.running_var\", \"Encoder.resnet50.residual_block2.bn2.num_batches_tracked\", \"Encoder.resnet50.residual_block2.conv3.weight\", \"Encoder.resnet50.residual_block2.conv3.bias\", \"Encoder.resnet50.residual_block2.bn3.weight\", \"Encoder.resnet50.residual_block2.bn3.bias\", \"Encoder.resnet50.residual_block2.bn3.running_mean\", \"Encoder.resnet50.residual_block2.bn3.running_var\", \"Encoder.resnet50.residual_block2.bn3.num_batches_tracked\", \"Encoder.resnet50.conv_block3.conv1.weight\", \"Encoder.resnet50.conv_block3.conv1.bias\", \"Encoder.resnet50.conv_block3.bn1.weight\", \"Encoder.resnet50.conv_block3.bn1.bias\", \"Encoder.resnet50.conv_block3.bn1.running_mean\", \"Encoder.resnet50.conv_block3.bn1.running_var\", \"Encoder.resnet50.conv_block3.bn1.num_batches_tracked\", \"Encoder.resnet50.conv_block3.conv2.weight\", \"Encoder.resnet50.conv_block3.conv2.bias\", \"Encoder.resnet50.conv_block3.bn2.weight\", \"Encoder.resnet50.conv_block3.bn2.bias\", \"Encoder.resnet50.conv_block3.bn2.running_mean\", \"Encoder.resnet50.conv_block3.bn2.running_var\", \"Encoder.resnet50.conv_block3.bn2.num_batches_tracked\", \"Encoder.resnet50.conv_block3.conv3.weight\", \"Encoder.resnet50.conv_block3.conv3.bias\", \"Encoder.resnet50.conv_block3.bn3.weight\", \"Encoder.resnet50.conv_block3.bn3.bias\", \"Encoder.resnet50.conv_block3.bn3.running_mean\", \"Encoder.resnet50.conv_block3.bn3.running_var\", \"Encoder.resnet50.conv_block3.bn3.num_batches_tracked\", \"Encoder.resnet50.conv_block3.conv4.weight\", \"Encoder.resnet50.conv_block3.conv4.bias\", \"Encoder.resnet50.conv_block3.bn4.weight\", \"Encoder.resnet50.conv_block3.bn4.bias\", \"Encoder.resnet50.conv_block3.bn4.running_mean\", \"Encoder.resnet50.conv_block3.bn4.running_var\", \"Encoder.resnet50.conv_block3.bn4.num_batches_tracked\", \"Encoder.resnet50.residual_block3.conv1.weight\", \"Encoder.resnet50.residual_block3.conv1.bias\", \"Encoder.resnet50.residual_block3.bn1.weight\", \"Encoder.resnet50.residual_block3.bn1.bias\", \"Encoder.resnet50.residual_block3.bn1.running_mean\", \"Encoder.resnet50.residual_block3.bn1.running_var\", \"Encoder.resnet50.residual_block3.bn1.num_batches_tracked\", \"Encoder.resnet50.residual_block3.conv2.weight\", \"Encoder.resnet50.residual_block3.conv2.bias\", \"Encoder.resnet50.residual_block3.bn2.weight\", \"Encoder.resnet50.residual_block3.bn2.bias\", \"Encoder.resnet50.residual_block3.bn2.running_mean\", \"Encoder.resnet50.residual_block3.bn2.running_var\", \"Encoder.resnet50.residual_block3.bn2.num_batches_tracked\", \"Encoder.resnet50.residual_block3.conv3.weight\", \"Encoder.resnet50.residual_block3.conv3.bias\", \"Encoder.resnet50.residual_block3.bn3.weight\", \"Encoder.resnet50.residual_block3.bn3.bias\", \"Encoder.resnet50.residual_block3.bn3.running_mean\", \"Encoder.resnet50.residual_block3.bn3.running_var\", \"Encoder.resnet50.residual_block3.bn3.num_batches_tracked\", \"Encoder.resnet50.conv_block4.conv1.weight\", \"Encoder.resnet50.conv_block4.conv1.bias\", \"Encoder.resnet50.conv_block4.bn1.weight\", \"Encoder.resnet50.conv_block4.bn1.bias\", \"Encoder.resnet50.conv_block4.bn1.running_mean\", \"Encoder.resnet50.conv_block4.bn1.running_var\", \"Encoder.resnet50.conv_block4.bn1.num_batches_tracked\", \"Encoder.resnet50.conv_block4.conv2.weight\", \"Encoder.resnet50.conv_block4.conv2.bias\", \"Encoder.resnet50.conv_block4.bn2.weight\", \"Encoder.resnet50.conv_block4.bn2.bias\", \"Encoder.resnet50.conv_block4.bn2.running_mean\", \"Encoder.resnet50.conv_block4.bn2.running_var\", \"Encoder.resnet50.conv_block4.bn2.num_batches_tracked\", \"Encoder.resnet50.conv_block4.conv3.weight\", \"Encoder.resnet50.conv_block4.conv3.bias\", \"Encoder.resnet50.conv_block4.bn3.weight\", \"Encoder.resnet50.conv_block4.bn3.bias\", \"Encoder.resnet50.conv_block4.bn3.running_mean\", \"Encoder.resnet50.conv_block4.bn3.running_var\", \"Encoder.resnet50.conv_block4.bn3.num_batches_tracked\", \"Encoder.resnet50.conv_block4.conv4.weight\", \"Encoder.resnet50.conv_block4.conv4.bias\", \"Encoder.resnet50.conv_block4.bn4.weight\", \"Encoder.resnet50.conv_block4.bn4.bias\", \"Encoder.resnet50.conv_block4.bn4.running_mean\", \"Encoder.resnet50.conv_block4.bn4.running_var\", \"Encoder.resnet50.conv_block4.bn4.num_batches_tracked\", \"Encoder.resnet50.residual_block4.conv1.weight\", \"Encoder.resnet50.residual_block4.conv1.bias\", \"Encoder.resnet50.residual_block4.bn1.weight\", \"Encoder.resnet50.residual_block4.bn1.bias\", \"Encoder.resnet50.residual_block4.bn1.running_mean\", \"Encoder.resnet50.residual_block4.bn1.running_var\", \"Encoder.resnet50.residual_block4.bn1.num_batches_tracked\", \"Encoder.resnet50.residual_block4.conv2.weight\", \"Encoder.resnet50.residual_block4.conv2.bias\", \"Encoder.resnet50.residual_block4.bn2.weight\", \"Encoder.resnet50.residual_block4.bn2.bias\", \"Encoder.resnet50.residual_block4.bn2.running_mean\", \"Encoder.resnet50.residual_block4.bn2.running_var\", \"Encoder.resnet50.residual_block4.bn2.num_batches_tracked\", \"Encoder.resnet50.residual_block4.conv3.weight\", \"Encoder.resnet50.residual_block4.conv3.bias\", \"Encoder.resnet50.residual_block4.bn3.weight\", \"Encoder.resnet50.residual_block4.bn3.bias\", \"Encoder.resnet50.residual_block4.bn3.running_mean\", \"Encoder.resnet50.residual_block4.bn3.running_var\", \"Encoder.resnet50.residual_block4.bn3.num_batches_tracked\", \"Encoder.resnet50.fc1.weight\", \"Encoder.resnet50.fc1.bias\", \"Decoder.attention.W1.weight\", \"Decoder.attention.W1.bias\", \"Decoder.attention.W2.weight\", \"Decoder.attention.W2.bias\", \"Decoder.attention.V.weight\", \"Decoder.attention.V.bias\". \n\tsize mismatch for Decoder.lstm.weight_ih_l0: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([2048, 300]).\n\tsize mismatch for Decoder.embed.weight: copying a param with shape torch.Size([8680, 256]) from checkpoint, the shape in current model is torch.Size([8680, 300])."
     ]
    }
   ],
   "source": [
    "phase = \"Test\"\n",
    "if phase == \"Test\":\n",
    "    VOCAB.clear()\n",
    "    WORD2IDX.clear()\n",
    "    IDX2WORD.clear()\n",
    "    if platform != 'colab':\n",
    "        with open('../dict/VOCAB_comp.pkl', 'rb') as handle:\n",
    "            VOCAB = pickle.load(handle)\n",
    "        with open('../dict/WORD2IDX_comp.pkl', 'rb') as handle:\n",
    "            WORD2IDX = pickle.load(handle)\n",
    "        with open('../dict/IDX2WORD_comp.pkl', 'rb') as handle:\n",
    "            IDX2WORD = pickle.load(handle)\n",
    "        print(\"Dictionary Loaded Successfully\")\n",
    "    else:\n",
    "        with open('/content/drive/My Drive/A4/dict/VOCAB_comp.pkl', 'rb') as handle:\n",
    "            VOCAB = pickle.load(handle)\n",
    "        with open('/content/drive/My Drive/A4/dict/WORD2IDX_comp.pkl', 'rb') as handle:\n",
    "            WORD2IDX = pickle.load(handle)\n",
    "        with open('/content/drive/My Drive/A4/dict/IDX2WORD_comp.pkl', 'rb') as handle:\n",
    "            IDX2WORD = pickle.load(handle)\n",
    "        print(\"Dictionary Loaded Successfully\")\n",
    "\n",
    "        \n",
    "if platform == \"colab\":\n",
    "    IMAGE_DIR = '/content/drive/My Drive/train/'\n",
    "else:\n",
    "    IMAGE_DIR = 'D:/Padhai/IIT Delhi MS(R)/2019-20 Sem II/COL774 Machine Learning/Assignment/Assignment4/public_test_images/'\n",
    "if restore == True:\n",
    "    net = ImageCaptionsNet()\n",
    "    net = net.double()\n",
    "    \n",
    "    state_dict = collections.OrderedDict()\n",
    "    state_dict = restore_checkpoint(\"Full_Model_own_finaltry.pth\")\n",
    "    net = ImageCaptionsNet()\n",
    "    net = net.double()\n",
    "    net.load_state_dict(state_dict)\n",
    "    print(\"State Dictionary Loaded Successfully.\")\n",
    "\n",
    "    # load params\n",
    "    #if device != 'cpu':\n",
    "     #   net = nn.DataParallel(net)\n",
    "     #   net = net.to(torch.device(\"cuda:0\"))\n",
    "\n",
    "\n",
    "# Creating the Dataset\n",
    "test_dataset = ImageCaptionsDataset(\n",
    "    IMAGE_DIR, captions_preprocessing_obj.captions_dict, img_transform=img_transform,\n",
    "    captions_transform=captions_preprocessing_obj.captions_transform, )\n",
    "\n",
    "\n",
    "\n",
    "def caption_image(image_feature, max_words=30):\n",
    "        results = []\n",
    "        states = None\n",
    "        x = image_feature.unsqueeze(0)\n",
    "        #print(x.shape)\n",
    "        with torch.no_grad():\n",
    "            for i in range(max_words):\n",
    "                \n",
    "                hiddens, states = net.Decoder.lstm(x, states)\n",
    "                #print(hiddens.shape)\n",
    "                decoder_op = net.Decoder.linear(hiddens.squeeze(1))\n",
    "                print(decoder_op.shape)\n",
    "                predicted_word = decoder_op.argmax(1)\n",
    "                prob = max(decoder_op[0].tolist())\n",
    "                #print(\"{} - {}\".format(IDX2WORD[predicted_word.item()], prob))\n",
    "                x = net.Decoder.embed(predicted_word).unsqueeze(0)\n",
    "                \n",
    "                results.append(predicted_word.item())\n",
    "                \n",
    "                if predicted_word == WORD2IDX[\"<end>\"]:\n",
    "                    break\n",
    "        \n",
    "        caption = [IDX2WORD[i] for i in  results]\n",
    "        return ' '.join(caption)\n",
    "\n",
    "\n",
    "def beam_search(img_feature, max_words=15, beam_k=3):\n",
    "    \n",
    "    #init with start token \n",
    "    init_caption = []\n",
    "    init_caption = [[[WORD2IDX[\"<start>\"]], float(0)]]\n",
    "    \n",
    "    #print(img_feature.shape)\n",
    "    #img_feature = img_feature.unsqueeze(dim=1)\n",
    "    while len(init_caption[0][0]) < max_words:\n",
    "        temp_cap = []\n",
    "        for c in init_caption:  \n",
    "            #print(c[0])\n",
    "            cap_pad = c[0] +  [0] * int(max_words - len(c[0]))\n",
    "            cap_pad = torch.LongTensor(cap_pad)\n",
    "            cap_pad = cap_pad.unsqueeze(dim=0)\n",
    "            lstm_op = net.Decoder(img_feature, cap_pad)        \n",
    "            lstm_op = lstm_op.reshape(max_words, lstm_op.shape[2])\n",
    "        \n",
    "            #TOP k prob\n",
    "            #print(lstm_op.shape)\n",
    "            #print(torch.argmax(lstm_op[0], dim=0).tolist())\n",
    "            top_pred = torch.argmax(lstm_op, dim=1)\n",
    "            #top_pred = torch.argsort(top_pred)[-beam_k:]\n",
    "            print(top_pred)\n",
    "            for i in range(beam_k): \n",
    "                word_idx = top_pred[i]\n",
    "                prob = c[1] + lstm_op[0][word_idx]\n",
    "                cap = c[0][:] + [word_idx]\n",
    "                \n",
    "                temp_cap.append([cap, prob])\n",
    "                \n",
    "        init_caption = temp_cap\n",
    "        init_caption = sorted(init_caption, reverse=False, key=lambda x: x[1])[-beam_k:]\n",
    "    #print(type(init_caption[-1][0]))\n",
    "    temp_caption = list(map(lambda x: IDX2WORD[x], init_caption[-1][0]))\n",
    "    \n",
    "    pred_caption = list()\n",
    "    for w in temp_caption:\n",
    "        if w != '<end>':\n",
    "            pred_caption.append(w)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return pred_caption\n",
    "        \n",
    "def beam_pred(image, max_words = 30, beam_k=3):\n",
    "    #print(image.shape)\n",
    "    x = net.Encoder(image)\n",
    "    #print(x.shape)\n",
    "    \n",
    "    #init with start token     \n",
    "    init_caption = [[[WORD2IDX[\"<start>\"]], float(0)]]\n",
    "    #print(\"init cap\",init_caption[0][0])\n",
    "    for i in range(max_words):\n",
    "        if len(init_caption[0][0]) > max_words:\n",
    "            break\n",
    "            \n",
    "        temp_cap = []\n",
    "        for c in init_caption:\n",
    "            cap_pad = c[0] +  [0] * int(max_words - len(c[0]))            \n",
    "            cap_pad = torch.LongTensor(cap_pad)\n",
    "            cap_pad = cap_pad.unsqueeze(dim=0)\n",
    "            print(cap_pad.shape, cap_pad)\n",
    "            lstm_predicts = net.Decoder(x, cap_pad)\n",
    "            lstm_predicts = lstm_predicts.reshape(max_words, lstm_predicts.shape[2])\n",
    "            \n",
    "            top_beams = torch.argsort(lstm_predicts, dim=1)\n",
    "            top_beams = top_beams[:, -1]\n",
    "            top_beams = top_beams[:beam_k]\n",
    "            \n",
    "            for i in range(beam_k):                \n",
    "                word_idx = top_beams[i]\n",
    "                prob = c[1] + float(torch.log(lstm_predicts[i][word_idx]))\n",
    "                cap = c[0][:] + [word_idx.item()]\n",
    "                temp_cap.append([cap, prob])\n",
    "                \n",
    "        init_caption = temp_cap\n",
    "        init_caption = sorted(init_caption, reverse=False, key=lambda x: x[1])[-beam_k:]\n",
    "        #print(\"init cap\", init_caption)\n",
    "        temp_caption = list(map(lambda x: IDX2WORD[x], init_caption[-1][0]))\n",
    "    \n",
    "        pred_caption = list()\n",
    "        for w in temp_caption:\n",
    "            if w != '<end>':\n",
    "                pred_caption.append(w)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return pred_caption  \n",
    "\n",
    "# Define your hyperparameters\n",
    "\n",
    "NUM_WORKERS = 0 # Parallel threads for dataloading\n",
    "MAX_WORDS = 15\n",
    "# Creating the DataLoader for batching purposes\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=NUM_WORKERS, \n",
    "                         collate_fn=custom_batch)\n",
    "import os\n",
    "if device != \"cpu\":\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "t0 = time()\n",
    "pred_caps = {}\n",
    "for batch_idx, sample in enumerate(test_loader):\n",
    "        #print(\"Image_idx\", batch_idx)\n",
    "        image_batch, captions_batch = sample['image'], sample['captions']\n",
    "        #print(\"AFTER\", image_batch)\n",
    "        #print(\"Original\", [IDX2WORD[i] for i in captions_batch)\n",
    "        #print(\"Cap\", [IDX2WORD[int(i)] for i in captions_batch[0]])\n",
    "        img_features = net.Encoder(image_batch)\n",
    "        #print(img_features)\n",
    "        img_features = img_features.view(-1)[torch.randperm(img_features.nelement())].view(img_features.size())\n",
    "        #img_features = torch.FloatTensor(np.random.randn(1,300))\n",
    "        #print(img_features[0][:4].tolist(), img_features[0][-5:].tolist())\n",
    "        #print(x.shape)\n",
    "        #pred_cap = beam_search(img_features)\n",
    "        pred_cap = caption_image(img_features, MAX_WORDS)\n",
    "        \n",
    "        pred_caps[batch_idx] = pred_cap\n",
    "        print(\"Predicted\",batch_idx, pred_cap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fkfskWPHhtAi"
   },
   "source": [
    "### TRAIN LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "RA1o2PSgBfKF",
    "outputId": "6f1bcaa3-df1b-464b-eb17-5052ea9b3343"
   },
   "outputs": [],
   "source": [
    "if platform == \"colab\":\n",
    "    IMAGE_DIR = '/content/drive/My Drive/train_images/'\n",
    "else:\n",
    "    IMAGE_DIR = 'D:/Padhai/IIT Delhi MS(R)/2019-20 Sem II/COL774 Machine Learning/Assignment/Assignment4/train_images/'\n",
    "\n",
    "if restore == True:\n",
    "    net = ImageCaptionsNet()\n",
    "    net = net.double()\n",
    "    #net = net.to(torch.device(\"cuda:0\"))\n",
    "    #net = nn.DataParallel(net)\n",
    "    new_state_dict = collections.OrderedDict()\n",
    "    new_state_dict = restore_checkpoint(\"caption_chkpt_multi.pth\")\n",
    "    \n",
    "    #net.load_state_dict(new_state_dict)\n",
    "    print(\"State Dictionary Loaded Successfully.\")\n",
    "    net = nn.DataParallel(net)\n",
    "    net = net.to(torch.device(\"cuda:0\"))\n",
    "\n",
    "# Creating the Dataset\n",
    "train_dataset = ImageCaptionsDataset(\n",
    "    IMAGE_DIR, captions_preprocessing_obj.captions_dict, img_transform=img_transform,\n",
    "    captions_transform=captions_preprocessing_obj.captions_transform\n",
    ")\n",
    "\n",
    "# Define your hyperparameters\n",
    "NUMBER_OF_EPOCHS = 3\n",
    "LEARNING_RATE = 1e-2\n",
    "BATCH_SIZE = 50\n",
    "NUM_WORKERS = 0 # Parallel threads for dataloading\n",
    "cw = torch.ones(len(VOCAB), dtype=torch.double)\n",
    "cw[WORD2IDX[\"<pad>\"]] = 0\n",
    "cw = cw.to(torch.device(\"cuda:0\"))\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=WORD2IDX[\"<pad>\"], weight=cw)\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "print(\"GLOBAL PARAMS:\", NUMBER_OF_EPOCHS, BATCH_SIZE, optimizer)\n",
    "loss_list = []\n",
    "# Creating the DataLoader for batching purposes\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS,\n",
    "                          collate_fn=custom_batch)\n",
    "\n",
    "if device != \"cpu\":\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    #torch.cuda.set_device(1)\n",
    "t0 = time()\n",
    "for epoch in range(NUMBER_OF_EPOCHS):\n",
    "    print(\"$$$$$----EPOCH {}----$$$$$$\".format(epoch+1))\n",
    "    iteration = 0\n",
    "    '''if epoch != 0:\n",
    "      LEARNING_RATE *= 0.9           \n",
    "      for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = LEARNING_RATE\n",
    "      print(\"OPTIMIZER =\", LEARNING_RATE, optimizer)'''\n",
    "\n",
    "    for batch_idx, sample in enumerate(train_loader):\n",
    "        iteration +=1\n",
    "        \n",
    "        if iteration%65 == 0:\n",
    "            LEARNING_RATE *= 0.92           \n",
    "            for param_group in optimizer.param_groups:\n",
    "              param_group['lr'] = LEARNING_RATE\n",
    "            print(\"OPTIMIZER =\", LEARNING_RATE, optimizer)\n",
    "\n",
    "        net.zero_grad()\n",
    "\n",
    "        image_batch, captions_batch = sample['image'], sample['captions']\n",
    "        \n",
    "        #print(\"image_shape\", image_batch.shape)\n",
    "        #print(\"batch_shape\", captions_batch.shape)\n",
    "        \n",
    "\n",
    "        # If GPU training required\n",
    "        if device != \"cpu\":\n",
    "          #print(\"cuda\")\n",
    "          image_batch, captions_batch = image_batch.to(torch.device(\"cuda:0\")), captions_batch.to(torch.device(\"cuda:0\"))\n",
    "        \n",
    "        output_captions = net((image_batch, captions_batch))\n",
    "        \n",
    "\n",
    "        #print(\"size for loss\", output_captions.shape, captions_batch.shape)\n",
    "        #torch.Size([10, 26, 9934]) torch.Size([10, 26])\n",
    "        loss = loss_function(output_captions.reshape(-1, output_captions.shape[2]), captions_batch.reshape(-1))\n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if iteration%25 == 0:\n",
    "            create_checkpoint(\"caption_chkpt_comp.pth\", net, optimizer, loss, iteration, epoch+1)\n",
    "        print(\"ITERATION:[{}/{}] | LOSS: {} | EPOCH = [{}/{}] | TIME ELAPSED ={}Mins\".format(iteration, round(29000/BATCH_SIZE)+1,\n",
    "              round(loss.item(), 6), epoch+1, NUMBER_OF_EPOCHS, round((time()-t0)/60,2)))\n",
    "    print(\"\\n$$Loss = {},EPOCH: [{}/{}]\\n\\n\".format(round(loss.item(), 6), epoch+1, NUMBER_OF_EPOCHS))\n",
    "    create_checkpoint(\"epoch_chkpt_comp.pth\", net, optimizer, loss, iteration, epoch+1)\n",
    "\n",
    "create_checkpoint(\"Final_Model_comp.pth\", net, optimizer, loss, iteration, epoch+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Nt32kDSARJG6",
    "outputId": "948747df-9f13-466c-c3d4-3f90195df36f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.resnet34()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uOXPnRtdv2E0"
   },
   "outputs": [],
   "source": [
    "reference = [['this', 'is', 'a', 'test'], ['this', 'is' 'test']]\n",
    "candidate = ['un', 'uomo', 'in', 'camicia', 'rossa', 'e', 'un', 'gilet', 'blu', 'e', 'una', 'donna', '.']\n",
    "score = sentence_bleu(reference, candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H_KQSLLVX5DA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "guOc4YBptLX7",
    "outputId": "3e3abaf6-3076-45b3-e8c7-6aae03788ca8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 773, 223, 374, 52, 29, 215, 773, 5, 39, 215, 440, 78, 103, 1, 1, 1, 601, 1]"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = '<start> un uomo in camicia rossa e un gilet blu e una donna , <unk> <unk> <unk> . <end>'\n",
    "[VOCAB[i] for  i in pred.split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cN9Al4KGXyqn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.8169, -1.3020,  1.1058,  1.9061,  0.2894,  0.8495,  0.1393, -1.4722,\n",
      "         -2.2089, -0.4604]])\n",
      "tensor([[ 0.1393,  1.1058,  1.9061, -0.4604,  1.8169, -1.3020, -1.4722, -2.2089,\n",
      "          0.8495,  0.2894]])\n",
      "tensor([[ 1.9061, -1.3020,  0.1393, -0.4604,  1.8169, -2.2089,  0.2894,  0.8495,\n",
      "         -1.4722,  1.1058]])\n",
      "tensor([[ 0.1393,  0.8495, -2.2089, -1.3020, -1.4722,  1.9061,  1.8169,  0.2894,\n",
      "         -0.4604,  1.1058]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1,10)\n",
    "print(x)\n",
    "print(x.view(-1)[torch.randperm(x.nelement())].view(x.size()))\n",
    "print(x.view(-1)[torch.randperm(x.nelement())].view(x.size()))\n",
    "\n",
    "print(x.view(-1)[torch.randperm(x.nelement())].view(x.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7557, -0.0301,  0.5531, -0.7872, -0.1490, -0.0187, -0.6112, -1.1719,\n",
       "          0.5524, -0.1333]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1,10)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = nn.ReLU(inplace=False)\n",
    "y = relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.5531, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5524,\n",
       "         0.0000]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "comp_clean.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
