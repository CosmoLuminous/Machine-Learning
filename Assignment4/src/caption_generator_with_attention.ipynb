{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "tPc-Vc0SDhNb",
    "outputId": "957191da-13f2-4814-cd13-d7fe5fdc75cb"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z0NAkjfkDunx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from collections import Counter\n",
    "from skimage import io, transform\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import numpy as np\n",
    "from time import time\n",
    "import collections\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Ar-pjO6VDvyM",
    "outputId": "abd52cca-065a-4295-861e-1230bf410e33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Let's use 0 GPUs!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "parallel = True\n",
    "platform = \"local\"\n",
    "restore = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-DiPHUqEcUtp"
   },
   "outputs": [],
   "source": [
    "phase = \"Test\"\n",
    "\n",
    "VOCAB = {}\n",
    "WORD2IDX = {}\n",
    "IDX2WORD = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NuV9OO2AD772"
   },
   "source": [
    "### Image Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bARgdNI3D4YX"
   },
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "        return img\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, image):\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return image\n",
    "\n",
    "\n",
    "IMAGE_RESIZE = (256, 256)\n",
    "# Sequentially compose the transforms\n",
    "img_transform = transforms.Compose([Rescale(IMAGE_RESIZE), ToTensor()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TILWv_RDEElV"
   },
   "source": [
    "### Captions Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "zR_D_a8UD_-n",
    "outputId": "50a32d24-3892-4998-9488-c8c0a75698f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE = 10700\n"
     ]
    }
   ],
   "source": [
    "class CaptionsPreprocessing:\n",
    "    \"\"\"Preprocess the captions, generate vocabulary and convert words to tensor tokens\n",
    "    Args:\n",
    "        captions_file_path (string): captions tsv file path\n",
    "    \"\"\"\n",
    "    def __init__(self, captions_file_path):\n",
    "        self.captions_file_path = captions_file_path\n",
    "\n",
    "        # Read raw captions\n",
    "        self.raw_captions_dict = self.read_raw_captions()\n",
    "\n",
    "        # Preprocess captions\n",
    "        self.captions_dict = self.process_captions()\n",
    "\n",
    "        # Create vocabulary\n",
    "        self.start = \"<start>\"\n",
    "        self.end = \"<end>\"\n",
    "        self.oov = \"<unk>\"\n",
    "        self.pad = \"<pad>\"\n",
    "        self.vocab = self.generate_vocabulary()\n",
    "        self.word2index = self.convert_word2index()        \n",
    "        self.index2word = self.convert_index2word()\n",
    "        self.max_len_caption = 50\n",
    "        \n",
    "\n",
    "    def read_raw_captions(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            Dictionary with raw captions list keyed by image ids (integers)\n",
    "        \"\"\"\n",
    "        captions_dict = {}\n",
    "        with open(self.captions_file_path, 'r', encoding='utf-8') as f:\n",
    "            for img_caption_line in f.readlines():\n",
    "                img_captions = img_caption_line.strip().split('\\t')\n",
    "                captions_dict[int(img_captions[0])] = img_captions[1:]\n",
    "\n",
    "        return captions_dict \n",
    "\n",
    "    def process_captions(self):\n",
    "        \"\"\"\n",
    "        Use this function to generate dictionary and other preprocessing on captions\n",
    "        \"\"\"\n",
    "\n",
    "        raw_captions_dict = self.raw_captions_dict \n",
    "        \n",
    "        # Do the preprocessing here                \n",
    "        captions_dict = raw_captions_dict\n",
    "\n",
    "        return captions_dict\n",
    "\n",
    " \n",
    "\n",
    "    def generate_vocabulary(self):\n",
    "        \"\"\"\n",
    "        Use this function to generate dictionary and other preprocessing on captions\n",
    "        \"\"\"\n",
    "        captions_dict = self.captions_dict\n",
    "\n",
    "        # Generate the vocabulary\n",
    "        \n",
    "        all_captions = \"\"        \n",
    "        for cap_lists in captions_dict.values():\n",
    "            all_captions += \" \".join(cap_lists)\n",
    "        all_captions = all_captions.lower().replace(\".\", \"\").split(\" \")\n",
    "        \n",
    "        vocab = {self.pad :1, self.oov :1, self.start :1, self.end :1}\n",
    "        vocab_update = Counter(all_captions) \n",
    "        vocab_update = {k:v for k,v in vocab_update.items() if v >= freq_threshold}\n",
    "        vocab.update(vocab_update)        \n",
    "        vocab_size = len(vocab)\n",
    "        \n",
    "        if phase == \"Train\":\n",
    "            VOCAB.update(vocab)\n",
    "            with open('../dict/VOCAB.pkl', 'wb') as handle:\n",
    "                pickle.dump(vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "        print(\"VOCAB SIZE =\", vocab_size)\n",
    "        return vocab\n",
    "    \n",
    "    def convert_word2index(self):\n",
    "        word2index = {}\n",
    "        vocab = self.vocab\n",
    "        idx = 0\n",
    "        for k, v in vocab.items():\n",
    "            word2index[k] = idx\n",
    "            idx +=1\n",
    "        if phase == \"Train\":\n",
    "            WORD2IDX.update(word2index)\n",
    "            with open('../dict/WORD2IDX.pkl', 'wb') as handle:\n",
    "                pickle.dump(word2index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        return word2index\n",
    "    \n",
    "    def convert_index2word(self):\n",
    "        index2word = {}\n",
    "        vocab = self.vocab\n",
    "        idx = 0\n",
    "        \n",
    "        for k, v in vocab.items():\n",
    "            index2word[idx] = k\n",
    "            idx +=1\n",
    "        if phase == \"Train\":\n",
    "            IDX2WORD.update(index2word)\n",
    "            with open('../dict/IDX2WORD.pkl', 'wb') as handle:\n",
    "                pickle.dump(index2word, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        return index2word\n",
    "\n",
    " \n",
    "\n",
    "    def captions_transform(self, img_caption_list, max_len):\n",
    "        \"\"\"\n",
    "        Use this function to generate tensor tokens for the text captions\n",
    "        Args:\n",
    "            img_caption_list: List of captions for a particular image\n",
    "        \"\"\"\n",
    "        if phase == \"Test\":\n",
    "            word2index = WORD2IDX\n",
    "            vocab = VOCAB\n",
    "        else:\n",
    "            word2index = self.word2index\n",
    "            vocab = self.vocab        \n",
    "        #embed = self.embed\n",
    "        start = self.start\n",
    "        end = self.end\n",
    "        oov = self.oov\n",
    "        print(\"MX LEN\", max_len)\n",
    "        processed_list = list(map(lambda x: start + \" \"+ x + \" \" + end, img_caption_list))\n",
    "        processed_list = list(map(lambda x: x.lower().replace(\".\", \"\").split(\" \"), processed_list))\n",
    "        processed_list = list(map(lambda x: list(map(lambda y: word2index[y] if y in vocab else word2index[oov],x)),\n",
    "                                  processed_list))\n",
    "        #processed_list = list(map(lambda x: x + ( [0] * int(max_len - len(x) + 2) ),processed_list))\n",
    "        #lengths = list(map(lambda x: len(x),processed_list))\n",
    "        #processed_list = list(map(lambda x: torch.LongTensor(x),processed_list))\n",
    "        #processed_list = pack_padded_sequence(processed_list, lengths, batch_first=True)\n",
    "        # Generate tensors\n",
    "        #print(np.array(processed_list).shape)\n",
    "        #processed_list = torch.LongTensor(processed_list)\n",
    "        #processed_captions = embed(processed_list)   \n",
    "        #print(processed_captions)    \n",
    "        \n",
    "        #return torch.zeros(len(img_caption_list), 10)\n",
    "        return processed_list\n",
    "\n",
    "\n",
    "if platform == \"colab\":\n",
    "    CAPTIONS_FILE_PATH = '/content/drive/My Drive/A4/train_captions.tsv'\n",
    "else:\n",
    "    CAPTIONS_FILE_PATH = \"../data/train_captions.tsv\"\n",
    "    \n",
    "embedding_dim = 256\n",
    "freq_threshold = 4\n",
    "captions_preprocessing_obj = CaptionsPreprocessing(CAPTIONS_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4cE5iXNPENzj"
   },
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uLxKe6XNESUq"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-6-2391f3fa2e94>, line 43)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-2391f3fa2e94>\"\u001b[1;36m, line \u001b[1;32m43\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "class ImageCaptionsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, img_dir, captions_dict, img_transform=None, captions_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dir (string): Directory with all the images.\n",
    "            captions_dict: Dictionary with captions list keyed by image ids (integers)\n",
    "            img_transform (callable, optional): Optional transform to be applied\n",
    "                on the image sample.\n",
    "\n",
    "            captions_transform: (callable, optional): Optional transform to be applied\n",
    "                on the caption sample (list).\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.captions_dict = captions_dict\n",
    "        self.img_transform = img_transform\n",
    "        self.captions_transform = captions_transform\n",
    "\n",
    "        self.image_ids = list(captions_dict.keys())\n",
    "        self.max_len = max([max(list(map(lambda x: len(x.lower().replace(\".\", \"\").split(\" \")), v))) for k,v in captions_dict.items()])\n",
    "        print(\"LOAD MAX LEN\", self.max_len,len(captions_dict))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, 'image_{}.jpg'.format(self.image_ids[idx]))\n",
    "        image = io.imread(img_name)\n",
    "        captions = self.captions_dict[self.image_ids[idx]]\n",
    "        if self.img_transform:\n",
    "            image = self.img_transform(image)\n",
    "\n",
    "        if self.captions_transform:            \n",
    "            captions = self.captions_transform(captions, self.max_len)\n",
    "            #print(\"LEN CAPS\", captions.shape)\n",
    "\n",
    "        sample = {'image': image, 'captions': captions}\n",
    "\n",
    "        return sample\n",
    "def collate_wrapper(batch):\n",
    "    for sample in batch:\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qqlKdxa3ESwY"
   },
   "outputs": [],
   "source": [
    "#ENCODER\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size, filters, stride=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            channels: Int: Number of Input channels to 1st convolutional layer\n",
    "            kernel_size: integer, Symmetric Conv Window = (kernel_size, kernel_size)\n",
    "            filters: python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "            stride: Tuple: (stride, stride)\n",
    "        \"\"\"\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        F1, F2, F3 = filters\n",
    "        #N, in_channels , H, W = shape\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "        padding = (1,1)\n",
    "        stride = (stride, stride)\n",
    "        self.conv1 = nn.Conv2d(in_channels = channels, out_channels = F1, kernel_size=(1,1), stride=stride, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(F1)\n",
    "        self.relu = nn.ReLU(inplace=True) \n",
    "        self.conv2 = nn.Conv2d(in_channels = F1, out_channels = F2, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.bn2 = nn.BatchNorm2d(F2)\n",
    "        self.conv3 = nn.Conv2d(in_channels = F2, out_channels = F3, kernel_size=(1,1), stride=stride, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(F3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_residual = x #backup x for residual connection\n",
    "        \n",
    "        #stage 1 main path\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        #print(\"RESI:\", x.shape)\n",
    "        \n",
    "        #stage 2 main path\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        #print(\"RESI:\", x.shape)\n",
    "        \n",
    "        #stage 3 main path\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        #print(\"RESI:\", x.shape)\n",
    "        \n",
    "        x += x_residual #add output with residual connection\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class ConvolutionalBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size, filters, stride=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            channels: Int: Number of Input channels to 1st convolutional layer\n",
    "            kernel_size: integer, Symmetric Conv Window = (kernel_size, kernel_size)\n",
    "            filters: python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "            stride: Tuple: (stride, stride)\n",
    "        \"\"\"\n",
    "        super(ConvolutionalBlock, self).__init__()\n",
    "        F1, F2, F3 = filters\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "        padding = (1,1)\n",
    "        stride = (stride, stride)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels = channels, out_channels = F1, kernel_size=(1,1), stride=stride, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(F1)\n",
    "        self.relu = nn.ReLU(inplace=True) \n",
    "        self.conv2 = nn.Conv2d(in_channels = F1, out_channels = F2, kernel_size=kernel_size, stride=(1,1), padding=padding)\n",
    "        self.bn2 = nn.BatchNorm2d(F2)\n",
    "        self.conv3 = nn.Conv2d(in_channels = F2, out_channels = F3, kernel_size=(1,1), stride=(1,1), padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(F3)\n",
    "        self.conv4 = nn.Conv2d(in_channels = channels, out_channels = F3, kernel_size=(1,1), stride=stride, padding=0)\n",
    "        self.bn4 = nn.BatchNorm2d(F3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x_residual = x #backup x for residual connection\n",
    "        \n",
    "        #stage 1 main path\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        #print(\"CONV:\", x.shape)\n",
    "        \n",
    "        #stage 2 main path\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        #print(\"CONV:\", x.shape)\n",
    "        \n",
    "        #stage 3 main path\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        #print(\"CONV:\", x.shape)\n",
    "        \n",
    "        #residual connection\n",
    "        x_residual = self.conv4(x_residual)\n",
    "        x_residual = self.bn4(x_residual)\n",
    "        x += x_residual #add output with residual connection\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, input_shape = (256, 256, 3), classes = 5):\n",
    "        \"\"\"\n",
    "        It Implements Famous Resnet50 Architecture\n",
    "        Args:\n",
    "            input_shape(tuple):(callable, optional): dimensions of image sample\n",
    "            classes(int):(callable, optional): Final output classes of softmax layer.\n",
    "        \"\"\"\n",
    "        super(ResNet50, self).__init__()\n",
    "        \n",
    "        self.pad = nn.ZeroPad2d((1, 1, 3, 3))        \n",
    "        ###STAGE1\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels=64, kernel_size=(7,7), stride = (2,2), padding=1) # convolve each of our 3-channel images with 6 different 5x5 kernels, giving us 6 feature maps\n",
    "        self.batch_norm1 = nn.BatchNorm2d(64) #BatchNorm\n",
    "        self.pool1 = nn.MaxPool2d((3,3), stride=(2,2), padding=1, dilation=1)\n",
    "        \n",
    "        ###STAGE2 channels, kernel_size=3, filters, stride=1, stage\n",
    "        self.conv_block1 = ConvolutionalBlock(channels = 64, kernel_size = 3, filters = [64, 64, 256],stride = 1)\n",
    "        self.residual_block1 = ResidualBlock(channels = 256, kernel_size = 3, filters = [64, 64, 256])\n",
    "        \n",
    "        ###STAGE3 \n",
    "        self.conv_block2 = ConvolutionalBlock(channels = 256, kernel_size = 3, filters = [128, 128, 512],stride = 2)\n",
    "        self.residual_block2 = ResidualBlock(channels = 512, kernel_size = 3, filters = [128, 128, 512],)\n",
    "        \n",
    "        ###STAGE4 \n",
    "        self.conv_block3 = ConvolutionalBlock(channels = 512, kernel_size = 3, filters = [256, 256, 1024], stride = 2)\n",
    "        self.residual_block3 = ResidualBlock(channels = 1024, kernel_size = 3, filters = [256, 256, 1024])\n",
    "        \n",
    "        ###STAGE5 \n",
    "        self.conv_block4 = ConvolutionalBlock(channels = 1024, kernel_size = 3, filters = [512, 512, 2048], stride = 2)\n",
    "        self.residual_block4 = ResidualBlock(channels = 2048, kernel_size = 3, filters = [512, 512, 2048])\n",
    "        \n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d(output_size = (1,1))\n",
    "        self.fc1 = nn.Linear(in_features=2048, out_features=classes, bias = True)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(\"IP_SIZE:\", x.shape)\n",
    "        \n",
    "        ###STAGE1        \n",
    "        #print(\"\\n STAGE1\")\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.pool1(x)\n",
    "        #print(\"OP_STAGE1_SIZE:\", x.shape)\n",
    "        \n",
    "        ###STAGE2 \n",
    "        #print(\"\\n STAGE2\")\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.residual_block1(x)\n",
    "        x = self.residual_block1(x)\n",
    "        #print(\"OP_STAGE2_SIZE:\", x.shape)\n",
    "        \n",
    "        ###STAGE3 \n",
    "        #print(\"\\n STAGE3\")\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.residual_block2(x)\n",
    "        x = self.residual_block2(x)\n",
    "        x = self.residual_block2(x)\n",
    "        #print(\"OP_STAGE3_SIZE:\", x.shape)\n",
    "        \n",
    "        ###STAGE4  \n",
    "        #print(\"\\n STAGE4\")\n",
    "        x = self.conv_block3(x)\n",
    "        x = self.residual_block3(x)\n",
    "        x = self.residual_block3(x)\n",
    "        x = self.residual_block3(x)\n",
    "        x = self.residual_block3(x)\n",
    "        x = self.residual_block3(x)\n",
    "        #print(\"OP_STAGE4_SIZE:\", x.shape)\n",
    "        \n",
    "        ###STAGE5  \n",
    "        #print(\"\\n STAGE5\")\n",
    "        x = self.conv_block4(x)\n",
    "        x = self.residual_block4(x)\n",
    "        x = self.residual_block4(x)\n",
    "        #print(\"OP_STAGE5_SIZE:\", x.shape)\n",
    "        \n",
    "        x = self.adaptive_pool(x)\n",
    "        #print(\"OP_ADAPTIVEPOOL_SHAPE\", x.shape)\n",
    "        \n",
    "        x = x.view(x.size(0), -1) # Flatten Vector\n",
    "        x = self.fc1(x)\n",
    "        #print(\"OP_FC1_SIZE:\", x.shape)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class Encoder(nn.Module):    \n",
    "    def __init__(self, embed_dim):\n",
    "        \"\"\"\n",
    "        CNN ENCODER\n",
    "        Args:\n",
    "            embed_dim(int): embedding dimension ie output dimension of last FC Layer\n",
    "        Returns:\n",
    "            x: Feature vector of size(BatchSize, embed_dim)\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.resnet50 = ResNet50(classes = embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.resnet50(x)\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3aFX2bueEa6w"
   },
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, units, vocab_size):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.W1 = nn.Linear(in_features = embed_dim, out_features = units)\n",
    "        self.W2 = nn.Linear(in_features=units, out_features=units)\n",
    "        self.V = nn.Linear(in_features=units, out_features=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, img_features, hidden):\n",
    "        \n",
    "        hidden = hidden.unsqueeze(dim=1)\n",
    "        hidden = hidden.double()\n",
    "        #print(\"feature and hidden shape\",img_features.shape, hidden.shape)\n",
    "        combined_score = self.tanh(self.W1(img_features) + self.W2(hidden))\n",
    "        \n",
    "        attention_weights = self.softmax(self.V(combined_score))\n",
    "        context_vector = attention_weights * img_features\n",
    "        context_vector = torch.sum(context_vector, dim=1)\n",
    "        \n",
    "        return context_vector, attention_weights    \n",
    "\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_dim, lstm_hidden_size,lstm_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.vocab_size = len(VOCAB)\n",
    "        print(\"VOCAB SIZE = \", self.vocab_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = embed_dim, hidden_size = lstm_hidden_size,\n",
    "                            num_layers = lstm_layers, batch_first = True)\n",
    "        \n",
    "        self.linear = nn.Linear(lstm_hidden_size, self.vocab_size)        \n",
    "        self.embed = nn.Embedding(self.vocab_size, embed_dim)\n",
    "        \n",
    "        self.attention = AttentionBlock(embed_dim, lstm_hidden_size, self.vocab_size)\n",
    "\n",
    "        \n",
    "    def forward(self, image_features, image_captions):\n",
    "        \n",
    "        if phase == \"Train\":\n",
    "            image_features = torch.Tensor.repeat_interleave(image_features, repeats=5 , dim=0)\n",
    "        image_features = image_features.unsqueeze(1)\n",
    "        \n",
    "        hidden = torch.zeros((image_features.shape[0], self.lstm_hidden_size))\n",
    "        \n",
    "        context, attention = self.attention(image_features, hidden)\n",
    "        \n",
    "        embedded_captions = self.embed(image_captions)\n",
    "        #print(\"EMBED SHAPE\", embedded_captions.shape)\n",
    "        #print(\"SHAPES BEFORE CONCAT\",context.unsqueeze(dim=1).shape, embedded_captions[:,:-1].shape)\n",
    "        input_lstm = torch.cat((context.unsqueeze(dim=1), embedded_captions[:,:-1]), dim = 1)\n",
    "        \n",
    "        lstm_outputs, _ = self.lstm(input_lstm)        \n",
    "        lstm_outputs = self.linear(lstm_outputs)\n",
    "        #print(\"lstm_outputs.shape\", lstm_outputs.shape)\n",
    "        \n",
    "        \n",
    "        return lstm_outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "79D1QrHWEbyz"
   },
   "outputs": [],
   "source": [
    "class ImageCaptionsNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageCaptionsNet, self).__init__()\n",
    "\n",
    "        # Define your architecture here\n",
    "        \n",
    "        ##CNN ENCODER RESNET-50\n",
    "        \n",
    "        self.Encoder = Encoder(embed_dim = embedding_dim)\n",
    "        self.Decoder = Decoder(embedding_dim, units, 1)\n",
    "        #self.Decoder = DecoderRNN(256, 512, len(captions_preprocessing_obj.vocab), 1)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = image_batch, captions_batch\n",
    "\n",
    "        # Forward Propogation\n",
    "        x = self.Encoder(image_batch)\n",
    "        #print(x.shape)\n",
    "        x = self.Decoder(x, captions_batch)\n",
    "        return x\n",
    "    \n",
    "units = 512\n",
    "if restore == False:\n",
    "    net = ImageCaptionsNet()\n",
    "    net = net.double()\n",
    "    # If GPU training is required\n",
    "    if parallel == True and device != \"cpu\":\n",
    "        print(\"Parallel Processing enabled\")\n",
    "        net = nn.DataParallel(net)\n",
    "\n",
    "    if device == \"cpu\":\n",
    "        print(\"Device to CPU\")\n",
    "    else:\n",
    "        print(\"Device to CUDA\")\n",
    "        net = net.to(torch.device(\"cuda:0\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m_GtiDw-XIDs"
   },
   "outputs": [],
   "source": [
    "def create_checkpoint(path,model, optim_obj, loss_obj,iteration, epoch):\n",
    "    checkpoint = {'epoch': epoch,\n",
    "                  'iteration': iteration,\n",
    "                  'model_state_dict': model.module.state_dict(),\n",
    "                  'optimizer_state_dict': optim_obj.state_dict(),\n",
    "                  'loss': loss_obj}\n",
    "\n",
    "    if platform == \"colab\":\n",
    "        directory = '/content/drive/My Drive/A4/checkpoint/'\n",
    "    else:\n",
    "        directory = '../checkpoint/'\n",
    "\n",
    "    torch.save(checkpoint, directory + path)\n",
    "    \n",
    "def restore_checkpoint(path):\n",
    "    new_state_dict = collections.OrderedDict()\n",
    "    if platform == \"colab\":\n",
    "        directory = '/content/drive/My Drive/A4/checkpoint/'\n",
    "        checkpoint = torch.load(directory + path)\n",
    "    else:\n",
    "        directory = '../checkpoint/'\n",
    "        checkpoint = torch.load(directory + path, map_location=torch.device('cpu'))    \n",
    "    \n",
    "    epoch = checkpoint['epoch']\n",
    "    new_state_dict = checkpoint['model_state_dict']\n",
    "    iteration = checkpoint['iteration']\n",
    "    optimizer_state_dict = checkpoint['optimizer_state_dict']\n",
    "    loss_obj = checkpoint['loss']\n",
    "    print(\"Iterations = {}, Epoch = {}, loss = {}\".format(iteration, epoch, loss_obj.item()))\n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if phase == \"Test\":\n",
    "    with open('../dict/VOCAB.pkl', 'rb') as handle:\n",
    "        VOCAB = pickle.load(handle)\n",
    "    with open('../dict/WORD2IDX.pkl', 'rb') as handle:\n",
    "        WORD2IDX = pickle.load(handle)\n",
    "    with open('../dict/IDX2WORD.pkl', 'rb') as handle:\n",
    "        IDX2WORD = pickle.load(handle)\n",
    "    print(\"Dictionary Loaded Successfully\")\n",
    "\n",
    "        \n",
    "if platform == \"colab\":\n",
    "    IMAGE_DIR = '/content/drive/My Drive/public_test_images/'\n",
    "else:\n",
    "    IMAGE_DIR = 'D:/Padhai/IIT Delhi MS(R)/2019-20 Sem II/COL774 Machine Learning/Assignment/Assignment4/train_images/'\n",
    "if restore == True:\n",
    "    net = ImageCaptionsNet()\n",
    "    net = net.double()\n",
    "    #net = net.to(torch.device(\"cuda:0\"))\n",
    "    #net = nn.DataParallel(net)\n",
    "    state_dict = collections.OrderedDict()\n",
    "    state_dict = restore_checkpoint(\"Final_Model_multi_withAttn.pth\")\n",
    "    \n",
    "    net.load_state_dict(state_dict)\n",
    "    print(\"State Dictionary Loaded Successfully.\")\n",
    "\n",
    "    # load params\n",
    "    if device != 'cpu':\n",
    "        net = nn.DataParallel(net)\n",
    "        net = net.to(torch.device(\"cuda:0\"))\n",
    "\n",
    "\n",
    "# Creating the Dataset\n",
    "test_dataset = ImageCaptionsDataset(\n",
    "    IMAGE_DIR, captions_preprocessing_obj.captions_dict, img_transform=img_transform,\n",
    "    captions_transform=captions_preprocessing_obj.captions_transform)\n",
    "\n",
    "\n",
    "\n",
    "def beam_search(img_feature, max_words=15, beam_k=3):\n",
    "    \n",
    "    #init with start token \n",
    "    init_caption = []\n",
    "    init_caption = [[[WORD2IDX[\"<start>\"]], float(0)]]\n",
    "    \n",
    "    \n",
    "    #print(img_feature.shape)\n",
    "    #img_feature = img_feature.unsqueeze(dim=1)\n",
    "    while len(init_caption[0][0]) < max_words:\n",
    "        temp_cap = []\n",
    "        for c in init_caption:  \n",
    "            #print(c[0])\n",
    "            cap_pad = c[0] +  [0] * int(max_words - len(c[0]))\n",
    "            cap_pad = torch.LongTensor(cap_pad).unsqueeze(dim=0)\n",
    "            lstm_op = net.Decoder(img_feature, cap_pad)        \n",
    "            lstm_op = lstm_op.reshape(max_words, lstm_op.shape[2])\n",
    "        \n",
    "            #TOP k prob\n",
    "            #print(lstm_op.shape)\n",
    "            #print(torch.argmax(lstm_op[0], dim=0).tolist())\n",
    "            top_pred = torch.argmax(lstm_op, dim=1)\n",
    "            #top_pred = torch.argsort(top_pred)[-beam_k:]\n",
    "            print(top_pred)\n",
    "            for i in range(beam_k): \n",
    "                word_idx = top_pred[i]\n",
    "                prob = c[1] + lstm_op[0][word_idx]\n",
    "                cap = c[0][:] + [word_idx]\n",
    "                \n",
    "                temp_cap.append([cap, prob])\n",
    "                \n",
    "        init_caption = temp_cap\n",
    "        init_caption = sorted(init_caption, reverse=False, key=lambda x: x[1])[-beam_k:]\n",
    "    #print(type(init_caption[-1][0]))\n",
    "    temp_caption = list(map(lambda x: IDX2WORD[x], init_caption[-1][0]))\n",
    "    \n",
    "    pred_caption = list()\n",
    "    for w in temp_caption:\n",
    "        if w != '<end>':\n",
    "            pred_caption.append(w)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return pred_caption\n",
    "def argmax_search(img_feature, max_words=15):\n",
    "    start_word = [\"<start>\"]\n",
    "    while True:\n",
    "        par_caps = [WORD2IDX[i] for i in start_word]\n",
    "        par_caps = cap_pad = par_caps +  [0] * int(max_words - len(par_caps))\n",
    "        #print(par_caps)\n",
    "        cap_pad = torch.LongTensor(par_caps).unsqueeze(dim=0)\n",
    "        lstm_op = net.Decoder(img_feature, cap_pad) \n",
    "        #print(\"before\", lstm_op.shape)\n",
    "        lstm_op = lstm_op.reshape(max_words, lstm_op.shape[2])\n",
    "        #print(\"after\",lstm_op.shape)\n",
    "        max_pred = torch.argmax(lstm_op, dim=1).tolist()\n",
    "        max_pred = torch.max(lstm_op, dim=1).tolist()\n",
    "        print(max_pred)\n",
    "        word_pred = IDX2WORD[max_pred]\n",
    "        start_word.append(word_pred)\n",
    "        \n",
    "        if word_pred == \"<end>\" or len(start_word) > max_words:\n",
    "            break\n",
    "            \n",
    "    return ' '.join(start_word[1:-1])\n",
    "\n",
    "def caption_image(image_feature, max_words=20):\n",
    "        result_caption = []\n",
    "        cap_temp = torch.LongTensor([0]*max_words).unsqueeze(0)\n",
    "        x = image_feature\n",
    "        #print(cap_temp.shape)\n",
    "        with torch.no_grad():\n",
    "            for i in range(max_words):\n",
    "                #print(\"img, cap\",image_feature.shape,x.shape, cap_temp.shape)\n",
    "                    \n",
    "                output = net.Decoder(x, cap_temp)                    \n",
    "                \n",
    "                #print(\"output shape\", output.shape)\n",
    "                predicted = output.argmax(2)\n",
    "                image_feature = net.Decoder.embed(predicted)\n",
    "                image_feature = image_feature.squeeze(0)\n",
    "                predicted = predicted.tolist()\n",
    "                #print(\"predicted\", predicted)\n",
    "                result_caption = predicted[0]\n",
    "                cap_temp = result_caption + [0] * int(max_words - len(result_caption))\n",
    "                #print(cap_temp)\n",
    "                cap_temp = torch.LongTensor(cap_temp).unsqueeze(0)\n",
    "\n",
    "                if predicted == 3:\n",
    "                    break\n",
    "        caption = [IDX2WORD[i] for i in  result_caption]\n",
    "        return ' '.join(caption)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "# Define your hyperparameters\n",
    "NUMBER_OF_EPOCHS = 1\n",
    "LEARNING_RATE = 1e-1\n",
    "BATCH_SIZE = 2\n",
    "NUM_WORKERS = 0 # Parallel threads for dataloading\n",
    "MAX_WORDS = 30\n",
    "# Creating the DataLoader for batching purposes\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, collate_fn=collate_wrapper)\n",
    "import os\n",
    "if device != \"cpu\":\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "t0 = time()\n",
    "pred_caps = {}\n",
    "for batch_idx, sample in enumerate(test_loader):\n",
    "        print(\"Image_idx\", batch_idx)\n",
    "        image_batch, captions_batch = sample['image'], sample['captions']\n",
    "        \n",
    "        print(\"Image_idx\", captions_batch)\n",
    "        img_features = net.Encoder(image_batch)\n",
    "        #print(x.shape)\n",
    "        #pred_cap = beam_search(img_features)\n",
    "        pred_cap = caption_image(img_features)\n",
    "        pred_caps[batch_idx] = pred_cap\n",
    "        print(batch_idx, pred_cap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "             0,    0]\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3l4HKg15EhS7"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "KgajTBg4EeYv",
    "outputId": "98d4d61e-7953-4b83-e3da-9166887b88aa"
   },
   "outputs": [],
   "source": [
    "if platform == \"colab\":\n",
    "    IMAGE_DIR = '/content/drive/My Drive/train_images/'\n",
    "else:\n",
    "    IMAGE_DIR = 'D:/Padhai/IIT Delhi MS(R)/2019-20 Sem II/COL774 Machine Learning/Assignment/Assignment4/train_images/'\n",
    "\n",
    "if restore == True:\n",
    "    net = ImageCaptionsNet()\n",
    "    net = net.double()\n",
    "    # load params\n",
    "    directory = '../checkpoint/'\n",
    "    path = \"Final_Model_multi_withAttn.pth\"\n",
    "    net.load_state_dict(torch.load(directory + path, map_location=torch.device('cpu'))['model_state_dict'])\n",
    "    print(\"State Dictionary Loaded Successfully.\")\n",
    "    net = nn.DataParallel(net)\n",
    "    net = net.to(torch.device(\"cuda:0\"))\n",
    "\n",
    "# Creating the Dataset\n",
    "train_dataset = ImageCaptionsDataset(\n",
    "    IMAGE_DIR, captions_preprocessing_obj.captions_dict, img_transform=img_transform,\n",
    "    captions_transform=captions_preprocessing_obj.captions_transform\n",
    ")\n",
    "\n",
    "# Define your hyperparameters\n",
    "NUMBER_OF_EPOCHS = 3\n",
    "LEARNING_RATE = 1e-1\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 0 # Parallel threads for dataloading\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=WORD2IDX[\"<pad>\"])\n",
    "optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE)\n",
    "loss_list = []\n",
    "# Creating the DataLoader for batching purposes\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate_wrapper)\n",
    "import os\n",
    "if device != \"cpu\":\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "t0 = time()\n",
    "for epoch in range(NUMBER_OF_EPOCHS):\n",
    "    print(\"$$$$$----EPOCH {}----$$$$$$\".format(epoch+3))\n",
    "    iteration = 0\n",
    "    for batch_idx, sample in enumerate(train_loader):\n",
    "        iteration +=1\n",
    "        net.zero_grad()\n",
    "\n",
    "        image_batch, captions_batch = sample['image'], sample['captions']\n",
    "        \n",
    "        #print(\"image_shape\", image_batch.shape)\n",
    "        #print(\"batch_shape\", captions_batch.shape)\n",
    "        \n",
    "        N, I, L = captions_batch.shape\n",
    "        captions_batch = torch.reshape(captions_batch, (N*I,L))\n",
    "\n",
    "        # If GPU training required\n",
    "        if device != \"cpu\":\n",
    "          #print(\"cuda\")\n",
    "          image_batch, captions_batch = image_batch.to(torch.device(\"cuda:0\")), captions_batch.to(torch.device(\"cuda:0\"))\n",
    "        #print(\"Running Caption Gen\")\n",
    "        output_captions = net((image_batch, captions_batch))\n",
    "        #print(\"output Achieved\")\n",
    "\n",
    "        print(\"size for loss\", output_captions.shape, captions_batch.shape)\n",
    "        loss = loss_function(output_captions.reshape(-1,output_captions.shape[2]), captions_batch.reshape(-1))\n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(\"creating checkpoint\")\n",
    "        #if iteration%5 == 0:\n",
    "        #   create_checkpoint(\"caption_chkpt_multi.pth\", net, optimizer, loss, iteration, epoch+1)\n",
    "        print(\"ITERATION:[{}/{}] | LOSS: {} | EPOCH = [{}/{}] | TIME ELAPSED ={}Mins\".format(iteration, round(29000/BATCH_SIZE),\n",
    "              round(loss.item(), 6), epoch+1, NUMBER_OF_EPOCHS, round((time()-t0)/60,2)))\n",
    "    print(\"\\n$$Loss = {},EPOCH: [{}/{}]\\n\\n\".format(round(loss.item(), 6), epoch+1, NUMBER_OF_EPOCHS))\n",
    "    create_checkpoint(\"epoch{}_chkpt_multi.pth\".format(epoch+1), net, optimizer, loss, iteration, epoch+1)\n",
    "\n",
    "create_checkpoint(\"Final_Model_multi.pth\", net, optimizer, loss, iteration, epoch+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([[6,3,2],[1,5,3]])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX2WORD[27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "51h4fNzkFzW1"
   },
   "outputs": [],
   "source": [
    "summary(net, (32,3,256,256))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "df8kug4joPSm",
    "outputId": "45205dcf-5588-4b58-b0d6-f139ff019881"
   },
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3, 5)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "IeeECFgABrPI",
    "outputId": "22998938-c015-4029-bd13-a320f6701ce3"
   },
   "outputs": [],
   "source": [
    "sum(p.numel() for p in net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "BfoSGxQ0-XJ7",
    "outputId": "e2857d09-4ae0-421f-a118-37051f4c173c"
   },
   "outputs": [],
   "source": [
    "z = restore_checkpoint(\"caption_chkpt_multi.pth\")\n",
    "z['model_state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "yg9kH1zPzX5r",
    "outputId": "bcf6353c-5552-4a53-f9b9-cfe7d0d9a5f4"
   },
   "outputs": [],
   "source": [
    "torch.mean(x, dim=0).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Xu4ZMjcCMwa"
   },
   "outputs": [],
   "source": [
    "#com gjjgdfgsdfgdfbdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WwFheAOkaoHU"
   },
   "outputs": [],
   "source": [
    "VOCAB['un']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "un uomo in un uomo con un uomo in un uomo con un uomo in un uomo <end> <pad>\n",
    "Image_idx 1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Caption_generator_restore_cp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
