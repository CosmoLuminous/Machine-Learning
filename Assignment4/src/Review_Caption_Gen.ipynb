{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Import modules'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "from collections import Counter\n",
    "from skimage import io, transform\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import numpy as np\n",
    "from time import time\n",
    "import collections\n",
    "import pickle\n",
    "import os\n",
    "import gensim\n",
    "import nltk\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cpu\n",
      "Using 0 GPUs!\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device =\", device)\n",
    "print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "platform = \"local\" #colab/local\n",
    "restore = False #Restore Checkpoint\n",
    "phase = \"Train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = {}\n",
    "WORD2IDX = {}\n",
    "IDX2WORD = {}\n",
    "TRAIN_CAPTIONS_DICT = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE = 129\n"
     ]
    }
   ],
   "source": [
    "class BuildVocab:\n",
    "    def __init__(self, captions_file_path):\n",
    "        self.captions_file_path = captions_file_path\n",
    "        self.raw_captions_dict = self.read_raw_captions()\n",
    "        # Preprocess captions\n",
    "        self.captions_dict = self.process_captions()\n",
    "        \n",
    "        # Create vocabulary\n",
    "        self.start = \"<start>\"\n",
    "        self.end = \"<end>\"\n",
    "        self.oov = \"<unk>\"\n",
    "        self.pad = \"<pad>\"\n",
    "        self.vocab = self.generate_vocabulary()\n",
    "        self.word2index = self.convert_word2index()        \n",
    "        self.index2word = self.convert_index2word()\n",
    "        \n",
    "    def read_raw_captions(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            Dictionary with raw captions list keyed by image ids (integers)\n",
    "        \"\"\"\n",
    "        captions_dict = {}\n",
    "        with open(self.captions_file_path, 'r', encoding='utf-8') as f:\n",
    "            for img_caption_line in f.readlines():\n",
    "                img_captions = img_caption_line.strip().split('\\t')\n",
    "                captions_dict[int(img_captions[0])] = img_captions[1:]\n",
    "        \n",
    "        TRAIN_CAPTIONS_DICT.clear()\n",
    "        TRAIN_CAPTIONS_DICT.update(captions_dict)        \n",
    "\n",
    "        return captions_dict \n",
    "\n",
    "    def process_captions(self):\n",
    "        \"\"\"\n",
    "        Use this function to generate dictionary and other preprocessing on captions\n",
    "        \"\"\"\n",
    "\n",
    "        raw_captions_dict = self.raw_captions_dict \n",
    "        \n",
    "        # Do the preprocessing here                \n",
    "        captions_dict = raw_captions_dict\n",
    "\n",
    "        return captions_dict\n",
    "    \n",
    "    def generate_vocabulary(self):\n",
    "        \n",
    "        captions_dict = self.captions_dict\n",
    "        \n",
    "        all_captions = \"\"        \n",
    "        for cap_lists in captions_dict.values():\n",
    "            all_captions += \" \".join(cap_lists)\n",
    "        \n",
    "        all_captions = nltk.tokenize.word_tokenize(all_captions.lower())\n",
    "        all_captions.sort()\n",
    "        \n",
    "        vocab = {self.pad :1, self.oov :1, self.start :1, self.end :1}\n",
    "        vocab_update = Counter(all_captions) \n",
    "        vocab_update = {k:v for k,v in vocab_update.items() if v >= freq_threshold}         \n",
    "        \n",
    "        vocab.update(vocab_update)\n",
    "\n",
    "        VOCAB.clear()\n",
    "        VOCAB.update(vocab)\n",
    "        \n",
    "        if platform == \"colab\":\n",
    "            fname = '/content/drive/My Drive/A4/dict/VOCAB_comp.pkl'\n",
    "        else:\n",
    "            fname = '../dict/VOCAB_comp.pkl'\n",
    "\n",
    "        with open(fname, 'wb') as handle:\n",
    "            pickle.dump(vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        print(\"VOCAB SIZE =\", len(VOCAB))\n",
    "        return vocab\n",
    "    \n",
    "    def convert_word2index(self):\n",
    "        \"\"\"\n",
    "        word to index converter\n",
    "        \"\"\"\n",
    "        word2index = {}\n",
    "        vocab = self.vocab\n",
    "        idx = 0\n",
    "        words = vocab.keys()\n",
    "        for w in words:\n",
    "            word2index[w] = idx\n",
    "            idx +=1\n",
    "\n",
    "        WORD2IDX.clear()\n",
    "        WORD2IDX.update(word2index)\n",
    "        if platform == \"colab\":\n",
    "            fname = '/content/drive/My Drive/A4/dict/WORD2IDX_comp.pkl'\n",
    "        else:\n",
    "            fname = '../dict/WORD2IDX_comp.pkl'\n",
    "        #if not os.path.isfile(fname):\n",
    "        with open(fname, 'wb') as handle:\n",
    "            pickle.dump(word2index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        return word2index\n",
    "    \n",
    "    def convert_index2word(self):\n",
    "        \"\"\"\n",
    "        index to word converter\n",
    "        \"\"\"\n",
    "        index2word = {}\n",
    "        w2i = self.word2index\n",
    "        idx = 0\n",
    "        \n",
    "        for k, v in w2i.items():\n",
    "            index2word[v] = k\n",
    "            \n",
    "        IDX2WORD.clear()\n",
    "        IDX2WORD.update(index2word)\n",
    "        if platform == \"colab\":\n",
    "            fname = '/content/drive/My Drive/A4/dict/IDX2WORD_comp.pkl'\n",
    "        else:\n",
    "            fname = '../dict/IDX2WORD_comp.pkl'\n",
    "        #if not os.path.isfile(fname):\n",
    "        with open(fname, 'wb') as handle:\n",
    "            pickle.dump(index2word, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        return index2word\n",
    "\n",
    "if platform == \"colab\":\n",
    "    CAPTIONS_FILE_PATH = '/content/drive/My Drive/A4/train_captions.tsv'\n",
    "else:\n",
    "    #CAPTIONS_FILE_PATH = \"D:/Padhai/IIT Delhi MS(R)/2019-20 Sem II/COL774 Machine Learning/Assignment/Assignment4/train_captions.tsv\"\n",
    "    CAPTIONS_FILE_PATH = \"../data/train_cap64.tsv\"\n",
    "\n",
    "freq_threshold = 5\n",
    "\n",
    "if phase == \"Train\":\n",
    "    BuildVocab(CAPTIONS_FILE_PATH)\n",
    "else:\n",
    "    VOCAB.clear()\n",
    "    WORD2IDX.clear()\n",
    "    IDX2WORD.clear()\n",
    "    if platform != 'colab':\n",
    "        with open('../dict/VOCAB.pkl', 'rb') as handle:\n",
    "            VOCAB = pickle.load(handle)\n",
    "        with open('../dict/WORD2IDX.pkl', 'rb') as handle:\n",
    "            WORD2IDX = pickle.load(handle)\n",
    "        with open('../dict/IDX2WORD.pkl', 'rb') as handle:\n",
    "            IDX2WORD = pickle.load(handle)\n",
    "        print(\"Vocab Loaded Successfully\")\n",
    "    else:\n",
    "        with open('/content/drive/My Drive/A4/dict/VOCAB.pkl', 'rb') as handle:\n",
    "            VOCAB = pickle.load(handle)\n",
    "        with open('/content/drive/My Drive/A4/dict/WORD2IDX.pkl', 'rb') as handle:\n",
    "            WORD2IDX = pickle.load(handle)\n",
    "        with open('/content/drive/My Drive/A4/dict/IDX2WORD.pkl', 'rb') as handle:\n",
    "            IDX2WORD = pickle.load(handle)\n",
    "        print(\"Vocab Loaded Successfully\")\n",
    "    print(\"VOCAB SIZE =\", len(VOCAB))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to pre-process the training images.\n",
    "img_transform = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, img_dir, captions_dict, img_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dir (string): Directory with all the images.\n",
    "            captions_dict: Dictionary with captions list keyed by image ids (integers)\n",
    "            img_transform (callable, optional): Optional transform to be applied\n",
    "                on the image sample.\n",
    "\n",
    "            captions_transform: (callable, optional): Optional transform to be applied\n",
    "                on the caption sample (list).\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.captions_dict = captions_dict\n",
    "        self.img_transform = img_transform\n",
    "        images = os.listdir(os.path.join(img_dir))\n",
    "        images = [i.split(\"_\")[1][:-4] for i in images]\n",
    "        images = [int(i) for i in images]\n",
    "        images.sort()\n",
    "        self.image_ids = images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #print('IMG No.', self.image_ids[idx])\n",
    "        img_name = os.path.join(self.img_dir, 'image_{}.jpg'.format(self.image_ids[idx]))\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        #print(\"RAW IMG\", image.shape)\n",
    "        if self.img_transform:\n",
    "            image = self.img_transform(image)\n",
    "            \n",
    "        start = \"<start>\"\n",
    "        end = \"<end>\"\n",
    "        oov = \"<unk>\"\n",
    "        \n",
    "        captions = self.captions_dict[self.image_ids[idx]]\n",
    "        processed_captions = list(map(lambda x: nltk.tokenize.word_tokenize(x.lower()) ,captions))\n",
    "        \n",
    "        processed_captions = list(map(lambda x: [start]+ x + [end], processed_captions))\n",
    "        processed_captions = list(map(lambda x: list(map(lambda y: WORD2IDX[y] if y in VOCAB else WORD2IDX[oov],x)),\n",
    "                                  processed_captions))\n",
    "        #processed_captions = list(map(lambda x: torch.LongTensor(x) , processed_captions))\n",
    "        #print(processed_captions)\n",
    "        sample = {'idx':idx, 'image': image, 'captions': processed_captions}\n",
    "\n",
    "        return sample\n",
    "    \n",
    "def custom_batch(batch):\n",
    "    batch_size = len(batch)\n",
    "    captions = []\n",
    "    normalize_img = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    list(map(lambda b: captions.extend(b['captions']),batch))    \n",
    "    x = list(map(lambda b: b['image'].unsqueeze(0),batch)) \n",
    "    #print(x)\n",
    "    captions = list(map(lambda c: torch.LongTensor(c),captions))\n",
    "    lengths = list(map(lambda c: len(c),captions))\n",
    "    captions = pad_sequence(captions, batch_first=True)\n",
    "    images = torch.cat(x, dim=0)\n",
    "    \n",
    "    sample = {'image': images, 'captions': captions}    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        resnet50 = models.resnet50(pretrained=True, progress=True)        \n",
    "        \n",
    "        for param in resnet50.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.resnet50 = resnet50\n",
    "        self.fc2 = nn.Linear(in_features=1000, out_features = embed_dim)\n",
    "        print(\"EMBED DIM =\", embed_dim)\n",
    "        print(\"resnet50 Loaded Successfully..!\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet50(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_units, lstm_layers = 1):\n",
    "        super(Decoder, self).__init__()\n",
    "        vocab_size = len(VOCAB)\n",
    "        print(\"VOCAB SIZE DECODER INIT =\", vocab_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = embed_dim, hidden_size = hidden_units,\n",
    "                            num_layers = lstm_layers, batch_first = True)\n",
    "        \n",
    "        self.embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "        self.linear = nn.Linear(hidden_units, vocab_size)\n",
    "    \n",
    "    def forward(self, image_features, image_captions):\n",
    "        \n",
    "        embedded_captions = self.embed(image_captions)\n",
    "        input_lstm = torch.cat((image_features, embedded_captions[:,:-1]), dim = 1)\n",
    "        #print(\"LSTM INPUT SHAPE\", input_lstm.shape)\n",
    "        lstm_outputs, _ = self.lstm(input_lstm)\n",
    "        \n",
    "        lstm_outputs = self.linear(lstm_outputs)\n",
    "        \n",
    "        return lstm_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Save and Restore Checkpoints'''\n",
    "def create_checkpoint(path,model, optim_obj, loss_obj,iteration, epoch):\n",
    "    checkpoint = {'epoch': epoch,\n",
    "                  'iteration': iteration,\n",
    "                  'model_state_dict': model.state_dict()}\n",
    "\n",
    "    if platform == \"colab\":\n",
    "        directory = '/content/drive/My Drive/A4/review_cp/'\n",
    "    else:\n",
    "        directory = '../review_cp/'\n",
    "\n",
    "    torch.save(checkpoint, directory + path)\n",
    "    \n",
    "def restore_checkpoint(path):\n",
    "    new_state_dict = collections.OrderedDict()\n",
    "    if platform == \"colab\":\n",
    "        directory = '/content/drive/My Drive/A4/review_cp/'\n",
    "        checkpoint = torch.load(directory + path, map_location=torch.device('cpu'))\n",
    "    else:\n",
    "        directory = '../review_cp/'\n",
    "        checkpoint = torch.load(directory + path, map_location=torch.device('cpu'))    \n",
    "    \n",
    "    epoch = checkpoint['epoch']\n",
    "    new_state_dict = checkpoint['model_state_dict']\n",
    "    iteration = checkpoint['iteration']\n",
    "    print(\"Iterations = {}, Epoch = {}\".format(iteration, epoch))\n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBED DIM = 256\n",
      "resnet50 Loaded Successfully..!\n",
      "VOCAB SIZE DECODER INIT = 129\n",
      "DECODER PARAMS =1676161, ENCODER TOTAL PARAMS =25813288, ENCODER TRAINABLE PARAMS =256256, ENCODER FC2 PARAMS =256256, TOTAL ADAM PARAMS: 1932417\n",
      "TOTAL EPOCHS: 3, BATCH SIZE: 2, OPTIMIZER: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "$$$$$----EPOCH 1----$$$$$$\n",
      "IMG No. 28\n",
      "IMG No. 31\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 22])\n",
      "ITERATION:[1/14501] | LOSS: 4.867113 | EPOCH = [1/3] | TIME ELAPSED =0.01Mins\n",
      "IMG No. 47\n",
      "IMG No. 48\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 20])\n",
      "ITERATION:[2/14501] | LOSS: 4.726323 | EPOCH = [1/3] | TIME ELAPSED =0.02Mins\n",
      "IMG No. 37\n",
      "IMG No. 2\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 23])\n",
      "ITERATION:[3/14501] | LOSS: 4.506893 | EPOCH = [1/3] | TIME ELAPSED =0.04Mins\n",
      "IMG No. 49\n",
      "IMG No. 32\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 25])\n",
      "ITERATION:[4/14501] | LOSS: 4.366681 | EPOCH = [1/3] | TIME ELAPSED =0.05Mins\n",
      "IMG No. 60\n",
      "IMG No. 19\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 30])\n",
      "ITERATION:[5/14501] | LOSS: 4.043561 | EPOCH = [1/3] | TIME ELAPSED =0.07Mins\n",
      "IMG No. 5\n",
      "IMG No. 62\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 16])\n",
      "ITERATION:[6/14501] | LOSS: 3.698833 | EPOCH = [1/3] | TIME ELAPSED =0.08Mins\n",
      "IMG No. 41\n",
      "IMG No. 42\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 41])\n",
      "ITERATION:[7/14501] | LOSS: 3.351243 | EPOCH = [1/3] | TIME ELAPSED =0.1Mins\n",
      "IMG No. 61\n",
      "IMG No. 12\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 17])\n",
      "ITERATION:[8/14501] | LOSS: 3.492592 | EPOCH = [1/3] | TIME ELAPSED =0.12Mins\n",
      "IMG No. 39\n",
      "IMG No. 6\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 29])\n",
      "ITERATION:[9/14501] | LOSS: 2.7464 | EPOCH = [1/3] | TIME ELAPSED =0.13Mins\n",
      "IMG No. 8\n",
      "IMG No. 55\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 27])\n",
      "ITERATION:[10/14501] | LOSS: 3.414176 | EPOCH = [1/3] | TIME ELAPSED =0.15Mins\n",
      "IMG No. 64\n",
      "IMG No. 50\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 33])\n",
      "ITERATION:[11/14501] | LOSS: 3.187881 | EPOCH = [1/3] | TIME ELAPSED =0.16Mins\n",
      "IMG No. 18\n",
      "IMG No. 36\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 41])\n",
      "ITERATION:[12/14501] | LOSS: 3.114085 | EPOCH = [1/3] | TIME ELAPSED =0.18Mins\n",
      "IMG No. 21\n",
      "IMG No. 59\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 21])\n",
      "ITERATION:[13/14501] | LOSS: 2.880815 | EPOCH = [1/3] | TIME ELAPSED =0.19Mins\n",
      "IMG No. 53\n",
      "IMG No. 27\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 25])\n",
      "ITERATION:[14/14501] | LOSS: 2.888531 | EPOCH = [1/3] | TIME ELAPSED =0.21Mins\n",
      "IMG No. 1\n",
      "IMG No. 29\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 15])\n",
      "ITERATION:[15/14501] | LOSS: 2.411639 | EPOCH = [1/3] | TIME ELAPSED =0.22Mins\n",
      "IMG No. 58\n",
      "IMG No. 26\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 33])\n",
      "ITERATION:[16/14501] | LOSS: 2.92648 | EPOCH = [1/3] | TIME ELAPSED =0.24Mins\n",
      "IMG No. 52\n",
      "IMG No. 45\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 23])\n",
      "ITERATION:[17/14501] | LOSS: 2.705241 | EPOCH = [1/3] | TIME ELAPSED =0.26Mins\n",
      "IMG No. 0\n",
      "IMG No. 9\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 16])\n",
      "ITERATION:[18/14501] | LOSS: 2.60396 | EPOCH = [1/3] | TIME ELAPSED =0.28Mins\n",
      "IMG No. 44\n",
      "IMG No. 20\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 26])\n",
      "ITERATION:[19/14501] | LOSS: 2.68805 | EPOCH = [1/3] | TIME ELAPSED =0.3Mins\n",
      "IMG No. 17\n",
      "IMG No. 56\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 25])\n",
      "ITERATION:[20/14501] | LOSS: 3.256607 | EPOCH = [1/3] | TIME ELAPSED =0.32Mins\n",
      "IMG No. 24\n",
      "IMG No. 43\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 24])\n",
      "ITERATION:[21/14501] | LOSS: 2.927105 | EPOCH = [1/3] | TIME ELAPSED =0.34Mins\n",
      "IMG No. 38\n",
      "IMG No. 11\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 26])\n",
      "ITERATION:[22/14501] | LOSS: 2.819673 | EPOCH = [1/3] | TIME ELAPSED =0.36Mins\n",
      "IMG No. 23\n",
      "IMG No. 14\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 16])\n",
      "ITERATION:[23/14501] | LOSS: 2.673894 | EPOCH = [1/3] | TIME ELAPSED =0.38Mins\n",
      "IMG No. 4\n",
      "IMG No. 63\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 21])\n",
      "ITERATION:[24/14501] | LOSS: 2.746454 | EPOCH = [1/3] | TIME ELAPSED =0.4Mins\n",
      "IMG No. 65\n",
      "IMG No. 16\n",
      "\n",
      "LEARNING RATE = 0.00094 Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.00094\n",
      "    weight_decay: 0\n",
      ")\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 22])\n",
      "ITERATION:[25/14501] | LOSS: 2.818993 | EPOCH = [1/3] | TIME ELAPSED =0.42Mins\n",
      "IMG No. 13\n",
      "IMG No. 46\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 23])\n",
      "ITERATION:[26/14501] | LOSS: 2.513417 | EPOCH = [1/3] | TIME ELAPSED =0.44Mins\n",
      "IMG No. 22\n",
      "IMG No. 57\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 26])\n",
      "ITERATION:[27/14501] | LOSS: 2.782034 | EPOCH = [1/3] | TIME ELAPSED =0.46Mins\n",
      "IMG No. 33\n",
      "IMG No. 10\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 26])\n",
      "ITERATION:[28/14501] | LOSS: 2.450824 | EPOCH = [1/3] | TIME ELAPSED =0.48Mins\n",
      "IMG No. 3\n",
      "IMG No. 40\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 25])\n",
      "ITERATION:[29/14501] | LOSS: 2.839247 | EPOCH = [1/3] | TIME ELAPSED =0.5Mins\n",
      "IMG No. 54\n",
      "IMG No. 30\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 21])\n",
      "ITERATION:[30/14501] | LOSS: 2.655682 | EPOCH = [1/3] | TIME ELAPSED =0.52Mins\n",
      "IMG No. 7\n",
      "IMG No. 66\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 23])\n",
      "ITERATION:[31/14501] | LOSS: 3.120499 | EPOCH = [1/3] | TIME ELAPSED =0.54Mins\n",
      "IMG No. 15\n",
      "IMG No. 35\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 23])\n",
      "ITERATION:[32/14501] | LOSS: 2.801758 | EPOCH = [1/3] | TIME ELAPSED =0.56Mins\n",
      "\n",
      "$$Loss = 2.801758,EPOCH: [1/3]\n",
      "\n",
      "\n",
      "$$$$$----EPOCH 2----$$$$$$\n",
      "IMG No. 55\n",
      "IMG No. 60\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 30])\n",
      "ITERATION:[1/14501] | LOSS: 2.957918 | EPOCH = [2/3] | TIME ELAPSED =0.58Mins\n",
      "IMG No. 7\n",
      "IMG No. 0\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 17])\n",
      "ITERATION:[2/14501] | LOSS: 2.794957 | EPOCH = [2/3] | TIME ELAPSED =0.6Mins\n",
      "IMG No. 35\n",
      "IMG No. 4\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 21])\n",
      "ITERATION:[3/14501] | LOSS: 2.542873 | EPOCH = [2/3] | TIME ELAPSED =0.62Mins\n",
      "IMG No. 33\n",
      "IMG No. 5\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 18])\n",
      "ITERATION:[4/14501] | LOSS: 2.234724 | EPOCH = [2/3] | TIME ELAPSED =0.64Mins\n",
      "IMG No. 44\n",
      "IMG No. 22\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 26])\n",
      "ITERATION:[5/14501] | LOSS: 2.685801 | EPOCH = [2/3] | TIME ELAPSED =0.66Mins\n",
      "IMG No. 36\n",
      "IMG No. 13\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 41])\n",
      "ITERATION:[6/14501] | LOSS: 2.610509 | EPOCH = [2/3] | TIME ELAPSED =0.68Mins\n",
      "IMG No. 59\n",
      "IMG No. 14\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 20])\n",
      "ITERATION:[7/14501] | LOSS: 2.331562 | EPOCH = [2/3] | TIME ELAPSED =0.7Mins\n",
      "IMG No. 54\n",
      "IMG No. 9\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 21])\n",
      "ITERATION:[8/14501] | LOSS: 2.461391 | EPOCH = [2/3] | TIME ELAPSED =0.72Mins\n",
      "IMG No. 15\n",
      "IMG No. 46\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 23])\n",
      "ITERATION:[9/14501] | LOSS: 2.361582 | EPOCH = [2/3] | TIME ELAPSED =0.74Mins\n",
      "IMG No. 3\n",
      "IMG No. 57\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 20])\n",
      "ITERATION:[10/14501] | LOSS: 2.560196 | EPOCH = [2/3] | TIME ELAPSED =0.76Mins\n",
      "IMG No. 10\n",
      "IMG No. 12\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 26])\n",
      "ITERATION:[11/14501] | LOSS: 2.48353 | EPOCH = [2/3] | TIME ELAPSED =0.78Mins\n",
      "IMG No. 26\n",
      "IMG No. 63\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 33])\n",
      "ITERATION:[12/14501] | LOSS: 2.47096 | EPOCH = [2/3] | TIME ELAPSED =0.8Mins\n",
      "IMG No. 45\n",
      "IMG No. 11\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 26])\n",
      "ITERATION:[13/14501] | LOSS: 2.643008 | EPOCH = [2/3] | TIME ELAPSED =0.82Mins\n",
      "IMG No. 6\n",
      "IMG No. 66\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 23])\n",
      "ITERATION:[14/14501] | LOSS: 2.219383 | EPOCH = [2/3] | TIME ELAPSED =0.84Mins\n",
      "IMG No. 65\n",
      "IMG No. 29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 17])\n",
      "ITERATION:[15/14501] | LOSS: 2.146194 | EPOCH = [2/3] | TIME ELAPSED =0.86Mins\n",
      "IMG No. 31\n",
      "IMG No. 28\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 22])\n",
      "ITERATION:[16/14501] | LOSS: 2.469762 | EPOCH = [2/3] | TIME ELAPSED =0.87Mins\n",
      "IMG No. 49\n",
      "IMG No. 39\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 29])\n",
      "ITERATION:[17/14501] | LOSS: 2.434672 | EPOCH = [2/3] | TIME ELAPSED =0.89Mins\n",
      "IMG No. 42\n",
      "IMG No. 21\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 21])\n",
      "ITERATION:[18/14501] | LOSS: 2.393933 | EPOCH = [2/3] | TIME ELAPSED =0.91Mins\n",
      "IMG No. 30\n",
      "IMG No. 38\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 18])\n",
      "ITERATION:[19/14501] | LOSS: 1.909273 | EPOCH = [2/3] | TIME ELAPSED =0.93Mins\n",
      "IMG No. 1\n",
      "IMG No. 20\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 16])\n",
      "ITERATION:[20/14501] | LOSS: 1.916919 | EPOCH = [2/3] | TIME ELAPSED =0.95Mins\n",
      "IMG No. 61\n",
      "IMG No. 32\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 15])\n",
      "ITERATION:[21/14501] | LOSS: 2.762729 | EPOCH = [2/3] | TIME ELAPSED =0.97Mins\n",
      "IMG No. 56\n",
      "IMG No. 64\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 25])\n",
      "ITERATION:[22/14501] | LOSS: 2.457809 | EPOCH = [2/3] | TIME ELAPSED =0.99Mins\n",
      "IMG No. 50\n",
      "IMG No. 8\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 33])\n",
      "ITERATION:[23/14501] | LOSS: 2.613138 | EPOCH = [2/3] | TIME ELAPSED =1.01Mins\n",
      "IMG No. 41\n",
      "IMG No. 47\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 41])\n",
      "ITERATION:[24/14501] | LOSS: 2.589723 | EPOCH = [2/3] | TIME ELAPSED =1.03Mins\n",
      "IMG No. 52\n",
      "IMG No. 23\n",
      "\n",
      "LEARNING RATE = 0.0008835999999999999 Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0008835999999999999\n",
      "    weight_decay: 0\n",
      ")\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 23])\n",
      "ITERATION:[25/14501] | LOSS: 2.419643 | EPOCH = [2/3] | TIME ELAPSED =1.05Mins\n",
      "IMG No. 40\n",
      "IMG No. 18\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 25])\n",
      "ITERATION:[26/14501] | LOSS: 2.497054 | EPOCH = [2/3] | TIME ELAPSED =1.07Mins\n",
      "IMG No. 53\n",
      "IMG No. 2\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 25])\n",
      "ITERATION:[27/14501] | LOSS: 2.364946 | EPOCH = [2/3] | TIME ELAPSED =1.09Mins\n",
      "IMG No. 27\n",
      "IMG No. 24\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 24])\n",
      "ITERATION:[28/14501] | LOSS: 2.248376 | EPOCH = [2/3] | TIME ELAPSED =1.11Mins\n",
      "IMG No. 58\n",
      "IMG No. 48\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 24])\n",
      "ITERATION:[29/14501] | LOSS: 2.580326 | EPOCH = [2/3] | TIME ELAPSED =1.14Mins\n",
      "IMG No. 62\n",
      "IMG No. 43\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 18])\n",
      "ITERATION:[30/14501] | LOSS: 2.446329 | EPOCH = [2/3] | TIME ELAPSED =1.16Mins\n",
      "IMG No. 17\n",
      "IMG No. 19\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 22])\n",
      "ITERATION:[31/14501] | LOSS: 2.832067 | EPOCH = [2/3] | TIME ELAPSED =1.19Mins\n",
      "IMG No. 37\n",
      "IMG No. 16\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 23])\n",
      "ITERATION:[32/14501] | LOSS: 2.431192 | EPOCH = [2/3] | TIME ELAPSED =1.21Mins\n",
      "\n",
      "$$Loss = 2.431192,EPOCH: [2/3]\n",
      "\n",
      "\n",
      "$$$$$----EPOCH 3----$$$$$$\n",
      "IMG No. 12\n",
      "IMG No. 37\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 23])\n",
      "ITERATION:[1/14501] | LOSS: 2.26316 | EPOCH = [3/3] | TIME ELAPSED =1.24Mins\n",
      "IMG No. 30\n",
      "IMG No. 2\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 18])\n",
      "ITERATION:[2/14501] | LOSS: 2.05221 | EPOCH = [3/3] | TIME ELAPSED =1.27Mins\n",
      "IMG No. 46\n",
      "IMG No. 45\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 21])\n",
      "ITERATION:[3/14501] | LOSS: 1.904132 | EPOCH = [3/3] | TIME ELAPSED =1.29Mins\n",
      "IMG No. 53\n",
      "IMG No. 26\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 33])\n",
      "ITERATION:[4/14501] | LOSS: 2.187039 | EPOCH = [3/3] | TIME ELAPSED =1.32Mins\n",
      "IMG No. 39\n",
      "IMG No. 44\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 29])\n",
      "ITERATION:[5/14501] | LOSS: 2.202477 | EPOCH = [3/3] | TIME ELAPSED =1.35Mins\n",
      "IMG No. 38\n",
      "IMG No. 60\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 30])\n",
      "ITERATION:[6/14501] | LOSS: 2.286393 | EPOCH = [3/3] | TIME ELAPSED =1.37Mins\n",
      "IMG No. 9\n",
      "IMG No. 8\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 27])\n",
      "ITERATION:[7/14501] | LOSS: 2.247749 | EPOCH = [3/3] | TIME ELAPSED =1.39Mins\n",
      "IMG No. 5\n",
      "IMG No. 28\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 16])\n",
      "ITERATION:[8/14501] | LOSS: 2.154771 | EPOCH = [3/3] | TIME ELAPSED =1.42Mins\n",
      "IMG No. 42\n",
      "IMG No. 29\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 18])\n",
      "ITERATION:[9/14501] | LOSS: 1.813164 | EPOCH = [3/3] | TIME ELAPSED =1.44Mins\n",
      "IMG No. 7\n",
      "IMG No. 35\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 18])\n",
      "ITERATION:[10/14501] | LOSS: 2.562711 | EPOCH = [3/3] | TIME ELAPSED =1.46Mins\n",
      "IMG No. 27\n",
      "IMG No. 17\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 21])\n",
      "ITERATION:[11/14501] | LOSS: 2.443467 | EPOCH = [3/3] | TIME ELAPSED =1.48Mins\n",
      "IMG No. 63\n",
      "IMG No. 54\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 21])\n",
      "ITERATION:[12/14501] | LOSS: 2.392685 | EPOCH = [3/3] | TIME ELAPSED =1.5Mins\n",
      "IMG No. 50\n",
      "IMG No. 33\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 33])\n",
      "ITERATION:[13/14501] | LOSS: 2.166484 | EPOCH = [3/3] | TIME ELAPSED =1.52Mins\n",
      "IMG No. 48\n",
      "IMG No. 23\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 16])\n",
      "ITERATION:[14/14501] | LOSS: 1.86041 | EPOCH = [3/3] | TIME ELAPSED =1.53Mins\n",
      "IMG No. 64\n",
      "IMG No. 41\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 41])\n",
      "ITERATION:[15/14501] | LOSS: 2.156599 | EPOCH = [3/3] | TIME ELAPSED =1.56Mins\n",
      "IMG No. 18\n",
      "IMG No. 58\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 25])\n",
      "ITERATION:[16/14501] | LOSS: 2.384923 | EPOCH = [3/3] | TIME ELAPSED =1.57Mins\n",
      "IMG No. 20\n",
      "IMG No. 0\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 16])\n",
      "ITERATION:[17/14501] | LOSS: 1.946659 | EPOCH = [3/3] | TIME ELAPSED =1.6Mins\n",
      "IMG No. 65\n",
      "IMG No. 16\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 22])\n",
      "ITERATION:[18/14501] | LOSS: 2.140201 | EPOCH = [3/3] | TIME ELAPSED =1.62Mins\n",
      "IMG No. 21\n",
      "IMG No. 13\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 23])\n",
      "ITERATION:[19/14501] | LOSS: 2.228335 | EPOCH = [3/3] | TIME ELAPSED =1.64Mins\n",
      "IMG No. 66\n",
      "IMG No. 15\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 23])\n",
      "ITERATION:[20/14501] | LOSS: 2.252537 | EPOCH = [3/3] | TIME ELAPSED =1.67Mins\n",
      "IMG No. 62\n",
      "IMG No. 36\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 41])\n",
      "ITERATION:[21/14501] | LOSS: 2.420891 | EPOCH = [3/3] | TIME ELAPSED =1.69Mins\n",
      "IMG No. 14\n",
      "IMG No. 19\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 22])\n",
      "ITERATION:[22/14501] | LOSS: 2.283683 | EPOCH = [3/3] | TIME ELAPSED =1.72Mins\n",
      "IMG No. 6\n",
      "IMG No. 31\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 22])\n",
      "ITERATION:[23/14501] | LOSS: 1.8433 | EPOCH = [3/3] | TIME ELAPSED =1.73Mins\n",
      "IMG No. 59\n",
      "IMG No. 4\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 21])\n",
      "ITERATION:[24/14501] | LOSS: 1.939987 | EPOCH = [3/3] | TIME ELAPSED =1.75Mins\n",
      "IMG No. 1\n",
      "IMG No. 57\n",
      "\n",
      "LEARNING RATE = 0.0008305839999999999 Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0008305839999999999\n",
      "    weight_decay: 0\n",
      ")\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 19])\n",
      "ITERATION:[25/14501] | LOSS: 1.857517 | EPOCH = [3/3] | TIME ELAPSED =1.77Mins\n",
      "IMG No. 11\n",
      "IMG No. 56\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 26])\n",
      "ITERATION:[26/14501] | LOSS: 2.506447 | EPOCH = [3/3] | TIME ELAPSED =1.79Mins\n",
      "IMG No. 3\n",
      "IMG No. 10\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 26])\n",
      "ITERATION:[27/14501] | LOSS: 2.410307 | EPOCH = [3/3] | TIME ELAPSED =1.81Mins\n",
      "IMG No. 52\n",
      "IMG No. 22\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 26])\n",
      "ITERATION:[28/14501] | LOSS: 2.42503 | EPOCH = [3/3] | TIME ELAPSED =1.83Mins\n",
      "IMG No. 43\n",
      "IMG No. 24\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 24])\n",
      "ITERATION:[29/14501] | LOSS: 2.132828 | EPOCH = [3/3] | TIME ELAPSED =1.85Mins\n",
      "IMG No. 55\n",
      "IMG No. 49\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 25])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION:[30/14501] | LOSS: 2.367513 | EPOCH = [3/3] | TIME ELAPSED =1.87Mins\n",
      "IMG No. 47\n",
      "IMG No. 32\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 20])\n",
      "ITERATION:[31/14501] | LOSS: 2.407211 | EPOCH = [3/3] | TIME ELAPSED =1.88Mins\n",
      "IMG No. 40\n",
      "IMG No. 61\n",
      "TRAIN INPUTS torch.Size([2, 3, 224, 224]) torch.Size([10, 25])\n",
      "ITERATION:[32/14501] | LOSS: 2.304646 | EPOCH = [3/3] | TIME ELAPSED =1.9Mins\n",
      "\n",
      "$$Loss = 2.304646,EPOCH: [3/3]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if platform == \"colab\":\n",
    "    IMAGE_DIR = '/content/drive/My Drive/train_images/'\n",
    "else:\n",
    "    #IMAGE_DIR = 'D:/Padhai/IIT Delhi MS(R)/2019-20 Sem II/COL774 Machine Learning/Assignment/Assignment4/train_images/'\n",
    "    IMAGE_DIR = \"../data/train/\"\n",
    "train_dataset = ImageCaptionsDataset(\n",
    "    IMAGE_DIR, TRAIN_CAPTIONS_DICT, img_transform=img_transform\n",
    ")\n",
    "NUMBER_OF_EPOCHS = 3\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 2\n",
    "NUM_WORKERS = 0 # Parallel threads for dataloading\n",
    "EMBED_DIM = 256\n",
    "HIDDEN_UNITS = 512\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "# Creating the DataLoader for batching purposes\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS,\n",
    "                          collate_fn=custom_batch)\n",
    "\n",
    "\n",
    "encoder = Encoder(EMBED_DIM)\n",
    "decoder = Decoder(EMBED_DIM, HIDDEN_UNITS)\n",
    "\n",
    "if device == \"cuda\":\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "    loss_function = loss_function.cuda()\n",
    "    print(\"ENCODER DECODER AND LOSS FUN. TO CUDA...!\")\n",
    "\n",
    "\n",
    "decoder_params = sum(p.numel() for p in decoder.parameters())\n",
    "encoder_total = sum(p.numel() for p in encoder.parameters())\n",
    "encoder_trainable_params = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "encoder_fc2_params = sum(p.numel() for p in encoder.fc2.parameters())\n",
    "\n",
    "\n",
    "paramaters = list(decoder.parameters()) + list(encoder.fc2.parameters())\n",
    "optimizer = optim.Adam(paramaters, lr=LEARNING_RATE)\n",
    "params_for_adam = sum(p.numel() for p in paramaters)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=WORD2IDX[\"<pad>\"])\n",
    "\n",
    "print(\"DECODER PARAMS ={}, ENCODER TOTAL PARAMS ={}, ENCODER TRAINABLE PARAMS ={}, ENCODER FC2 PARAMS ={}, TOTAL ADAM PARAMS: {}\"\n",
    "      .format(decoder_params, encoder_total, encoder_trainable_params, encoder_fc2_params, params_for_adam))\n",
    "print(\"TOTAL EPOCHS: {}, BATCH SIZE: {}, OPTIMIZER: {}\".format(NUMBER_OF_EPOCHS, BATCH_SIZE, optimizer))\n",
    "\n",
    "\n",
    "if device != \"cpu\":\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "t0 = time()\n",
    "for epoch in range(NUMBER_OF_EPOCHS):\n",
    "    print(\"$$$$$----EPOCH {}----$$$$$$\".format(epoch+1))\n",
    "    iteration = 0\n",
    "    \n",
    "    for batch_idx, sample in enumerate(train_loader):\n",
    "        iteration +=1\n",
    "        if iteration%25 == 0:\n",
    "            LEARNING_RATE *= 0.98\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = LEARNING_RATE\n",
    "            print(\"----LEARNING RATE =\", LEARNING_RATE)\n",
    "            \n",
    "            \n",
    "        image_batch, captions_batch = sample['image'], sample['captions']\n",
    "        print(\"TRAIN INPUTS\", image_batch.shape, captions_batch.shape)\n",
    "        #print(image_batch)\n",
    "        #print(captions_batch)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if device != \"cpu\":\n",
    "            image_batch, captions_batch = image_batch.cuda(), captions_batch.cuda()\n",
    "        \n",
    "        image_features = encoder(image_batch)\n",
    "        #print(\"IMAGE FEATURES:\",image_features)\n",
    "        image_features = torch.Tensor.repeat_interleave(image_features, repeats=5 , dim=0)\n",
    "        #print(\"IMAGE INTERLEAVE FEATURES:\",image_features)\n",
    "        image_features = image_features.unsqueeze(1)\n",
    "        \n",
    "        decoder_op = decoder(image_features, captions_batch)\n",
    "        #print(\"DECODER OP\", decoder_op.shape)\n",
    "        #print(decoder_op)\n",
    "        #print(\"LOSS FUN INPUT\", decoder_op.view(-1, VOCAB_SIZE).shape, captions_batch.view(-1).shape)\n",
    "        loss = loss_function(decoder_op.view(-1, VOCAB_SIZE), captions_batch.view(-1))\n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "        \n",
    "        if iteration%50 == 0:\n",
    "            create_checkpoint(\"encoder_review.pth\", encoder, optimizer, loss, iteration, epoch+1)\n",
    "            create_checkpoint(\"decoder_review.pth\", decoder, optimizer, loss, iteration, epoch+1)\n",
    "            \n",
    "        print(\"ITERATION:[{}/{}] | LOSS: {} | EPOCH = [{}/{}] | TIME ELAPSED ={}Mins\".format(iteration, round(29000/BATCH_SIZE)+1,\n",
    "              round(loss.item(), 6), epoch+1, NUMBER_OF_EPOCHS, round((time()-t0)/60,2)))\n",
    "    print(\"\\n$$Loss = {},EPOCH: [{}/{}]\\n\\n\".format(round(loss.item(), 6), epoch+1, NUMBER_OF_EPOCHS))\n",
    "    create_checkpoint(\"encoder_review_epoch.pth\", encoder, optimizer, loss, iteration, epoch+1)\n",
    "    create_checkpoint(\"decoder_review_epoch.pth\", decoder, optimizer, loss, iteration, epoch+1)\n",
    "\n",
    "create_checkpoint(\"encoder_review_final.pth\", encoder, optimizer, loss, iteration, epoch+1)\n",
    "create_checkpoint(\"decoder_review_final.pth\", decoder, optimizer, loss, iteration, epoch+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
