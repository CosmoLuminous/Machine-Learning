{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnWhNJC8M5d5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from collections import Counter\n",
    "from skimage import io, transform\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kC9JUlQMM5d9"
   },
   "source": [
    "### Image Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g6ClqmTUM5d9"
   },
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "        return img\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, image):\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return image\n",
    "\n",
    "\n",
    "IMAGE_RESIZE = (256, 256)\n",
    "# Sequentially compose the transforms\n",
    "img_transform = transforms.Compose([Rescale(IMAGE_RESIZE), ToTensor()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K5CM4cBhM5eA"
   },
   "source": [
    "### Captions Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rbEQS053M5eA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE = 127\n"
     ]
    }
   ],
   "source": [
    "class CaptionsPreprocessing:\n",
    "    \"\"\"Preprocess the captions, generate vocabulary and convert words to tensor tokens\n",
    "    Args:\n",
    "        captions_file_path (string): captions tsv file path\n",
    "    \"\"\"\n",
    "    def __init__(self, captions_file_path):\n",
    "        self.captions_file_path = captions_file_path\n",
    "\n",
    "        # Read raw captions\n",
    "        self.raw_captions_dict = self.read_raw_captions()\n",
    "\n",
    "        # Preprocess captions\n",
    "        self.captions_dict = self.process_captions()\n",
    "\n",
    "        # Create vocabulary\n",
    "        self.start = \"<start>\"\n",
    "        self.end = \"<end>\"\n",
    "        self.oov = \"<unk>\"\n",
    "        self.pad = \"<pad>\"\n",
    "        self.vocab = self.generate_vocabulary()\n",
    "        self.word2index = self.convert_word2index()        \n",
    "        self.index2word = self.convert_index2word()\n",
    "        self.embed = nn.Embedding(len(self.vocab), embedding_dim)\n",
    "        self.max_len_caption = 50\n",
    "\n",
    "    def read_raw_captions(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            Dictionary with raw captions list keyed by image ids (integers)\n",
    "        \"\"\"\n",
    "        captions_dict = {}\n",
    "        with open(self.captions_file_path, 'r', encoding='utf-8') as f:\n",
    "            for img_caption_line in f.readlines():\n",
    "                img_captions = img_caption_line.strip().split('\\t')\n",
    "                captions_dict[int(img_captions[0])] = img_captions[1:]\n",
    "\n",
    "        return captions_dict \n",
    "\n",
    "    def process_captions(self):\n",
    "        \"\"\"\n",
    "        Use this function to generate dictionary and other preprocessing on captions\n",
    "        \"\"\"\n",
    "\n",
    "        raw_captions_dict = self.raw_captions_dict \n",
    "        \n",
    "        # Do the preprocessing here                \n",
    "        captions_dict = raw_captions_dict\n",
    "\n",
    "        return captions_dict\n",
    "\n",
    " \n",
    "\n",
    "    def generate_vocabulary(self):\n",
    "        \"\"\"\n",
    "        Use this function to generate dictionary and other preprocessing on captions\n",
    "        \"\"\"\n",
    "        captions_dict = self.captions_dict\n",
    "\n",
    "        # Generate the vocabulary\n",
    "        \n",
    "        all_captions = \"\"        \n",
    "        for cap_lists in captions_dict.values():\n",
    "            all_captions += \" \".join(cap_lists)\n",
    "        all_captions = all_captions.lower().replace(\".\", \"\").split(\" \")\n",
    "        \n",
    "        vocab = {self.pad :1, self.oov :1, self.start :1, self.end :1}\n",
    "        vocab_update = Counter(all_captions) \n",
    "        vocab_update = {k:v for k,v in vocab_update.items() if v >= freq_threshold}\n",
    "        vocab.update(vocab_update)\n",
    "        vocab_size = len(vocab)   \n",
    "        VOCAB.update(vocab)\n",
    "        print(\"VOCAB SIZE =\", vocab_size)\n",
    "        return vocab\n",
    "    \n",
    "    def convert_word2index(self):\n",
    "        word2index = {}\n",
    "        vocab = self.vocab\n",
    "        idx = 0\n",
    "        for k, v in vocab.items():\n",
    "            word2index[k] = idx\n",
    "            idx +=1\n",
    "        \n",
    "        return word2index\n",
    "    \n",
    "    def convert_index2word(self):\n",
    "        index2word = {}\n",
    "        vocab = self.vocab\n",
    "        idx = 0\n",
    "        \n",
    "        for k, v in vocab.items():\n",
    "            index2word[idx] = k\n",
    "            idx +=1\n",
    "        \n",
    "        return index2word\n",
    "\n",
    " \n",
    "\n",
    "    def captions_transform(self, img_caption_list):\n",
    "        \"\"\"\n",
    "        Use this function to generate tensor tokens for the text captions\n",
    "        Args:\n",
    "            img_caption_list: List of captions for a particular image\n",
    "        \"\"\"\n",
    "        word2index = self.word2index\n",
    "        vocab = self.vocab\n",
    "        #index2word = self.index2word        \n",
    "        embed = self.embed\n",
    "        start = self.start\n",
    "        end = self.end\n",
    "        oov = self.oov\n",
    "        max_len_caption = self.max_len_caption\n",
    "                \n",
    "        processed_list = list(map(lambda x: start + \" \"+ x + \" \" + end, img_caption_list))\n",
    "        processed_list = list(map(lambda x: x.lower().replace(\".\", \"\").split(\" \"), processed_list))\n",
    "        processed_list = list(map(lambda x: list(map(lambda y: word2index[y] if y in vocab else word2index[oov],x)),\n",
    "                                  processed_list))\n",
    "        processed_list = list(map(lambda x: x + ( [0] * int(max_len_caption - len(x)) ),processed_list))\n",
    "        \n",
    "        # Generate tensors\n",
    "        processed_list = torch.LongTensor(processed_list)\n",
    "        processed_captions = embed(processed_list)   \n",
    "        #print(processed_captions)    \n",
    "        \n",
    "        #return torch.zeros(len(img_caption_list), 10)\n",
    "        return processed_captions\n",
    "\n",
    "\n",
    "CAPTIONS_FILE_PATH = '../data/train_cap64.tsv'\n",
    "embedding_dim = 256\n",
    "freq_threshold = 5\n",
    "captions_preprocessing_obj = CaptionsPreprocessing(CAPTIONS_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "print(len(captions_preprocessing_obj.captions_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lzmf-nlhM5eC"
   },
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bUuWkFPpM5eD"
   },
   "outputs": [],
   "source": [
    "class ImageCaptionsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, img_dir, captions_dict, img_transform=None, captions_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dir (string): Directory with all the images.\n",
    "            captions_dict: Dictionary with captions list keyed by image ids (integers)\n",
    "            img_transform (callable, optional): Optional transform to be applied\n",
    "                on the image sample.\n",
    "\n",
    "            captions_transform: (callable, optional): Optional transform to be applied\n",
    "                on the caption sample (list).\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.captions_dict = captions_dict\n",
    "        self.img_transform = img_transform\n",
    "        self.captions_transform = captions_transform\n",
    "\n",
    "        self.image_ids = list(captions_dict.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, 'image_{}.jpg'.format(self.image_ids[idx]))\n",
    "        image = io.imread(img_name)\n",
    "        captions = self.captions_dict[self.image_ids[idx]]\n",
    "\n",
    "        if self.img_transform:\n",
    "            image = self.img_transform(image)\n",
    "\n",
    "        if self.captions_transform:\n",
    "            captions = self.captions_transform(captions)\n",
    "\n",
    "        sample = {'image': image, 'captions': captions}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENCODER\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size, filters, stride=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            channels: Int: Number of Input channels to 1st convolutional layer\n",
    "            kernel_size: integer, Symmetric Conv Window = (kernel_size, kernel_size)\n",
    "            filters: python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "            stride: Tuple: (stride, stride)\n",
    "        \"\"\"\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        F1, F2, F3 = filters\n",
    "        #N, in_channels , H, W = shape\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "        padding = (1,1)\n",
    "        stride = (stride, stride)\n",
    "        self.conv1 = nn.Conv2d(in_channels = channels, out_channels = F1, kernel_size=(1,1), stride=stride, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(F1)\n",
    "        self.relu = nn.ReLU(inplace=True) \n",
    "        self.conv2 = nn.Conv2d(in_channels = F1, out_channels = F2, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.bn2 = nn.BatchNorm2d(F2)\n",
    "        self.conv3 = nn.Conv2d(in_channels = F2, out_channels = F3, kernel_size=(1,1), stride=stride, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(F3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_residual = x #backup x for residual connection\n",
    "        \n",
    "        #stage 1 main path\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        #print(\"RESI:\", x.shape)\n",
    "        \n",
    "        #stage 2 main path\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        #print(\"RESI:\", x.shape)\n",
    "        \n",
    "        #stage 3 main path\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        #print(\"RESI:\", x.shape)\n",
    "        \n",
    "        x += x_residual #add output with residual connection\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class ConvolutionalBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size, filters, stride=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            channels: Int: Number of Input channels to 1st convolutional layer\n",
    "            kernel_size: integer, Symmetric Conv Window = (kernel_size, kernel_size)\n",
    "            filters: python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "            stride: Tuple: (stride, stride)\n",
    "        \"\"\"\n",
    "        super(ConvolutionalBlock, self).__init__()\n",
    "        F1, F2, F3 = filters\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "        padding = (1,1)\n",
    "        stride = (stride, stride)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels = channels, out_channels = F1, kernel_size=(1,1), stride=stride, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(F1)\n",
    "        self.relu = nn.ReLU(inplace=True) \n",
    "        self.conv2 = nn.Conv2d(in_channels = F1, out_channels = F2, kernel_size=kernel_size, stride=(1,1), padding=padding)\n",
    "        self.bn2 = nn.BatchNorm2d(F2)\n",
    "        self.conv3 = nn.Conv2d(in_channels = F2, out_channels = F3, kernel_size=(1,1), stride=(1,1), padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(F3)\n",
    "        self.conv4 = nn.Conv2d(in_channels = channels, out_channels = F3, kernel_size=(1,1), stride=stride, padding=0)\n",
    "        self.bn4 = nn.BatchNorm2d(F3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x_residual = x #backup x for residual connection\n",
    "        \n",
    "        #stage 1 main path\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        #print(\"CONV:\", x.shape)\n",
    "        \n",
    "        #stage 2 main path\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        #print(\"CONV:\", x.shape)\n",
    "        \n",
    "        #stage 3 main path\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        #print(\"CONV:\", x.shape)\n",
    "        \n",
    "        #residual connection\n",
    "        x_residual = self.conv4(x_residual)\n",
    "        x_residual = self.bn4(x_residual)\n",
    "        x += x_residual #add output with residual connection\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, input_shape = (256, 256, 3), classes = 5):\n",
    "        \"\"\"\n",
    "        It Implements Famous Resnet50 Architecture\n",
    "        Args:\n",
    "            input_shape(tuple):(callable, optional): dimensions of image sample\n",
    "            classes(int):(callable, optional): Final output classes of softmax layer.\n",
    "        \"\"\"\n",
    "        super(ResNet50, self).__init__()\n",
    "        \n",
    "        self.pad = nn.ZeroPad2d((1, 1, 3, 3))        \n",
    "        ###STAGE1\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels=64, kernel_size=(7,7), stride = (2,2), padding=1) # convolve each of our 3-channel images with 6 different 5x5 kernels, giving us 6 feature maps\n",
    "        self.batch_norm1 = nn.BatchNorm2d(64) #BatchNorm\n",
    "        self.pool1 = nn.MaxPool2d((3,3), stride=(2,2), padding=1, dilation=1)\n",
    "        \n",
    "        ###STAGE2 channels, kernel_size=3, filters, stride=1, stage\n",
    "        self.conv_block1 = ConvolutionalBlock(channels = 64, kernel_size = 3, filters = [64, 64, 256],stride = 1)\n",
    "        self.residual_block1 = ResidualBlock(channels = 256, kernel_size = 3, filters = [64, 64, 256])\n",
    "        \n",
    "        ###STAGE3 \n",
    "        self.conv_block2 = ConvolutionalBlock(channels = 256, kernel_size = 3, filters = [128, 128, 512],stride = 2)\n",
    "        self.residual_block2 = ResidualBlock(channels = 512, kernel_size = 3, filters = [128, 128, 512],)\n",
    "        \n",
    "        ###STAGE4 \n",
    "        self.conv_block3 = ConvolutionalBlock(channels = 512, kernel_size = 3, filters = [256, 256, 1024], stride = 2)\n",
    "        self.residual_block3 = ResidualBlock(channels = 1024, kernel_size = 3, filters = [256, 256, 1024])\n",
    "        \n",
    "        ###STAGE5 \n",
    "        self.conv_block4 = ConvolutionalBlock(channels = 1024, kernel_size = 3, filters = [512, 512, 2048], stride = 2)\n",
    "        self.residual_block4 = ResidualBlock(channels = 2048, kernel_size = 3, filters = [512, 512, 2048])\n",
    "        \n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d(output_size = (1,1))\n",
    "        self.fc1 = nn.Linear(in_features=2048, out_features=classes, bias = True)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(\"IP_SIZE:\", x.shape)\n",
    "        \n",
    "        ###STAGE1        \n",
    "        #print(\"\\n STAGE1\")\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.pool1(x)\n",
    "        print(\"OP_STAGE1_SIZE:\", x.shape)\n",
    "        \n",
    "        ###STAGE2 \n",
    "        #print(\"\\n STAGE2\")\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.residual_block1(x)\n",
    "        x = self.residual_block1(x)\n",
    "        print(\"OP_STAGE2_SIZE:\", x.shape)\n",
    "        \n",
    "        ###STAGE3 \n",
    "        #print(\"\\n STAGE3\")\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.residual_block2(x)\n",
    "        x = self.residual_block2(x)\n",
    "        x = self.residual_block2(x)\n",
    "        print(\"OP_STAGE3_SIZE:\", x.shape)\n",
    "        \n",
    "        ###STAGE4  \n",
    "        #print(\"\\n STAGE4\")\n",
    "        x = self.conv_block3(x)\n",
    "        x = self.residual_block3(x)\n",
    "        x = self.residual_block3(x)\n",
    "        x = self.residual_block3(x)\n",
    "        x = self.residual_block3(x)\n",
    "        x = self.residual_block3(x)\n",
    "        print(\"OP_STAGE4_SIZE:\", x.shape)\n",
    "        \n",
    "        ###STAGE5  \n",
    "        #print(\"\\n STAGE5\")\n",
    "        x = self.conv_block4(x)\n",
    "        x = self.residual_block4(x)\n",
    "        x = self.residual_block4(x)\n",
    "        print(\"OP_STAGE5_SIZE:\", x.shape)\n",
    "        \n",
    "        x = self.adaptive_pool(x)\n",
    "        print(\"OP_ADAPTIVEPOOL_SHAPE\", x.shape)\n",
    "        \n",
    "        x = x.view(x.size(0), -1) # Flatten Vector\n",
    "        x = self.fc1(x)\n",
    "        print(\"OP_FC1_SIZE:\", x.shape)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class Encoder(nn.Module):    \n",
    "    def __init__(self, embed_dim):\n",
    "        \"\"\"\n",
    "        CNN ENCODER\n",
    "        Args:\n",
    "            embed_dim(int): embedding dimension ie output dimension of last FC Layer\n",
    "        Returns:\n",
    "            x: Feature vector of size(BatchSize, embed_dim)\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.resnet50 = ResNet50(classes = embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.resnet50(x)\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DECODER\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_dim, lstm_hidden_size, lstm_layers = 1):\n",
    "        \"\"\"\n",
    "        It Implements Famous Resnet50 Architecture\n",
    "        Args:\n",
    "            embed_dim(int): embedding dimension ie output dimension of last FC Layer\n",
    "            lstm_hidden_size(int): size of hidden units of LSTM Cell\n",
    "            lstm_layers(int, optional): Number of recurrent layers\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.vocab_size = len(VOCAB)\n",
    "        self.lstm = nn.LSTM(input_size = embed_dim, hidden_size = lstm_hidden_size,\n",
    "                            num_layers = lstm_layers, batch_first = True)\n",
    "        \n",
    "        self.linear = nn.Linear(lstm_hidden_size, self.vocab_size)\n",
    "        \n",
    "    def forward(self, image_features, embedded_captions_list):\n",
    "        N, I, L, E = embedded_captions_list.shape\n",
    "        time_instances = L - 1\n",
    "        print(\"Decoder Shape 1\",image_features.shape, embedded_captions_list.shape)\n",
    "        \n",
    "        image_features = torch.Tensor.repeat_interleave(image_features, repeats=5 , dim=0)\n",
    "        image_features = image_features.unsqueeze(1)\n",
    "        embedded_captions_list = torch.reshape(embedded_captions_list, (N*I,L,E))\n",
    "        print(\"Decoder Shape Processed\",image_features.shape, embedded_captions_list.shape)\n",
    "        \n",
    "        input_lstm = torch.cat((image_features, embedded_captions_list), dim = 1)\n",
    "        \n",
    "        lstm_outputs, _ = self.lstm(input_lstm)        \n",
    "        print(\"LSTM OP SHAPE\", lstm_outputs.shape)\n",
    "        lstm_outputs = self.linear(lstm_outputs)\n",
    "        print(\"LSTM OP POST LINEAR SHAPE\", lstm_outputs.shape)\n",
    "        return lstm_outputs, input_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6eaz6MgvM5eG"
   },
   "outputs": [],
   "source": [
    "class ImageCaptionsNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageCaptionsNet, self).__init__()\n",
    "\n",
    "        # Define your architecture here\n",
    "        \n",
    "        ##CNN ENCODER RESNET-50\n",
    "        \n",
    "        self.Encoder = Encoder(embed_dim = embedding_dim)\n",
    "        self.Decoder = Decoder(embedding_dim, units, 1)\n",
    "        #self.Decoder = DecoderRNN(256, 512, len(captions_preprocessing_obj.vocab), 1)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = image_batch, captions_batch\n",
    "\n",
    "        # Forward Propogation\n",
    "        x = self.Encoder(image_batch)\n",
    "        #print(x.shape)\n",
    "        x = self.Decoder(x, captions_batch)\n",
    "        return x\n",
    "units = 512\n",
    "net = ImageCaptionsNet()\n",
    "net = net.double()\n",
    "# If GPU training is required\n",
    "# net = net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rUQtOQ5JM5eI"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZmqfKZIPM5eI",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_shape torch.Size([8, 3, 256, 256])\n",
      "batch_shape torch.Size([8, 5, 50, 256])\n",
      "IP_SIZE: torch.Size([8, 3, 256, 256])\n",
      "OP_STAGE1_SIZE: torch.Size([8, 64, 63, 63])\n",
      "OP_STAGE2_SIZE: torch.Size([8, 256, 63, 63])\n",
      "OP_STAGE3_SIZE: torch.Size([8, 512, 32, 32])\n",
      "OP_STAGE4_SIZE: torch.Size([8, 1024, 16, 16])\n",
      "OP_STAGE5_SIZE: torch.Size([8, 2048, 8, 8])\n",
      "OP_ADAPTIVEPOOL_SHAPE torch.Size([8, 2048, 1, 1])\n",
      "OP_FC1_SIZE: torch.Size([8, 256])\n",
      "Decoder Shape 1 torch.Size([8, 256]) torch.Size([8, 5, 50, 256])\n",
      "Decoder Shape Processed torch.Size([40, 1, 256]) torch.Size([40, 50, 256])\n",
      "LSTM OP SHAPE torch.Size([40, 51, 512])\n",
      "LSTM OP POST LINEAR SHAPE torch.Size([40, 51, 256])\n",
      "size for loss torch.Size([40, 51, 256]) torch.Size([40, 51, 256])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected target size (40, 256), got torch.Size([40, 51, 256])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-a6a6ab2c4f29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0moutput_captions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptions_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptions_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"size for loss\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_captions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptions_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_captions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptions_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    946\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m--> 948\u001b[1;33m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2420\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2421\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2422\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2226\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2227\u001b[0m             raise ValueError('Expected target size {}, got {}'.format(\n\u001b[1;32m-> 2228\u001b[1;33m                 out_size, target.size()))\n\u001b[0m\u001b[0;32m   2229\u001b[0m         \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2230\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected target size (40, 256), got torch.Size([40, 51, 256])"
     ]
    }
   ],
   "source": [
    "IMAGE_DIR = '../data/train/'\n",
    "\n",
    "# Creating the Dataset\n",
    "train_dataset = ImageCaptionsDataset(\n",
    "    IMAGE_DIR, captions_preprocessing_obj.captions_dict, img_transform=img_transform,\n",
    "    captions_transform=captions_preprocessing_obj.captions_transform\n",
    ")\n",
    "\n",
    "# Define your hyperparameters\n",
    "NUMBER_OF_EPOCHS = 3\n",
    "LEARNING_RATE = 1e-1\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 0 # Parallel threads for dataloading\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Creating the DataLoader for batching purposes\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "import os\n",
    "for epoch in range(NUMBER_OF_EPOCHS):\n",
    "    for batch_idx, sample in enumerate(train_loader):\n",
    "        net.zero_grad()\n",
    "\n",
    "        image_batch, captions_batch = sample['image'], sample['captions']\n",
    "        \n",
    "        print(\"image_shape\", image_batch.shape)\n",
    "        print(\"batch_shape\", captions_batch.shape)\n",
    "        # If GPU training required\n",
    "        # image_batch, captions_batch = image_batch.cuda(), captions_batch.cuda()\n",
    "\n",
    "        output_captions, captions_batch = net((image_batch, captions_batch))\n",
    "        print(\"size for loss\", output_captions.shape, captions_batch.shape)\n",
    "        loss = loss_function(output_captions, captions_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Iteration: \" + str(epoch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(net, (32,3,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[\"abc\"],[\"bcd\"],[\"asdf\"],[\"fsdfsd\"],[\"fsdfd\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W0pi-EQYM5eF"
   },
   "source": [
    "### Model Architecture\n",
    "\n",
    "\n",
    "#### 3 - Building  first ResNet model (50 layers)\n",
    "You now have the necessary blocks to build a very deep ResNet. The following figure describes in detail the architecture of this neural network. \"ID BLOCK\" in the diagram stands for \"Identity block,\" and \"ID BLOCK x3\" means you should stack 3 identity blocks together.\n",
    "\n",
    "\n",
    "The details of this ResNet-50 model are:\n",
    "- Zero-padding pads the input with a pad of (3,3)\n",
    "- Stage 1:\n",
    "    - The 2D Convolution has 64 filters of shape (7,7) and uses a stride of (2,2). Its name is \"conv1\".\n",
    "    - BatchNorm is applied to the 'channels' axis of the input.\n",
    "    - MaxPooling uses a (3,3) window and a (2,2) stride.\n",
    "- Stage 2:\n",
    "    - The convolutional block uses three sets of filters of size [64,64,256], \"f\" is 3, \"s\" is 1 and the block is \"a\".\n",
    "    - The 2 identity blocks use three sets of filters of size [64,64,256], \"f\" is 3 and the blocks are \"b\" and \"c\".\n",
    "- Stage 3:\n",
    "    - The convolutional block uses three sets of filters of size [128,128,512], \"f\" is 3, \"s\" is 2 and the block is \"a\".\n",
    "    - The 3 identity blocks use three sets of filters of size [128,128,512], \"f\" is 3 and the blocks are \"b\", \"c\" and \"d\".\n",
    "- Stage 4:\n",
    "    - The convolutional block uses three sets of filters of size [256, 256, 1024], \"f\" is 3, \"s\" is 2 and the block is \"a\".\n",
    "    - The 5 identity blocks use three sets of filters of size [256, 256, 1024], \"f\" is 3 and the blocks are \"b\", \"c\", \"d\", \"e\" and \"f\".\n",
    "- Stage 5:\n",
    "    - The convolutional block uses three sets of filters of size [512, 512, 2048], \"f\" is 3, \"s\" is 2 and the block is \"a\".\n",
    "    - The 2 identity blocks use three sets of filters of size [512, 512, 2048], \"f\" is 3 and the blocks are \"b\" and \"c\".\n",
    "- The 2D Average Pooling uses a window of shape (2,2) and its name is \"avg_pool\".\n",
    "- The 'flatten' layer doesn't have any hyperparameters or name.\n",
    "- The Fully Connected (Dense) layer reduces its input to the number of classes using a softmax activation. Its name should be `'fc' + str(classes)`.\n",
    "\n",
    "**Exercise**: Implement the ResNet with 50 layers described in the figure above. We have implemented Stages 1 and 2. Please implement the rest. (The syntax for implementing Stages 3-5 should be quite similar to that of Stage 2.) Make sure you follow the naming convention in the text above. \n",
    "\n",
    "You'll need to use this function: \n",
    "- Average pooling [see reference](https://keras.io/layers/pooling/#averagepooling2d)\n",
    "\n",
    "Here are some other functions we used in the code below:\n",
    "- Conv2D: [See reference](https://keras.io/layers/convolutional/#conv2d)\n",
    "- BatchNorm: [See reference](https://keras.io/layers/normalization/#batchnormalization) (axis: Integer, the axis that should be normalized (typically the features axis))\n",
    "- Zero padding: [See reference](https://keras.io/layers/convolutional/#zeropadding2d)\n",
    "- Max pooling: [See reference](https://keras.io/layers/pooling/#maxpooling2d)\n",
    "- Fully connected layer: [See reference](https://keras.io/layers/core/#dense)\n",
    "- Addition: [See reference](https://keras.io/layers/merge/#add)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1,2,3],[4,5,6]]\n",
    "print(a.shape)\n",
    "np.repeat(a,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2, 3],\n",
      "         [3, 4, 3]],\n",
      "\n",
      "        [[5, 6, 4],\n",
      "         [7, 8, 7]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[[1, 2, 3],\n",
      "          [3, 4, 3]]],\n",
      "\n",
      "\n",
      "        [[[5, 6, 4],\n",
      "          [7, 8, 7]]]])\n",
      "torch.Size([4, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[[1, 2,3],[3, 4,3]],[[5, 6,4],[7, 8,7]]])\n",
    "print(t)\n",
    "print(t.shape)\n",
    "\n",
    "print(t.unsqueeze(1))\n",
    "\n",
    "t = torch.reshape(t, (4,3))\n",
    "print(t.shape)\n",
    "t.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [1, 2, 3],\n",
      "        [1, 2, 3],\n",
      "        [1, 2, 3],\n",
      "        [1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [4, 5, 6],\n",
      "        [4, 5, 6],\n",
      "        [4, 5, 6],\n",
      "        [4, 5, 6]]) torch.Size([10, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3],[4,5,6]])\n",
    "print(x.shape)\n",
    "x = x.repeat_interleave(5, dim=0)\n",
    "print(x, x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.reshape((5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.view(-1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECODER\n",
    "\"\"\"\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, embed_dim, units):\n",
    "        super(Attention, self).__init__()        \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(1)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.units = units\n",
    "        self.U = nn.Linear(units, units)\n",
    "        self.W = nn.Linear(embed_dim, units)\n",
    "        self.V = nn.Linear(units, 1)\n",
    "    \n",
    "    def forward(self, features, hidden_state):        \n",
    "        W_s = self.W(features)\n",
    "        U_hidden = self.U(hidden_state).unsqueeze(1)\n",
    "        attention = self.V(self.tanh(W_s + U_h)).squeeze(2)\n",
    "        attention = self.softmax(e)\n",
    "        context = (img_features * attention.unsqueeze(2)).sum(1)\n",
    "        return context, attention\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_dim, lstm_hidden_size, lstm_layers = 1):\n",
    "        \"\"\"'''\n",
    "        It Implements Famous Resnet50 Architecture\n",
    "        Args:\n",
    "            embed_dim(int): embedding dimension ie output dimension of last FC Layer\n",
    "            lstm_hidden_size(int): size of hidden units of LSTM Cell\n",
    "            lstm_layers(int, optional): Number of recurrent layers\n",
    "        '''\"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.vocab_size = len(VOCAB)\n",
    "        \n",
    "        self.hidden_state = nn.Linear(embed_dim, lstm_hidden_size)\n",
    "        self.cell_state = nn.Linear(embed_dim, lstm_hidden_size)\n",
    "        \n",
    "        self.attention = Attention(embed_dim, lstm_hidden_size)\n",
    "        self.beta = nn.Linear(lstm_hidden_size, embed_dim)        \n",
    "        \n",
    "        # lstm cell\n",
    "        self.lstm = nn.LSTMCell(input_size=embed_dim+lstm_hidden_size, hidden_size=lstm_hidden_size)\n",
    "        self.model_output = nn.Linear(lstm_hidden_size, self.vocab_size)\n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "    def forward(self, image_features, embedded_captions_list):\n",
    "        N, I, L, E = embedded_captions_list.shape\n",
    "        time_instances = L - 1\n",
    "        print(\"Decoder Shape 1\",image_features.shape, embedded_captions_list.shape)\n",
    "        \n",
    "        image_features = torch.Tensor.repeat_interleave(image_features, repeats=5 , dim=0)\n",
    "        embedded_captions_list = torch.reshape(embedded_captions_list, (N*I,L,E))\n",
    "        print(\"Decoder Shape Processed\",image_features.shape, embedded_captions_list.shape)\n",
    "        \n",
    "        hidden_state, cell_state = self.initialize_lstm_state(image_features)\n",
    "        \n",
    "        alphas = torch.zeros(batch_size, max_timespan, image_features.size(1))\n",
    "        predictions = torch.zeros(N, time_instances, self.vocab_size)\n",
    "                \n",
    "        for t in range(time_instances):\n",
    "            #DO PROCESSING HERE WITH LSTM\n",
    "            \n",
    "            context, alpha = self.attention(image_features, hidden_state)\n",
    "            gated_context = self.sigmoid(self.f_beta(hidden_state)) * context\n",
    "            \n",
    "            input_lstm = torch.cat((embedded_captions_list, gated_context), dim=1)\n",
    "            \n",
    "            hidden_state, cell_state = self.lstm(input_lstm, (hidden_state, cell_state))\n",
    "            \n",
    "            out = self.model_output(self.dropout(hidden_state))\n",
    "            \n",
    "            preds[:, t] = output\n",
    "            alphas[:, t] = alpha\n",
    "        \n",
    "        return predictions, alphas, embedded_captions_list\n",
    "    \n",
    "    def initialize_lstm_state(self, img_features):\n",
    "        avg_features = img_features.mean(dim=0)\n",
    "\n",
    "        c = self.cell_state(avg_features)\n",
    "        c = self.tanh(c)\n",
    "\n",
    "        h = self.hidden_state(avg_features)\n",
    "        h = self.tanh(h)\n",
    "\n",
    "        return h, c\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        # define the properties\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # lstm cell\n",
    "        self.lstm_cell = nn.LSTMCell(input_size=embed_size, hidden_size=hidden_size)\n",
    "    \n",
    "        # output fully connected layer\n",
    "        self.fc_out = nn.Linear(in_features=self.hidden_size, out_features=self.vocab_size)\n",
    "    \n",
    "        # embedding layer\n",
    "        self.embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_size)\n",
    "    \n",
    "        # activations\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        \n",
    "        # batch size\n",
    "        batch_size = features.size(0)\n",
    "        \n",
    "        # init the hidden and cell states to zeros\n",
    "        hidden_state = torch.zeros((batch_size, self.hidden_size))\n",
    "        cell_state = torch.zeros((batch_size, self.hidden_size))\n",
    "    \n",
    "        # define the output tensor placeholder\n",
    "        outputs = torch.empty((batch_size, captions.size(1), self.vocab_size))\n",
    "\n",
    "        # embed the captions\n",
    "        #captions_embed = self.embed(captions)\n",
    "        captions_embed = captions\n",
    "        \n",
    "        # pass the caption word by word\n",
    "        for t in range(captions.size(1)):\n",
    "\n",
    "            # for the first time step the input is the feature vector\n",
    "            if t == 0:\n",
    "                hidden_state, cell_state = self.lstm_cell(features, (hidden_state, cell_state))\n",
    "                \n",
    "            # for the 2nd+ time step, using teacher forcer\n",
    "            else:\n",
    "                hidden_state, cell_state = self.lstm_cell(captions_embed[:, t, :], (hidden_state, cell_state))\n",
    "            \n",
    "            # output of the attention mechanism\n",
    "            out = self.fc_out(hidden_state)\n",
    "            \n",
    "            # build the output tensor\n",
    "            outputs[:, t, :] = out\n",
    "    \n",
    "        return outputs'''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pytorchtut.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
